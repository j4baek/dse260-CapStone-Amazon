{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Recommender System with SageMaker, MXNet, and Gluon\n",
    "## _**Making Product - Shoes Recommendations Using Neural Networks and Embeddings**_\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "  1. [Explore](#Explore)\n",
    "  1. [Clean](#Clean)\n",
    "  1. [Prepare](#Prepare)\n",
    "1. [Train Locally](#Train-Locally)\n",
    "  1. [Define Network](#Define-Network)\n",
    "  1. [Set Parameters](#Set-Parameters)\n",
    "  1. [Execute](#Execute)\n",
    "1. [Train with SageMaker](#Train-with-SageMaker)\n",
    "  1. [Wrap Code](#Wrap-Code)\n",
    "  1. [Move Data](#Move-Data)\n",
    "  1. [Submit](#Submit)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "1. [Wrap-up](#Wrap-up)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "#### In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "#### Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.  The minimal required dataset for this is a history of user item ratings.  In our case, we'll use 1 to 5 star ratings from over 2M Amazon customers.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "#### Matrix factorization has been the cornerstone of most user-item prediction models.  This method starts with the large, sparse, user-item ratings in a single matrix, where users index the rows, and items index the columns.  It then seeks to find two lower-dimensional, dense matrices which, when multiplied together, preserve the information and relationships in the larger matrix.\n",
    "\n",
    "![image](./images/1_PefuBiYr9Bp7lo_zotGj0Q.png)\n",
    "\n",
    "### ** Matrix factorization has been extended and genarlized with deep learning and embeddings.  These techniques allows us to introduce non-linearities for enhanced performance and flexibility.  This notebook will fit a neural network-based model to generate recommendations for the Amazon dataset.  It will start by exploring our data in the notebook and even training a model on a sample of the data.  Later we'll expand to the full dataset and fit our model using a SageMaker managed training cluster.  We'll then deploy to an endpoint and check our method.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "#### _This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "#### Let's start by specifying:\n",
    "\n",
    "#### - The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "#### - The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (0.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To measure all running time\n",
    "# https://github.com/cpcloud/ipython-autotime\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 801 ms\n"
     ]
    }
   ],
   "source": [
    "bucket = 'dse-cohort5-group1'\n",
    "prefix = 'sagemaker/amazon_reviews_us_Shoes_v1_00'\n",
    "\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Python libraries we'll need for the remainder of this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image==0.14.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (0.14.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (1.2.1)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (2.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (0.5.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (1.11.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (3.0.3)\n",
      "Requirement already satisfied: dask[array]>=1.0.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (2.16.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (0.5.2)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scikit-image==0.14.2) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from scipy>=0.17.0->scikit-image==0.14.2) (1.16.4)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from networkx>=1.8->scikit-image==0.14.2) (4.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image==0.14.2) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image==0.14.2) (2.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image==0.14.2) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image==0.14.2) (2.7.3)\n",
      "Requirement already satisfied: toolz>=0.8.2; extra == \"array\" in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from dask[array]>=1.0.0->scikit-image==0.14.2) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image==0.14.2) (39.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "# Install a scikit-image package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-image==0.14.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, ndarray\n",
    "from mxnet.metric import MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "import random_tuner as rt\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 462 ms\n"
     ]
    }
   ],
   "source": [
    "# for basic visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# for advanced visualizations\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected = True)\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data - https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "\n",
    "### Explore\n",
    "\n",
    "Let's start by bringing in our dataset from an S3 public bucket.  \n",
    "More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "_Note, because this dataset is over a half gigabyte, the load from S3 may take ~10 minutes.  Also, since Amazon SageMaker Notebooks start with a 5GB persistent volume by default, and we don't need to keep this data on our instance for long, we'll bring it to the temporary volume (which has up to 20GB of storage)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /tmp/recsys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-19 23:19:31 1597185012 Apparel_Jewelry_Shoes_df.csv.gz\n",
      "time: 668 ms\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_df.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_df.csv.gz to ../../../../../../tmp/recsys/Apparel_Jewelry_Shoes_df.csv.gz\n",
      "time: 8.19 s\n"
     ]
    }
   ],
   "source": [
    "!rm -rf  /tmp/recsys/\n",
    "!mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_df.csv.gz /tmp/recsys/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) so that we can begin to understand it.\n",
    "\n",
    "*Note, we'll set `error_bad_lines=False` when reading the file in as there appear to be a very small number of records which would create a problem otherwise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user 1.5G May 19 23:19 /tmp/recsys/Apparel_Jewelry_Shoes_df.csv.gz\n",
      "time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "!ls -alh /tmp/recsys/Apparel_Jewelry_Shoes_df.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning:\n",
      "\n",
      "Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/tmp/recsys/Apparel_Jewelry_Shoes_df.csv.gz', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>catalog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8060263</th>\n",
       "      <td>3701443</td>\n",
       "      <td>US</td>\n",
       "      <td>16053677</td>\n",
       "      <td>R1JL39AO665RYX</td>\n",
       "      <td>B0085CCAZS</td>\n",
       "      <td>463655543</td>\n",
       "      <td>Womens 100% Cotton Zipper Accent Army Military...</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>For small heads only</td>\n",
       "      <td>Nothing wrong with the actual product, but \\\\\"...</td>\n",
       "      <td>2012-12-23</td>\n",
       "      <td>Apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506215</th>\n",
       "      <td>4147395</td>\n",
       "      <td>US</td>\n",
       "      <td>12667741</td>\n",
       "      <td>R2KM60O197NW1M</td>\n",
       "      <td>B005ZKUUG4</td>\n",
       "      <td>862440161</td>\n",
       "      <td>Leisureland Men's Plush Coral Fleece Solid Col...</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fleece Robe</td>\n",
       "      <td>Good fit, very warm.  Fleece is the ONLY way t...</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>Apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5673200</th>\n",
       "      <td>1314380</td>\n",
       "      <td>US</td>\n",
       "      <td>30157579</td>\n",
       "      <td>R25OMG475PLKA4</td>\n",
       "      <td>B00JUP6ER4</td>\n",
       "      <td>256827720</td>\n",
       "      <td>Allegra K Women's Dolman Sleeves Round Neck Ov...</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>So far so good. Loved it when I tried it on bu...</td>\n",
       "      <td>So far so good. Loved it when I tried it on bu...</td>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>Apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977100</th>\n",
       "      <td>1977100</td>\n",
       "      <td>US</td>\n",
       "      <td>12732153</td>\n",
       "      <td>R15NRRWXWOGSS8</td>\n",
       "      <td>B00D1KWP2C</td>\n",
       "      <td>144774668</td>\n",
       "      <td>Merrell Women's Hollyleaf Sandal</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Need to order 7.5</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>Shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10250477</th>\n",
       "      <td>9783</td>\n",
       "      <td>US</td>\n",
       "      <td>41942110</td>\n",
       "      <td>R1WRXJ15HQFBNA</td>\n",
       "      <td>B00XBBOV9Q</td>\n",
       "      <td>888146846</td>\n",
       "      <td>Dog Mom Barrel Charm Bead for Snake Chain Char...</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>It is a little wider/longer than the other bea...</td>\n",
       "      <td>This was the first bead I ever bought that was...</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>Jewelry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11042098</th>\n",
       "      <td>801404</td>\n",
       "      <td>US</td>\n",
       "      <td>36959404</td>\n",
       "      <td>RT9S3LFY19FW8</td>\n",
       "      <td>B00DID9JK8</td>\n",
       "      <td>388564180</td>\n",
       "      <td>2mm .925 Italian Sterling Silver 24\", 30\", &amp; 3...</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Silver necklaces</td>\n",
       "      <td>Love them. Each a different length and easy to...</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>Jewelry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363668</th>\n",
       "      <td>5004848</td>\n",
       "      <td>US</td>\n",
       "      <td>16282471</td>\n",
       "      <td>R3REZR2T8BQ6E</td>\n",
       "      <td>B002YB177I</td>\n",
       "      <td>669171809</td>\n",
       "      <td>Tilley TWF1 Montana Hat</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>He loved it!</td>\n",
       "      <td>Great hat!</td>\n",
       "      <td>2014-12-30</td>\n",
       "      <td>Apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716716</th>\n",
       "      <td>357896</td>\n",
       "      <td>US</td>\n",
       "      <td>43961970</td>\n",
       "      <td>R38G21155W6NB4</td>\n",
       "      <td>B00QMB2C2A</td>\n",
       "      <td>717801795</td>\n",
       "      <td>Susana Monaco Women's Cotton Pom Pom with Croc...</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I want to love this dress because of how it lo...</td>\n",
       "      <td>I want to love this dress because of how it lo...</td>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>Apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239718</th>\n",
       "      <td>239718</td>\n",
       "      <td>US</td>\n",
       "      <td>33890781</td>\n",
       "      <td>R1UTGO2KW0PH72</td>\n",
       "      <td>B00YT52UXC</td>\n",
       "      <td>203184148</td>\n",
       "      <td>Orly Kids' Meow-G (Little Big )</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>These run large, but otherwise are cute. My 8 ...</td>\n",
       "      <td>2015-07-23</td>\n",
       "      <td>Shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10553895</th>\n",
       "      <td>313201</td>\n",
       "      <td>US</td>\n",
       "      <td>15327607</td>\n",
       "      <td>R2P1PY5T8ML355</td>\n",
       "      <td>B00EFTAA8Y</td>\n",
       "      <td>854802856</td>\n",
       "      <td>INBLUE Men's Stainless Steel Ring Band Silver ...</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>My Biker Man's Wedding Band</td>\n",
       "      <td>Awesome. Very Masculine for my &amp;#34;Biker&amp;#34;...</td>\n",
       "      <td>2015-04-07</td>\n",
       "      <td>Jewelry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0 marketplace  customer_id       review_id  product_id  \\\n",
       "8060263      3701443          US     16053677  R1JL39AO665RYX  B0085CCAZS   \n",
       "8506215      4147395          US     12667741  R2KM60O197NW1M  B005ZKUUG4   \n",
       "5673200      1314380          US     30157579  R25OMG475PLKA4  B00JUP6ER4   \n",
       "1977100      1977100          US     12732153  R15NRRWXWOGSS8  B00D1KWP2C   \n",
       "10250477        9783          US     41942110  R1WRXJ15HQFBNA  B00XBBOV9Q   \n",
       "11042098      801404          US     36959404   RT9S3LFY19FW8  B00DID9JK8   \n",
       "9363668      5004848          US     16282471   R3REZR2T8BQ6E  B002YB177I   \n",
       "4716716       357896          US     43961970  R38G21155W6NB4  B00QMB2C2A   \n",
       "239718        239718          US     33890781  R1UTGO2KW0PH72  B00YT52UXC   \n",
       "10553895      313201          US     15327607  R2P1PY5T8ML355  B00EFTAA8Y   \n",
       "\n",
       "          product_parent                                      product_title  \\\n",
       "8060263        463655543  Womens 100% Cotton Zipper Accent Army Military...   \n",
       "8506215        862440161  Leisureland Men's Plush Coral Fleece Solid Col...   \n",
       "5673200        256827720  Allegra K Women's Dolman Sleeves Round Neck Ov...   \n",
       "1977100        144774668                   Merrell Women's Hollyleaf Sandal   \n",
       "10250477       888146846  Dog Mom Barrel Charm Bead for Snake Chain Char...   \n",
       "11042098       388564180  2mm .925 Italian Sterling Silver 24\", 30\", & 3...   \n",
       "9363668        669171809                            Tilley TWF1 Montana Hat   \n",
       "4716716        717801795  Susana Monaco Women's Cotton Pom Pom with Croc...   \n",
       "239718         203184148                    Orly Kids' Meow-G (Little Big )   \n",
       "10553895       854802856  INBLUE Men's Stainless Steel Ring Band Silver ...   \n",
       "\n",
       "         product_category star_rating  helpful_votes  total_votes vine  \\\n",
       "8060263           Apparel           3            1.0          1.0    N   \n",
       "8506215           Apparel           5            0.0          0.0    N   \n",
       "5673200           Apparel           5            0.0          0.0    N   \n",
       "1977100             Shoes           4            0.0          1.0    N   \n",
       "10250477          Jewelry           4            1.0          1.0    N   \n",
       "11042098          Jewelry           5            1.0          1.0    N   \n",
       "9363668           Apparel           5            0.0          0.0    N   \n",
       "4716716           Apparel           4            1.0          1.0    N   \n",
       "239718              Shoes           3            1.0          1.0    N   \n",
       "10553895          Jewelry           5            3.0          3.0    N   \n",
       "\n",
       "         verified_purchase                                    review_headline  \\\n",
       "8060263                  N                               For small heads only   \n",
       "8506215                  Y                                        Fleece Robe   \n",
       "5673200                  Y  So far so good. Loved it when I tried it on bu...   \n",
       "1977100                  Y                                         Four Stars   \n",
       "10250477                 Y  It is a little wider/longer than the other bea...   \n",
       "11042098                 N                                   Silver necklaces   \n",
       "9363668                  Y                                       He loved it!   \n",
       "4716716                  Y  I want to love this dress because of how it lo...   \n",
       "239718                   Y                                        Three Stars   \n",
       "10553895                 Y                        My Biker Man's Wedding Band   \n",
       "\n",
       "                                                review_body review_date  \\\n",
       "8060263   Nothing wrong with the actual product, but \\\\\"...  2012-12-23   \n",
       "8506215   Good fit, very warm.  Fleece is the ONLY way t...  2014-03-09   \n",
       "5673200   So far so good. Loved it when I tried it on bu...  2015-05-19   \n",
       "1977100                                   Need to order 7.5  2014-10-02   \n",
       "10250477  This was the first bead I ever bought that was...  2015-08-27   \n",
       "11042098  Love them. Each a different length and easy to...  2014-09-08   \n",
       "9363668                                          Great hat!  2014-12-30   \n",
       "4716716   I want to love this dress because of how it lo...  2015-06-12   \n",
       "239718    These run large, but otherwise are cute. My 8 ...  2015-07-23   \n",
       "10553895  Awesome. Very Masculine for my &#34;Biker&#34;...  2015-04-07   \n",
       "\n",
       "          catalog  \n",
       "8060263   Apparel  \n",
       "8506215   Apparel  \n",
       "5673200   Apparel  \n",
       "1977100     Shoes  \n",
       "10250477  Jewelry  \n",
       "11042098  Jewelry  \n",
       "9363668   Apparel  \n",
       "4716716   Apparel  \n",
       "239718      Shoes  \n",
       "10553895  Jewelry  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 589 ms\n"
     ]
    }
   ],
   "source": [
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.0G\n",
      "drwxrwxr-x 5 ec2-user ec2-user 4.0K May 20 02:55 .\n",
      "drwxrwxr-x 9 ec2-user ec2-user 4.0K May 20 01:54 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.5G May 19 23:15 Apparel_Jewelry_Shoes_df.csv.gz\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.5G May 19 20:57 Apparel_Jewelry_Shoes_df.tsv.gz\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 777K May 20 01:59 Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  80K May 20 02:55 Apparel_Jewelry_Shoes.ipynb\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 5.6M May 20 00:56 Apparel_Jewelry_Shoes_predictions_from_user.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 5.6M May 20 00:56 Apparel_Jewelry_Shoes_predictions_from_user.pickle\n",
      "drwxrwxr-x 3 ec2-user ec2-user 4.0K May 19 20:37 Archive\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K May 19 20:37 .ipynb_checkpoints\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K May 19 20:40 __pycache__\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 5.0K May 19 20:40 random_tuner.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 9.0K May 19 20:37 recommender.py\n",
      "time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Amazon product dataset data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset includes information like:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case this products).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written.\n",
    "- `catalog`: The date cataglory\n",
    "\n",
    "For this example, let's limit ourselves to `customer_id`, `product_id`, and `star_rating`.  Including additional features in our recommendation system could be beneficial, but would require substantial processing (particularly the text data) which would take us beyond the scope of this notebook.\n",
    "\n",
    "*Note: we'll keep `product_title` on the dataset to help verify our recommendations later in the notebook, but it will not be used in algorithm training.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because most people haven't use most products, and people rate fewer products than we actually watch, we'd expect our data to be sparse.  Our algorithm should work well with this sparse problem in general, but we may still want to clean out some of the long tail.  Let's look at some basic percentiles to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12007686, 17)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.89 ms\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.200769e+07</td>\n",
       "      <td>1.200769e+07</td>\n",
       "      <td>1.200769e+07</td>\n",
       "      <td>1.200768e+07</td>\n",
       "      <td>1.200768e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.361737e+06</td>\n",
       "      <td>2.564378e+07</td>\n",
       "      <td>5.011007e+08</td>\n",
       "      <td>9.146331e-01</td>\n",
       "      <td>1.101358e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.588463e+06</td>\n",
       "      <td>1.565620e+07</td>\n",
       "      <td>2.894260e+08</td>\n",
       "      <td>1.526607e+01</td>\n",
       "      <td>1.573970e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000300e+04</td>\n",
       "      <td>2.220000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000640e+06</td>\n",
       "      <td>1.269099e+07</td>\n",
       "      <td>2.497713e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.118425e+06</td>\n",
       "      <td>2.364762e+07</td>\n",
       "      <td>5.018660e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.619386e+06</td>\n",
       "      <td>4.031507e+07</td>\n",
       "      <td>7.534187e+08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.881873e+06</td>\n",
       "      <td>5.309657e+07</td>\n",
       "      <td>9.999999e+08</td>\n",
       "      <td>4.127800e+04</td>\n",
       "      <td>4.188900e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0   customer_id  product_parent  helpful_votes   total_votes\n",
       "count  1.200769e+07  1.200769e+07    1.200769e+07   1.200768e+07  1.200768e+07\n",
       "mean   2.361737e+06  2.564378e+07    5.011007e+08   9.146331e-01  1.101358e+00\n",
       "std    1.588463e+06  1.565620e+07    2.894260e+08   1.526607e+01  1.573970e+01\n",
       "min    0.000000e+00  1.000300e+04    2.220000e+02   0.000000e+00  0.000000e+00\n",
       "25%    1.000640e+06  1.269099e+07    2.497713e+08   0.000000e+00  0.000000e+00\n",
       "50%    2.118425e+06  2.364762e+07    5.018660e+08   0.000000e+00  0.000000e+00\n",
       "75%    3.619386e+06  4.031507e+07    7.534187e+08   1.000000e+00  1.000000e+00\n",
       "max    5.881873e+06  5.309657e+07    9.999999e+08   4.127800e+04  4.188900e+04"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "# Describing the data set\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0              0\n",
       "marketplace             0\n",
       "customer_id             0\n",
       "review_id               0\n",
       "product_id              0\n",
       "product_parent          0\n",
       "product_title          38\n",
       "product_category        0\n",
       "star_rating            10\n",
       "helpful_votes          11\n",
       "total_votes            11\n",
       "vine                   11\n",
       "verified_purchase      11\n",
       "review_headline       106\n",
       "review_body          1469\n",
       "review_date           147\n",
       "catalog                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "# checking if there is any null data or not\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13 s\n"
     ]
    }
   ],
   "source": [
    "# remove numm data\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           0\n",
       "marketplace          0\n",
       "customer_id          0\n",
       "review_id            0\n",
       "product_id           0\n",
       "product_parent       0\n",
       "product_title        0\n",
       "product_category     0\n",
       "star_rating          0\n",
       "helpful_votes        0\n",
       "total_votes          0\n",
       "vine                 0\n",
       "verified_purchase    0\n",
       "review_headline      0\n",
       "review_body          0\n",
       "review_date          0\n",
       "catalog              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.19 s\n"
     ]
    }
   ],
   "source": [
    "# checking if there is any null data or not\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">Unnamed: 0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">customer_id</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">helpful_votes</th>\n",
       "      <th colspan=\"8\" halign=\"left\">total_votes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star_rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>827639.0</td>\n",
       "      <td>2.266308e+06</td>\n",
       "      <td>1.641208e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855181.50</td>\n",
       "      <td>1863711.0</td>\n",
       "      <td>3583768.50</td>\n",
       "      <td>5881863.0</td>\n",
       "      <td>827639.0</td>\n",
       "      <td>2.480226e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5341.0</td>\n",
       "      <td>827639.0</td>\n",
       "      <td>2.255358</td>\n",
       "      <td>16.016836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>709339.0</td>\n",
       "      <td>2.400289e+06</td>\n",
       "      <td>1.595248e+06</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1028330.50</td>\n",
       "      <td>2192972.0</td>\n",
       "      <td>3660481.00</td>\n",
       "      <td>5881865.0</td>\n",
       "      <td>709339.0</td>\n",
       "      <td>2.620339e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>904.0</td>\n",
       "      <td>709339.0</td>\n",
       "      <td>1.413192</td>\n",
       "      <td>6.063569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1180827.0</td>\n",
       "      <td>2.392412e+06</td>\n",
       "      <td>1.583831e+06</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1037452.50</td>\n",
       "      <td>2184988.0</td>\n",
       "      <td>3629866.00</td>\n",
       "      <td>5881871.0</td>\n",
       "      <td>1180827.0</td>\n",
       "      <td>2.580105e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>1180827.0</td>\n",
       "      <td>1.136638</td>\n",
       "      <td>5.707008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2253164.0</td>\n",
       "      <td>2.423911e+06</td>\n",
       "      <td>1.579042e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1073383.75</td>\n",
       "      <td>2241395.0</td>\n",
       "      <td>3663239.50</td>\n",
       "      <td>5881873.0</td>\n",
       "      <td>2253164.0</td>\n",
       "      <td>2.579251e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1338.0</td>\n",
       "      <td>2253164.0</td>\n",
       "      <td>0.958812</td>\n",
       "      <td>6.071467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7002219.0</td>\n",
       "      <td>2.348141e+06</td>\n",
       "      <td>1.586956e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>983950.00</td>\n",
       "      <td>2098852.0</td>\n",
       "      <td>3610272.50</td>\n",
       "      <td>5881872.0</td>\n",
       "      <td>7002219.0</td>\n",
       "      <td>2.559842e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41278.0</td>\n",
       "      <td>7002219.0</td>\n",
       "      <td>0.971093</td>\n",
       "      <td>19.313839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41889.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1247.0</td>\n",
       "      <td>1.499485e+06</td>\n",
       "      <td>4.869797e+03</td>\n",
       "      <td>1490251.0</td>\n",
       "      <td>1495374.50</td>\n",
       "      <td>1499834.0</td>\n",
       "      <td>1503816.00</td>\n",
       "      <td>1507319.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>2.844994e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>1.987169</td>\n",
       "      <td>5.235681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1263.0</td>\n",
       "      <td>1.515104e+06</td>\n",
       "      <td>4.535193e+03</td>\n",
       "      <td>1507333.0</td>\n",
       "      <td>1511097.50</td>\n",
       "      <td>1515042.0</td>\n",
       "      <td>1518961.50</td>\n",
       "      <td>1523017.0</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>2.914515e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>2.399842</td>\n",
       "      <td>6.254322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>967.0</td>\n",
       "      <td>1.498753e+06</td>\n",
       "      <td>4.948805e+03</td>\n",
       "      <td>1490250.0</td>\n",
       "      <td>1494399.00</td>\n",
       "      <td>1498692.0</td>\n",
       "      <td>1502940.00</td>\n",
       "      <td>1507299.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>2.940755e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>1.361944</td>\n",
       "      <td>2.819137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>938.0</td>\n",
       "      <td>1.515145e+06</td>\n",
       "      <td>4.484366e+03</td>\n",
       "      <td>1507448.0</td>\n",
       "      <td>1511426.00</td>\n",
       "      <td>1515055.0</td>\n",
       "      <td>1519070.00</td>\n",
       "      <td>1522991.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>2.951091e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>1.377399</td>\n",
       "      <td>2.948581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1491.0</td>\n",
       "      <td>1.499127e+06</td>\n",
       "      <td>4.968108e+03</td>\n",
       "      <td>1490272.0</td>\n",
       "      <td>1494863.00</td>\n",
       "      <td>1499137.0</td>\n",
       "      <td>1503481.50</td>\n",
       "      <td>1507323.0</td>\n",
       "      <td>1491.0</td>\n",
       "      <td>2.931049e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1491.0</td>\n",
       "      <td>0.962441</td>\n",
       "      <td>2.626900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1551.0</td>\n",
       "      <td>1.515297e+06</td>\n",
       "      <td>4.483430e+03</td>\n",
       "      <td>1507335.0</td>\n",
       "      <td>1511508.00</td>\n",
       "      <td>1515445.0</td>\n",
       "      <td>1519092.00</td>\n",
       "      <td>1523011.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>2.869814e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>1.133462</td>\n",
       "      <td>2.897092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3041.0</td>\n",
       "      <td>1.498624e+06</td>\n",
       "      <td>4.954594e+03</td>\n",
       "      <td>1490260.0</td>\n",
       "      <td>1494283.00</td>\n",
       "      <td>1498494.0</td>\n",
       "      <td>1502946.00</td>\n",
       "      <td>1507327.0</td>\n",
       "      <td>3041.0</td>\n",
       "      <td>2.910851e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3041.0</td>\n",
       "      <td>0.851365</td>\n",
       "      <td>2.442013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>2765.0</td>\n",
       "      <td>1.515074e+06</td>\n",
       "      <td>4.538212e+03</td>\n",
       "      <td>1507330.0</td>\n",
       "      <td>1511033.00</td>\n",
       "      <td>1515106.0</td>\n",
       "      <td>1518964.00</td>\n",
       "      <td>1523008.0</td>\n",
       "      <td>2765.0</td>\n",
       "      <td>2.875697e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2765.0</td>\n",
       "      <td>0.922966</td>\n",
       "      <td>2.811144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10328.0</td>\n",
       "      <td>1.498708e+06</td>\n",
       "      <td>4.914360e+03</td>\n",
       "      <td>1490252.0</td>\n",
       "      <td>1494482.75</td>\n",
       "      <td>1498707.5</td>\n",
       "      <td>1502912.50</td>\n",
       "      <td>1507326.0</td>\n",
       "      <td>10328.0</td>\n",
       "      <td>2.924568e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>10328.0</td>\n",
       "      <td>0.965143</td>\n",
       "      <td>3.499858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>9172.0</td>\n",
       "      <td>1.515193e+06</td>\n",
       "      <td>4.538829e+03</td>\n",
       "      <td>1507328.0</td>\n",
       "      <td>1511290.75</td>\n",
       "      <td>1515188.5</td>\n",
       "      <td>1519158.75</td>\n",
       "      <td>1523016.0</td>\n",
       "      <td>9172.0</td>\n",
       "      <td>2.892707e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>9172.0</td>\n",
       "      <td>1.107174</td>\n",
       "      <td>4.221096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows  40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0                                                     \\\n",
       "                 count          mean           std        min         25%   \n",
       "star_rating                                                                 \n",
       "1             827639.0  2.266308e+06  1.641208e+06        0.0   855181.50   \n",
       "2             709339.0  2.400289e+06  1.595248e+06       14.0  1028330.50   \n",
       "3            1180827.0  2.392412e+06  1.583831e+06        4.0  1037452.50   \n",
       "4            2253164.0  2.423911e+06  1.579042e+06        0.0  1073383.75   \n",
       "5            7002219.0  2.348141e+06  1.586956e+06        0.0   983950.00   \n",
       "1               1247.0  1.499485e+06  4.869797e+03  1490251.0  1495374.50   \n",
       "1.0             1263.0  1.515104e+06  4.535193e+03  1507333.0  1511097.50   \n",
       "2                967.0  1.498753e+06  4.948805e+03  1490250.0  1494399.00   \n",
       "2.0              938.0  1.515145e+06  4.484366e+03  1507448.0  1511426.00   \n",
       "3               1491.0  1.499127e+06  4.968108e+03  1490272.0  1494863.00   \n",
       "3.0             1551.0  1.515297e+06  4.483430e+03  1507335.0  1511508.00   \n",
       "4               3041.0  1.498624e+06  4.954594e+03  1490260.0  1494283.00   \n",
       "4.0             2765.0  1.515074e+06  4.538212e+03  1507330.0  1511033.00   \n",
       "5              10328.0  1.498708e+06  4.914360e+03  1490252.0  1494482.75   \n",
       "5.0             9172.0  1.515193e+06  4.538829e+03  1507328.0  1511290.75   \n",
       "\n",
       "                                              customer_id                ...  \\\n",
       "                   50%         75%        max       count          mean  ...   \n",
       "star_rating                                                              ...   \n",
       "1            1863711.0  3583768.50  5881863.0    827639.0  2.480226e+07  ...   \n",
       "2            2192972.0  3660481.00  5881865.0    709339.0  2.620339e+07  ...   \n",
       "3            2184988.0  3629866.00  5881871.0   1180827.0  2.580105e+07  ...   \n",
       "4            2241395.0  3663239.50  5881873.0   2253164.0  2.579251e+07  ...   \n",
       "5            2098852.0  3610272.50  5881872.0   7002219.0  2.559842e+07  ...   \n",
       "1            1499834.0  1503816.00  1507319.0      1247.0  2.844994e+07  ...   \n",
       "1.0          1515042.0  1518961.50  1523017.0      1263.0  2.914515e+07  ...   \n",
       "2            1498692.0  1502940.00  1507299.0       967.0  2.940755e+07  ...   \n",
       "2.0          1515055.0  1519070.00  1522991.0       938.0  2.951091e+07  ...   \n",
       "3            1499137.0  1503481.50  1507323.0      1491.0  2.931049e+07  ...   \n",
       "3.0          1515445.0  1519092.00  1523011.0      1551.0  2.869814e+07  ...   \n",
       "4            1498494.0  1502946.00  1507327.0      3041.0  2.910851e+07  ...   \n",
       "4.0          1515106.0  1518964.00  1523008.0      2765.0  2.875697e+07  ...   \n",
       "5            1498707.5  1502912.50  1507326.0     10328.0  2.924568e+07  ...   \n",
       "5.0          1515188.5  1519158.75  1523016.0      9172.0  2.892707e+07  ...   \n",
       "\n",
       "            helpful_votes          total_votes                                 \\\n",
       "                      75%      max       count      mean        std  min  25%   \n",
       "star_rating                                                                     \n",
       "1                     1.0   5341.0    827639.0  2.255358  16.016836  0.0  0.0   \n",
       "2                     1.0    904.0    709339.0  1.413192   6.063569  0.0  0.0   \n",
       "3                     1.0    944.0   1180827.0  1.136638   5.707008  0.0  0.0   \n",
       "4                     0.0   1338.0   2253164.0  0.958812   6.071467  0.0  0.0   \n",
       "5                     0.0  41278.0   7002219.0  0.971093  19.313839  0.0  0.0   \n",
       "1                     2.0    119.0      1247.0  1.987169   5.235681  0.0  0.0   \n",
       "1.0                   2.0    112.0      1263.0  2.399842   6.254322  0.0  0.0   \n",
       "2                     1.0     28.0       967.0  1.361944   2.819137  0.0  0.0   \n",
       "2.0                   1.0     33.0       938.0  1.377399   2.948581  0.0  0.0   \n",
       "3                     1.0     50.0      1491.0  0.962441   2.626900  0.0  0.0   \n",
       "3.0                   1.0     48.0      1551.0  1.133462   2.897092  0.0  0.0   \n",
       "4                     1.0     50.0      3041.0  0.851365   2.442013  0.0  0.0   \n",
       "4.0                   1.0     75.0      2765.0  0.922966   2.811144  0.0  0.0   \n",
       "5                     1.0    143.0     10328.0  0.965143   3.499858  0.0  0.0   \n",
       "5.0                   1.0    200.0      9172.0  1.107174   4.221096  0.0  0.0   \n",
       "\n",
       "                                \n",
       "             50%  75%      max  \n",
       "star_rating                     \n",
       "1            0.0  2.0   5402.0  \n",
       "2            0.0  1.0    943.0  \n",
       "3            0.0  1.0    993.0  \n",
       "4            0.0  1.0   1434.0  \n",
       "5            0.0  1.0  41889.0  \n",
       "1            1.0  2.0    133.0  \n",
       "1.0          1.0  3.0    120.0  \n",
       "2            0.0  2.0     31.0  \n",
       "2.0          0.0  2.0     34.0  \n",
       "3            0.0  1.0     55.0  \n",
       "3.0          0.0  1.0     50.0  \n",
       "4            0.0  1.0     51.0  \n",
       "4.0          0.0  1.0     82.0  \n",
       "5            0.0  1.0    154.0  \n",
       "5.0          0.0  1.0    207.0  \n",
       "\n",
       "[15 rows x 40 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "# Describing the data according to the ratings\n",
    "df.groupby('star_rating').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'marketplace', 'customer_id', 'review_id', 'product_id',\n",
       "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
       "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
       "       'review_headline', 'review_body', 'review_date', 'catalog'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.44 ms\n"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "df = df[['customer_id', 'product_id', 'star_rating', 'product_parent', 'product_category', 'product_title', 'helpful_votes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12005951, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.25 ms\n"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_title</th>\n",
       "      <th>helpful_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3805737</th>\n",
       "      <td>30754068</td>\n",
       "      <td>B001UV3JI8</td>\n",
       "      <td>5</td>\n",
       "      <td>902384165</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Dansko Women's Beth Pump</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11114060</th>\n",
       "      <td>5206232</td>\n",
       "      <td>B00EPBAJBK</td>\n",
       "      <td>5</td>\n",
       "      <td>55137529</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Two-Tone Sterling Silver and Gold over Sterlin...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263515</th>\n",
       "      <td>49170958</td>\n",
       "      <td>B00KHGBQQY</td>\n",
       "      <td>4</td>\n",
       "      <td>834624607</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Blazin Roxx Women's Camo Sequin Slipper Booties</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367354</th>\n",
       "      <td>46305033</td>\n",
       "      <td>B002SCNDL6</td>\n",
       "      <td>5</td>\n",
       "      <td>114312586</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>SquareTrade 2-Year Jewelry Protection Plan ($1...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10779725</th>\n",
       "      <td>10571472</td>\n",
       "      <td>B007Z113NK</td>\n",
       "      <td>5</td>\n",
       "      <td>909060868</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>VINANI brand Germany 925 Sterling Silver Penda...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864926</th>\n",
       "      <td>3803727</td>\n",
       "      <td>B00IVIQ3HM</td>\n",
       "      <td>5</td>\n",
       "      <td>898100755</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Diesel Men's Stulip-Sho Shirt Pale/Grey 2XL</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718752</th>\n",
       "      <td>44983641</td>\n",
       "      <td>B004R9P6CW</td>\n",
       "      <td>5</td>\n",
       "      <td>538874711</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>HOBO INTERNATIONAL Teena Bucket Bag</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452894</th>\n",
       "      <td>5290263</td>\n",
       "      <td>B00HET0VBI</td>\n",
       "      <td>5</td>\n",
       "      <td>829494175</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Womens Ankle Strap Flat Sandal Shoes W/Peep To...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9683228</th>\n",
       "      <td>48051538</td>\n",
       "      <td>B001GFKE6S</td>\n",
       "      <td>5</td>\n",
       "      <td>637349598</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Kimono with Gold Calligraphy design (cotton Yu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866690</th>\n",
       "      <td>776563</td>\n",
       "      <td>B00SA4HTCK</td>\n",
       "      <td>5</td>\n",
       "      <td>787358218</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Skechers Performance Women's Go Walk Upstage S...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          customer_id  product_id star_rating  product_parent  \\\n",
       "3805737      30754068  B001UV3JI8           5       902384165   \n",
       "11114060      5206232  B00EPBAJBK           5        55137529   \n",
       "1263515      49170958  B00KHGBQQY           4       834624607   \n",
       "11367354     46305033  B002SCNDL6           5       114312586   \n",
       "10779725     10571472  B007Z113NK           5       909060868   \n",
       "5864926       3803727  B00IVIQ3HM           5       898100755   \n",
       "3718752      44983641  B004R9P6CW           5       538874711   \n",
       "2452894       5290263  B00HET0VBI           5       829494175   \n",
       "9683228      48051538  B001GFKE6S           5       637349598   \n",
       "866690         776563  B00SA4HTCK           5       787358218   \n",
       "\n",
       "         product_category                                      product_title  \\\n",
       "3805737             Shoes                           Dansko Women's Beth Pump   \n",
       "11114060          Jewelry  Two-Tone Sterling Silver and Gold over Sterlin...   \n",
       "1263515             Shoes    Blazin Roxx Women's Camo Sequin Slipper Booties   \n",
       "11367354          Jewelry  SquareTrade 2-Year Jewelry Protection Plan ($1...   \n",
       "10779725          Jewelry  VINANI brand Germany 925 Sterling Silver Penda...   \n",
       "5864926           Apparel        Diesel Men's Stulip-Sho Shirt Pale/Grey 2XL   \n",
       "3718752             Shoes                HOBO INTERNATIONAL Teena Bucket Bag   \n",
       "2452894             Shoes  Womens Ankle Strap Flat Sandal Shoes W/Peep To...   \n",
       "9683228           Apparel  Kimono with Gold Calligraphy design (cotton Yu...   \n",
       "866690              Shoes  Skechers Performance Women's Go Walk Upstage S...   \n",
       "\n",
       "          helpful_votes  \n",
       "3805737             0.0  \n",
       "11114060            2.0  \n",
       "1263515             0.0  \n",
       "11367354            0.0  \n",
       "10779725            0.0  \n",
       "5864926             0.0  \n",
       "3718752             1.0  \n",
       "2452894             1.0  \n",
       "9683228             0.0  \n",
       "866690              6.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 525 ms\n"
     ]
    }
   ],
   "source": [
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select voted review only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12005951, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.19 ms\n"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3043992, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 760 ms\n"
     ]
    }
   ],
   "source": [
    "df = df[df['helpful_votes'] > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8961959"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.12 ms\n"
     ]
    }
   ],
   "source": [
    "12005951-3043992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers\n",
      " 0.00      1.0\n",
      "0.01      1.0\n",
      "0.02      1.0\n",
      "0.03      1.0\n",
      "0.04      1.0\n",
      "0.05      1.0\n",
      "0.10      1.0\n",
      "0.25      1.0\n",
      "0.50      1.0\n",
      "0.75      1.0\n",
      "0.90      2.0\n",
      "0.95      3.0\n",
      "0.96      4.0\n",
      "0.97      4.0\n",
      "0.98      5.0\n",
      "0.99      7.0\n",
      "1.00    291.0\n",
      "Name: customer_id, dtype: float64\n",
      "products\n",
      " 0.00       1.0\n",
      "0.01       1.0\n",
      "0.02       1.0\n",
      "0.03       1.0\n",
      "0.04       1.0\n",
      "0.05       1.0\n",
      "0.10       1.0\n",
      "0.25       1.0\n",
      "0.50       1.0\n",
      "0.75       2.0\n",
      "0.90       3.0\n",
      "0.95       4.0\n",
      "0.96       5.0\n",
      "0.97       6.0\n",
      "0.98       8.0\n",
      "0.99      11.0\n",
      "1.00    1198.0\n",
      "Name: product_id, dtype: float64\n",
      "time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Clean\n",
    "\n",
    "#### As we can see, only about 1% of customers have rated 7 or more products, and only 1% of products have been rated by 11+ customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's filter out this long tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "customers = customers[customers >= 8]\n",
    "products = products[products >= 12]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20925, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.07 ms\n"
     ]
    }
   ],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 226 ms\n"
     ]
    }
   ],
   "source": [
    "reduced_df.to_csv('Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user 2.3M May 20 03:08 Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv\n",
      "time: 238 ms\n"
     ]
    }
   ],
   "source": [
    "!ls -alh Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "!rm -rf Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz\n",
    "!gzip Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.0G\n",
      "drwxrwxr-x 5 ec2-user ec2-user 4.0K May 20 03:08 .\n",
      "drwxrwxr-x 9 ec2-user ec2-user 4.0K May 20 01:54 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.5G May 19 23:15 Apparel_Jewelry_Shoes_df.csv.gz\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.5G May 19 20:57 Apparel_Jewelry_Shoes_df.tsv.gz\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 657K May 20 03:08 Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 133K May 20 03:08 Apparel_Jewelry_Shoes.ipynb\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 5.6M May 20 00:56 Apparel_Jewelry_Shoes_predictions_from_user.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 5.6M May 20 00:56 Apparel_Jewelry_Shoes_predictions_from_user.pickle\n",
      "drwxrwxr-x 3 ec2-user ec2-user 4.0K May 19 20:37 Archive\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K May 19 20:37 .ipynb_checkpoints\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K May 19 20:40 __pycache__\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 5.0K May 19 20:40 random_tuner.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 9.0K May 19 20:37 recommender.py\n",
      "time: 238 ms\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz to s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz\n",
      "time: 862 ms\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll recreate our customer and product lists since there are customers with more than 5 reviews, but all of their reviews are on products with less than 5 reviews (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 03:08:55     671859 Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz\n",
      "time: 776 ms\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_help_voted_And_cut_lognTail.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42 ms\n"
     ]
    }
   ],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll number each user and item, giving them their own sequential index.  This will allow us to hold the information in a sparse format where the sequential indices indicate the row and column in our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_title</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52181093</td>\n",
       "      <td>B008709MDG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>694981365</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>.925 Sterling Silver 6mm Round Cubic Zirconia ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2229</td>\n",
       "      <td>5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52181093</td>\n",
       "      <td>B004Z1OSRQ</td>\n",
       "      <td>5</td>\n",
       "      <td>609609182</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Sterling Silve Simulated Birthstone Round Crys...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2229</td>\n",
       "      <td>5531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52181093</td>\n",
       "      <td>B000673JT6</td>\n",
       "      <td>5</td>\n",
       "      <td>190442726</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Multicolor Amber and Sterling Silver Adjustabl...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2229</td>\n",
       "      <td>9846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16396959</td>\n",
       "      <td>B00BUDYV0G</td>\n",
       "      <td>5</td>\n",
       "      <td>262398244</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>MG Collection's HALEY Gold Studded Handbag Purse</td>\n",
       "      <td>39.0</td>\n",
       "      <td>335</td>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45195752</td>\n",
       "      <td>B00BUDYV0G</td>\n",
       "      <td>3</td>\n",
       "      <td>262398244</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>MG Collection's HALEY Gold Studded Handbag Purse</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id star_rating  product_parent product_category  \\\n",
       "0     52181093  B008709MDG         5.0       694981365          Jewelry   \n",
       "1     52181093  B004Z1OSRQ           5       609609182          Jewelry   \n",
       "2     52181093  B000673JT6           5       190442726          Jewelry   \n",
       "3     16396959  B00BUDYV0G           5       262398244            Shoes   \n",
       "4     45195752  B00BUDYV0G           3       262398244            Shoes   \n",
       "\n",
       "                                       product_title  helpful_votes  user  \\\n",
       "0  .925 Sterling Silver 6mm Round Cubic Zirconia ...            2.0  2229   \n",
       "1  Sterling Silve Simulated Birthstone Round Crys...           11.0  2229   \n",
       "2  Multicolor Amber and Sterling Silver Adjustabl...            2.0  2229   \n",
       "3   MG Collection's HALEY Gold Studded Handbag Purse           39.0   335   \n",
       "4   MG Collection's HALEY Gold Studded Handbag Purse            2.0  1178   \n",
       "\n",
       "   item  \n",
       "0  5676  \n",
       "1  5531  \n",
       "2  9846  \n",
       "3  2156  \n",
       "4  2156  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67.8 ms\n"
     ]
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0])})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20925, 9)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.21 ms\n"
     ]
    }
   ],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "Let's start by splitting in training and test sets.  This will allow us to estimate the model's accuracy on product our customers rated, but wasn't included in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 92.9 ms\n"
     ]
    }
   ],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert our Pandas DataFrames into MXNet NDArrays, use those to create a member of the SparseMatrixDataset class, and add that to an MXNet Data Iterator.  This process is the same for both test and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 502 ms\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['star_rating'].values, dtype=np.float32))\n",
    "test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['star_rating'].values, dtype=np.float32))\n",
    "\n",
    "train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\n",
    "test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Locally\n",
    "\n",
    "### Define Network\n",
    "\n",
    "Let's start by defining the neural network version of our matrix factorization task.  In this case, our network is quite simple.  The main components are:\n",
    "- [Embeddings](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding) which turn our indexes into dense vectors of fixed size.  In this case, 64.\n",
    "- [Dense layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dense) with ReLU activation.  Each dense layer has the same number of units as our number of embeddings.  Our ReLU activation here also adds some non-linearity to our matrix factorization.\n",
    "- [Dropout layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dropout) which can be used to prevent over-fitting.\n",
    "- Matrix multiplication of our user matrix and our item matrix to create an estimate of our rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.95 ms\n"
     ]
    }
   ],
   "source": [
    "# Matrix factorization\n",
    "class MFBlock(gluon.HybridBlock):\n",
    "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\n",
    "        super(MFBlock, self).__init__()\n",
    "        \n",
    "        self.max_users = max_users\n",
    "        self.max_items = max_items\n",
    "        self.dropout_p = dropout_p\n",
    "        self.num_emb = num_emb\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\n",
    "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\n",
    "            \n",
    "            self.dropout_user = gluon.nn.Dropout(dropout_p)\n",
    "            self.dropout_item = gluon.nn.Dropout(dropout_p)\n",
    "\n",
    "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            \n",
    "    def hybrid_forward(self, F, users, items):\n",
    "        a = self.user_embeddings(users)\n",
    "        a = self.dense_user(a)\n",
    "        \n",
    "        b = self.item_embeddings(items)\n",
    "        b = self.dense_item(b)\n",
    "\n",
    "        predictions = self.dropout_user(a) * self.dropout_item(b)     \n",
    "        predictions = F.sum(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 702 s\n"
     ]
    }
   ],
   "source": [
    "# print(net.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: plot Pages: 1 -->\n",
       "<svg width=\"281pt\" height=\"630pt\"\n",
       " viewBox=\"0.00 0.00 280.60 630.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 626)\">\n",
       "<title>plot</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-626 276.6018,-626 276.6018,4 -4,4\"/>\n",
       "<!-- user -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>user</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"#000000\" cx=\"47.6018\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"47.6018\" y=\"-25.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">user</text>\n",
       "</g>\n",
       "<!-- embedding0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>embedding0</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"94.6018,-152 .6018,-152 .6018,-94 94.6018,-94 94.6018,-152\"/>\n",
       "<text text-anchor=\"middle\" x=\"47.6018\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">embedding0</text>\n",
       "</g>\n",
       "<!-- embedding0&#45;&gt;user -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>embedding0&#45;&gt;user</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.6018,-83.6321C47.6018,-75.1148 47.6018,-66.2539 47.6018,-58.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47.6018,-93.7731 43.1019,-83.773 47.6018,-88.7731 47.6019,-83.7731 47.6019,-83.7731 47.6019,-83.7731 47.6018,-88.7731 52.1019,-83.7731 47.6018,-93.7731 47.6018,-93.7731\"/>\n",
       "</g>\n",
       "<!-- dropout0 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>dropout0</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"108.6018,-246 14.6018,-246 14.6018,-188 108.6018,-188 108.6018,-246\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.6018\" y=\"-213.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dropout0</text>\n",
       "</g>\n",
       "<!-- dropout0&#45;&gt;embedding0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>dropout0&#45;&gt;embedding0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M55.7385,-177.6321C54.47,-169.1148 53.1503,-160.2539 51.9521,-152.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"57.2489,-187.7731 51.3248,-178.5451 56.5123,-182.8276 55.7757,-177.8822 55.7757,-177.8822 55.7757,-177.8822 56.5123,-182.8276 60.2266,-177.2192 57.2489,-187.7731 57.2489,-187.7731\"/>\n",
       "</g>\n",
       "<!-- _mul0 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>_mul0</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"96.6018,-340 2.6018,-340 2.6018,-282 96.6018,-282 96.6018,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"49.6018\" y=\"-307.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">_mul0</text>\n",
       "</g>\n",
       "<!-- _mul0&#45;&gt;embedding0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>_mul0&#45;&gt;embedding0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M18.7064,-273.3286C13.3139,-264.8145 8.5075,-255.4761 5.6018,-246 -1.9553,-221.3548 -1.6912,-212.7246 5.6018,-188 9.3269,-175.3711 16.3549,-162.8858 23.6111,-152.2848\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"24.483,-281.874 15.1545,-276.1095 21.6828,-277.7316 18.8826,-273.5893 18.8826,-273.5893 18.8826,-273.5893 21.6828,-277.7316 22.6107,-271.0691 24.483,-281.874 24.483,-281.874\"/>\n",
       "</g>\n",
       "<!-- _mul0&#45;&gt;dropout0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>_mul0&#45;&gt;dropout0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M54.6275,-271.6321C55.7148,-263.1148 56.846,-254.2539 57.873,-246.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"53.3329,-281.7731 50.1355,-271.2837 53.9661,-276.8133 54.5993,-271.8536 54.5993,-271.8536 54.5993,-271.8536 53.9661,-276.8133 59.0631,-272.4235 53.3329,-281.7731 53.3329,-281.7731\"/>\n",
       "</g>\n",
       "<!-- item -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>item</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"#000000\" cx=\"178.6018\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"178.6018\" y=\"-25.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">item</text>\n",
       "</g>\n",
       "<!-- embedding1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>embedding1</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"225.6018,-152 131.6018,-152 131.6018,-94 225.6018,-94 225.6018,-152\"/>\n",
       "<text text-anchor=\"middle\" x=\"178.6018\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">embedding1</text>\n",
       "</g>\n",
       "<!-- embedding1&#45;&gt;item -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>embedding1&#45;&gt;item</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M178.6018,-83.6321C178.6018,-75.1148 178.6018,-66.2539 178.6018,-58.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"178.6018,-93.7731 174.1019,-83.773 178.6018,-88.7731 178.6019,-83.7731 178.6019,-83.7731 178.6019,-83.7731 178.6018,-88.7731 183.1019,-83.7731 178.6018,-93.7731 178.6018,-93.7731\"/>\n",
       "</g>\n",
       "<!-- dropout1 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>dropout1</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"272.6018,-246 178.6018,-246 178.6018,-188 272.6018,-188 272.6018,-246\"/>\n",
       "<text text-anchor=\"middle\" x=\"225.6018\" y=\"-213.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dropout1</text>\n",
       "</g>\n",
       "<!-- dropout1&#45;&gt;embedding1 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>dropout1&#45;&gt;embedding1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M206.4784,-178.7532C202.0495,-169.8954 197.4056,-160.6075 193.2062,-152.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.9883,-187.7731 202.4912,-180.8413 208.7522,-183.3009 206.5162,-178.8288 206.5162,-178.8288 206.5162,-178.8288 208.7522,-183.3009 210.5411,-176.8163 210.9883,-187.7731 210.9883,-187.7731\"/>\n",
       "</g>\n",
       "<!-- _mul1 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>_mul1</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"208.6018,-340 114.6018,-340 114.6018,-282 208.6018,-282 208.6018,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.6018\" y=\"-307.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">_mul1</text>\n",
       "</g>\n",
       "<!-- _mul1&#45;&gt;embedding1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>_mul1&#45;&gt;embedding1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M163.4442,-271.4237C164.7498,-247.2823 166.7751,-215.8179 169.6018,-188 170.7996,-176.2126 172.5095,-163.275 174.1178,-152.067\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"162.9116,-281.6246 158.9392,-271.4036 163.1724,-276.6314 163.4331,-271.6382 163.4331,-271.6382 163.4331,-271.6382 163.1724,-276.6314 167.927,-271.8729 162.9116,-281.6246 162.9116,-281.6246\"/>\n",
       "</g>\n",
       "<!-- _mul1&#45;&gt;dropout1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>_mul1&#45;&gt;dropout1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M187.2617,-273.3121C193.4065,-264.2868 199.8763,-254.7843 205.7149,-246.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"181.501,-281.7731 183.4092,-270.9745 184.315,-277.6401 187.1289,-273.5071 187.1289,-273.5071 187.1289,-273.5071 184.315,-277.6401 190.8486,-276.0397 181.501,-281.7731 181.501,-281.7731\"/>\n",
       "</g>\n",
       "<!-- _mul2 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>_mul2</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"152.6018,-434 58.6018,-434 58.6018,-376 152.6018,-376 152.6018,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.6018\" y=\"-401.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">_mul2</text>\n",
       "</g>\n",
       "<!-- _mul2&#45;&gt;_mul0 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>_mul2&#45;&gt;_mul0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M82.983,-367.0328C77.656,-358.091 72.0589,-348.6959 67.0028,-340.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.19,-375.7731 79.206,-369.4852 85.631,-371.4776 83.0719,-367.1821 83.0719,-367.1821 83.0719,-367.1821 85.631,-371.4776 86.9379,-364.8789 88.19,-375.7731 88.19,-375.7731\"/>\n",
       "</g>\n",
       "<!-- _mul2&#45;&gt;_mul1 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>_mul2&#45;&gt;_mul1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M128.2206,-367.0328C133.5476,-358.091 139.1447,-348.6959 144.2008,-340.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"123.0136,-375.7731 124.2657,-364.8789 125.5726,-371.4776 128.1317,-367.1821 128.1317,-367.1821 128.1317,-367.1821 125.5726,-371.4776 131.9976,-369.4852 123.0136,-375.7731 123.0136,-375.7731\"/>\n",
       "</g>\n",
       "<!-- dropout2 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>dropout2</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"152.6018,-528 58.6018,-528 58.6018,-470 152.6018,-470 152.6018,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.6018\" y=\"-495.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dropout2</text>\n",
       "</g>\n",
       "<!-- dropout2&#45;&gt;_mul2 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>dropout2&#45;&gt;_mul2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.6018,-459.6321C105.6018,-451.1148 105.6018,-442.2539 105.6018,-434.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.6018,-469.7731 101.1019,-459.773 105.6018,-464.7731 105.6019,-459.7731 105.6019,-459.7731 105.6019,-459.7731 105.6018,-464.7731 110.1019,-459.7731 105.6018,-469.7731 105.6018,-469.7731\"/>\n",
       "</g>\n",
       "<!-- flatten0 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>flatten0</title>\n",
       "<polygon fill=\"#fdb462\" stroke=\"#000000\" points=\"152.6018,-622 58.6018,-622 58.6018,-564 152.6018,-564 152.6018,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.6018\" y=\"-589.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">flatten0</text>\n",
       "</g>\n",
       "<!-- flatten0&#45;&gt;dropout2 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>flatten0&#45;&gt;dropout2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.6018,-553.6321C105.6018,-545.1148 105.6018,-536.2539 105.6018,-528.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.6018,-563.7731 101.1019,-553.773 105.6018,-558.7731 105.6019,-553.7731 105.6019,-553.7731 105.6019,-553.7731 105.6018,-558.7731 110.1019,-553.7731 105.6018,-563.7731 105.6018,-563.7731\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f6da749fa58>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 147 ms\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "user = mx.symbol.Variable('user')\n",
    "item = mx.symbol.Variable('item')\n",
    "score = mx.symbol.Variable('score')\n",
    "\n",
    "# Set dummy dimensions\n",
    "k = 64\n",
    "max_user = 100\n",
    "max_item = 50\n",
    "\n",
    "# user feature lookup\n",
    "user = mx.symbol.Embedding(data = user, input_dim = max_user, output_dim = k)\n",
    "user_drop = mx.symbol.Dropout(data = user)\n",
    "_user = user * user_drop\n",
    "# item feature lookup\n",
    "item = mx.symbol.Embedding(data = item, input_dim = max_item, output_dim = k)\n",
    "item_drop = mx.symbol.Dropout(data = item)\n",
    "_item = item * item_drop\n",
    "# user = mx.symbol.Dropout()\n",
    "\n",
    "# predict by the inner product, which is elementwise product and then sum\n",
    "net = _user * _item \n",
    "\n",
    "# net = mx.symbol.sum_axis(data = net, axis = 1)\n",
    "\n",
    "net = mx.symbol.Dropout(data = net)\n",
    "net = mx.symbol.Flatten(data = net)\n",
    "\n",
    "# loss layer\n",
    "# net = mx.symbol.LinearRegressionOutput(data = net, label = score)\n",
    "\n",
    "# Visualize your network\n",
    "mx.viz.plot_network(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.2 ms\n"
     ]
    }
   ],
   "source": [
    "num_embeddings = 64\n",
    "\n",
    "net = MFBlock(max_users=customer_index.shape[0], \n",
    "              max_items=product_index.shape[0],\n",
    "              num_emb=num_embeddings,\n",
    "              dropout_p=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MFBlock"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "type(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters\n",
    "\n",
    "Let's initialize network weights and set our optimization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 942 s\n"
     ]
    }
   ],
   "source": [
    "# Set optimization parameters\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mx.gpu():  gpu(0)\n",
      "time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize network parameters\n",
    "ctx = mx.gpu()\n",
    "print(\"mx.gpu(): \", ctx)\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=60),\n",
    "                                ctx=ctx,\n",
    "                                force_reinit=True)\n",
    "net.hybridize()\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n",
    "\n",
    "Let's define a function to carry out the training of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 644 s\n"
     ]
    }
   ],
   "source": [
    "train_mse_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.append>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.12 ms\n"
     ]
    }
   ],
   "source": [
    "train_mse_list.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.41 ms\n"
     ]
    }
   ],
   "source": [
    "def execute(train_iter, test_iter, net, epochs, ctx):\n",
    "    \n",
    "    loss_function = gluon.loss.L2Loss()\n",
    "    for e in range(epochs):\n",
    "\n",
    "    \n",
    "        print(\"epoch: {}\".format(e))\n",
    "        \n",
    "        for i, (user, item, label) in enumerate(train_iter):\n",
    "                user = user.as_in_context(ctx)\n",
    "                item = item.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                \n",
    "                with mx.autograd.record():\n",
    "                    output = net(user, item)               \n",
    "                    loss = loss_function(output, label)\n",
    "                    \n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "\n",
    "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\n",
    "                                                                   eval_net(train_iter, net, ctx, loss_function),\n",
    "                                                                   eval_net(test_iter, net, ctx, loss_function)))\n",
    "        \n",
    "       \n",
    "    print(\"end of training\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's also define a function which evaluates our network on a given dataset.  This is called by our `execute` function above to provide mean squared error values on our training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "def eval_net(data, net, ctx, loss_function):\n",
    "    acc = MSE()\n",
    "    for i, (user, item, label) in enumerate(data):\n",
    "        \n",
    "            user = user.as_in_context(ctx)\n",
    "            item = item.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            predictions = net(user, item).reshape((batch_size, 1))\n",
    "            acc.update(preds=[predictions], labels=[label])\n",
    "   \n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 3.7679208517074585. 3.764685344696045\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 2.0611860275268556. 2.0680049777030947\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 2.356699275970459. 2.350901699066162\n",
      "end of training\n",
      "CPU times: user 622 ms, sys: 82.8 ms, total: 705 ms\n",
      "Wall time: 642 ms\n",
      "time: 644 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and prediction in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 1.7117404341697693. 1.702070713043213\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 1.8563762426376342. 1.866938328742981\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 1.5899645566940308. 1.5651812553405762\n",
      "epoch: 3\n",
      "EPOCH 3: MSE ON TRAINING and TEST: 1.5203769965605303. 1.528638330372897\n",
      "epoch: 4\n",
      "EPOCH 4: MSE ON TRAINING and TEST: 1.4039780259132386. 1.4096571803092957\n",
      "epoch: 5\n",
      "EPOCH 5: MSE ON TRAINING and TEST: 1.3118886709213258. 1.3076252818107605\n",
      "epoch: 6\n",
      "EPOCH 6: MSE ON TRAINING and TEST: 1.278482437133789. 1.2903834342956544\n",
      "epoch: 7\n",
      "EPOCH 7: MSE ON TRAINING and TEST: 1.2089710474014281. 1.2046672821044921\n",
      "epoch: 8\n",
      "EPOCH 8: MSE ON TRAINING and TEST: 1.0999534249305725. 1.1035776615142823\n",
      "epoch: 9\n",
      "EPOCH 9: MSE ON TRAINING and TEST: 1.0699813485145568. 1.06204993724823\n",
      "epoch: 10\n",
      "EPOCH 10: MSE ON TRAINING and TEST: 1.018407333980907. 1.0180831172249534\n",
      "epoch: 11\n",
      "EPOCH 11: MSE ON TRAINING and TEST: 0.9465285956859588. 0.9503132402896881\n",
      "epoch: 12\n",
      "EPOCH 12: MSE ON TRAINING and TEST: 0.9510770916938782. 0.9496089816093445\n",
      "epoch: 13\n",
      "EPOCH 13: MSE ON TRAINING and TEST: 0.8538328246636824. 0.8531869232654572\n",
      "epoch: 14\n",
      "EPOCH 14: MSE ON TRAINING and TEST: 0.8908627748489379. 0.8867946743965149\n",
      "epoch: 15\n",
      "EPOCH 15: MSE ON TRAINING and TEST: 0.8032460272312164. 0.8078660547733307\n",
      "epoch: 16\n",
      "EPOCH 16: MSE ON TRAINING and TEST: 0.7954018175601959. 0.7896780073642731\n",
      "epoch: 17\n",
      "EPOCH 17: MSE ON TRAINING and TEST: 0.7541745960712433. 0.7538700645620172\n",
      "epoch: 18\n",
      "EPOCH 18: MSE ON TRAINING and TEST: 0.7798220872879028. 0.7826499760150909\n",
      "epoch: 19\n",
      "EPOCH 19: MSE ON TRAINING and TEST: 0.6878958106040954. 0.679536509513855\n",
      "epoch: 20\n",
      "EPOCH 20: MSE ON TRAINING and TEST: 0.7554594332521612. 0.752770620584488\n",
      "epoch: 21\n",
      "EPOCH 21: MSE ON TRAINING and TEST: 0.6083185911178589. 0.6090028762817383\n",
      "epoch: 22\n",
      "EPOCH 22: MSE ON TRAINING and TEST: 0.6177459955215454. 0.6149884760379791\n",
      "epoch: 23\n",
      "EPOCH 23: MSE ON TRAINING and TEST: 0.6633978068828583. 0.6625508606433869\n",
      "epoch: 24\n",
      "EPOCH 24: MSE ON TRAINING and TEST: 0.5984331846237183. 0.6002978519959883\n",
      "epoch: 25\n",
      "EPOCH 25: MSE ON TRAINING and TEST: 0.5784491956233978. 0.5763977468013763\n",
      "epoch: 26\n",
      "EPOCH 26: MSE ON TRAINING and TEST: 0.634015828371048. 0.6285171151161194\n",
      "epoch: 27\n",
      "EPOCH 27: MSE ON TRAINING and TEST: 0.5261123288761486. 0.526972821354866\n",
      "epoch: 28\n",
      "EPOCH 28: MSE ON TRAINING and TEST: 0.5604914844036102. 0.5596976399421691\n",
      "epoch: 29\n",
      "EPOCH 29: MSE ON TRAINING and TEST: 0.5602195978164672. 0.5604854822158813\n",
      "epoch: 30\n",
      "EPOCH 30: MSE ON TRAINING and TEST: 0.5064924982461062. 0.5060188634829088\n",
      "epoch: 31\n",
      "EPOCH 31: MSE ON TRAINING and TEST: 0.49801193475723265. 0.4981200397014618\n",
      "epoch: 32\n",
      "EPOCH 32: MSE ON TRAINING and TEST: 0.4869837611913681. 0.4872701793909073\n",
      "epoch: 33\n",
      "EPOCH 33: MSE ON TRAINING and TEST: 0.48183166682720185. 0.482223778963089\n",
      "epoch: 34\n",
      "EPOCH 34: MSE ON TRAINING and TEST: 0.49983463883399964. 0.49977205097675326\n",
      "epoch: 35\n",
      "EPOCH 35: MSE ON TRAINING and TEST: 0.43722591996192933. 0.43379141092300416\n",
      "epoch: 36\n",
      "EPOCH 36: MSE ON TRAINING and TEST: 0.46953247785568236. 0.47196972370147705\n",
      "epoch: 37\n",
      "EPOCH 37: MSE ON TRAINING and TEST: 0.44990085742690344. 0.4510288319804452\n",
      "epoch: 38\n",
      "EPOCH 38: MSE ON TRAINING and TEST: 0.46363693475723267. 0.4627643197774887\n",
      "epoch: 39\n",
      "EPOCH 39: MSE ON TRAINING and TEST: 0.4517577111721039. 0.4519389092922211\n",
      "epoch: 40\n",
      "EPOCH 40: MSE ON TRAINING and TEST: 0.4389887094497681. 0.43648357391357423\n",
      "epoch: 41\n",
      "EPOCH 41: MSE ON TRAINING and TEST: 0.45067536532878877. 0.45101426243782045\n",
      "epoch: 42\n",
      "EPOCH 42: MSE ON TRAINING and TEST: 0.4038623750209808. 0.40654155910015105\n",
      "epoch: 43\n",
      "EPOCH 43: MSE ON TRAINING and TEST: 0.4520785570144653. 0.45291217863559724\n",
      "epoch: 44\n",
      "EPOCH 44: MSE ON TRAINING and TEST: 0.4037467807531357. 0.4023145626891743\n",
      "epoch: 45\n",
      "EPOCH 45: MSE ON TRAINING and TEST: 0.39468683302402496. 0.39268673956394196\n",
      "epoch: 46\n",
      "EPOCH 46: MSE ON TRAINING and TEST: 0.3783443719148636. 0.3778362154960632\n",
      "epoch: 47\n",
      "EPOCH 47: MSE ON TRAINING and TEST: 0.37861398404294794. 0.3796254307031631\n",
      "epoch: 48\n",
      "EPOCH 48: MSE ON TRAINING and TEST: 0.3755664348602295. 0.37450330555438993\n",
      "epoch: 49\n",
      "EPOCH 49: MSE ON TRAINING and TEST: 0.3814052164554596. 0.3833146899938583\n",
      "epoch: 50\n",
      "EPOCH 50: MSE ON TRAINING and TEST: 0.34782399535179137. 0.3457668304443359\n",
      "epoch: 51\n",
      "EPOCH 51: MSE ON TRAINING and TEST: 0.3715300768613815. 0.3732247677716342\n",
      "epoch: 52\n",
      "EPOCH 52: MSE ON TRAINING and TEST: 0.3670467913150787. 0.36607215702533724\n",
      "epoch: 53\n",
      "EPOCH 53: MSE ON TRAINING and TEST: 0.3532498389482498. 0.35377506017684934\n",
      "epoch: 54\n",
      "EPOCH 54: MSE ON TRAINING and TEST: 0.3623513796112754. 0.3616669595241547\n",
      "epoch: 55\n",
      "EPOCH 55: MSE ON TRAINING and TEST: 0.37881745100021363. 0.37783476114273074\n",
      "epoch: 56\n",
      "EPOCH 56: MSE ON TRAINING and TEST: 0.329323872923851. 0.3276326060295105\n",
      "epoch: 57\n",
      "EPOCH 57: MSE ON TRAINING and TEST: 0.3428671032190323. 0.34116589426994326\n",
      "epoch: 58\n",
      "EPOCH 58: MSE ON TRAINING and TEST: 0.32785622477531434. 0.3278396725654602\n",
      "epoch: 59\n",
      "EPOCH 59: MSE ON TRAINING and TEST: 0.3287335127592087. 0.3326776474714279\n",
      "epoch: 60\n",
      "EPOCH 60: MSE ON TRAINING and TEST: 0.31136066019535064. 0.31080623865127566\n",
      "epoch: 61\n",
      "EPOCH 61: MSE ON TRAINING and TEST: 0.32575513422489166. 0.32438612878322604\n",
      "epoch: 62\n",
      "EPOCH 62: MSE ON TRAINING and TEST: 0.3055210500955582. 0.3055644422769547\n",
      "epoch: 63\n",
      "EPOCH 63: MSE ON TRAINING and TEST: 0.3432201474905014. 0.3431980609893799\n",
      "epoch: 64\n",
      "EPOCH 64: MSE ON TRAINING and TEST: 0.30237706953828986. 0.3020086884498596\n",
      "epoch: 65\n",
      "EPOCH 65: MSE ON TRAINING and TEST: 0.3181758612394333. 0.31899648904800415\n",
      "epoch: 66\n",
      "EPOCH 66: MSE ON TRAINING and TEST: 0.2846263527870178. 0.28299581706523896\n",
      "epoch: 67\n",
      "EPOCH 67: MSE ON TRAINING and TEST: 0.37855230569839476. 0.37900036871433257\n",
      "epoch: 68\n",
      "EPOCH 68: MSE ON TRAINING and TEST: 0.2702000916004181. 0.2715307533740997\n",
      "epoch: 69\n",
      "EPOCH 69: MSE ON TRAINING and TEST: 0.3344676196575165. 0.3307460993528366\n",
      "epoch: 70\n",
      "EPOCH 70: MSE ON TRAINING and TEST: 0.2920357584953308. 0.29154127538204194\n",
      "epoch: 71\n",
      "EPOCH 71: MSE ON TRAINING and TEST: 0.29774095795371314. 0.2988590327176181\n",
      "epoch: 72\n",
      "EPOCH 72: MSE ON TRAINING and TEST: 0.3126456826925278. 0.3128980487585068\n",
      "epoch: 73\n",
      "EPOCH 73: MSE ON TRAINING and TEST: 0.25048008412122724. 0.2489675223827362\n",
      "epoch: 74\n",
      "EPOCH 74: MSE ON TRAINING and TEST: 0.3266376717524095. 0.3277865171432495\n",
      "epoch: 75\n",
      "EPOCH 75: MSE ON TRAINING and TEST: 0.2818869322538376. 0.282357195019722\n",
      "epoch: 76\n",
      "EPOCH 76: MSE ON TRAINING and TEST: 0.26058472096920016. 0.2583465605974197\n",
      "epoch: 77\n",
      "EPOCH 77: MSE ON TRAINING and TEST: 0.30263393223285673. 0.3053117096424103\n",
      "epoch: 78\n",
      "EPOCH 78: MSE ON TRAINING and TEST: 0.25739540457725524. 0.2569639872420918\n",
      "epoch: 79\n",
      "EPOCH 79: MSE ON TRAINING and TEST: 0.2947973430156708. 0.2919798940420151\n",
      "epoch: 80\n",
      "EPOCH 80: MSE ON TRAINING and TEST: 0.25684445202350614. 0.2569157600402832\n",
      "epoch: 81\n",
      "EPOCH 81: MSE ON TRAINING and TEST: 0.27823499928821216. 0.2769045650959015\n",
      "epoch: 82\n",
      "EPOCH 82: MSE ON TRAINING and TEST: 0.2969542384147644. 0.2988888055086136\n",
      "epoch: 83\n",
      "EPOCH 83: MSE ON TRAINING and TEST: 0.25496221482753756. 0.2542448192834854\n",
      "epoch: 84\n",
      "EPOCH 84: MSE ON TRAINING and TEST: 0.2710621178150177. 0.2723173230886459\n",
      "epoch: 85\n",
      "EPOCH 85: MSE ON TRAINING and TEST: 0.26086286157369615. 0.2605516409332102\n",
      "epoch: 86\n",
      "EPOCH 86: MSE ON TRAINING and TEST: 0.2704684674739838. 0.26999142169952395\n",
      "epoch: 87\n",
      "EPOCH 87: MSE ON TRAINING and TEST: 0.25506546944379804. 0.2550701454281807\n",
      "epoch: 88\n",
      "EPOCH 88: MSE ON TRAINING and TEST: 0.24027527326887305. 0.24040410667657852\n",
      "epoch: 89\n",
      "EPOCH 89: MSE ON TRAINING and TEST: 0.2746165007352829. 0.2754469603300095\n",
      "epoch: 90\n",
      "EPOCH 90: MSE ON TRAINING and TEST: 0.2572971060872078. 0.25556466430425645\n",
      "epoch: 91\n",
      "EPOCH 91: MSE ON TRAINING and TEST: 0.24133132939988916. 0.2419013719667088\n",
      "epoch: 92\n",
      "EPOCH 92: MSE ON TRAINING and TEST: 0.2398861289024353. 0.2401415467262268\n",
      "epoch: 93\n",
      "EPOCH 93: MSE ON TRAINING and TEST: 0.22706483751535417. 0.22598380893468856\n",
      "epoch: 94\n",
      "EPOCH 94: MSE ON TRAINING and TEST: 0.2593084886670113. 0.25967664271593094\n",
      "epoch: 95\n",
      "EPOCH 95: MSE ON TRAINING and TEST: 0.23084288090467453. 0.23046089857816696\n",
      "epoch: 96\n",
      "EPOCH 96: MSE ON TRAINING and TEST: 0.2641121238470078. 0.26497899889945986\n",
      "epoch: 97\n",
      "EPOCH 97: MSE ON TRAINING and TEST: 0.2236200362443924. 0.22222457975149154\n",
      "epoch: 98\n",
      "EPOCH 98: MSE ON TRAINING and TEST: 0.23060393739830365. 0.23145989396355368\n",
      "epoch: 99\n",
      "EPOCH 99: MSE ON TRAINING and TEST: 0.26242380142211913. 0.26327069103717804\n",
      "end of training\n",
      "CPU times: user 17 s, sys: 1.36 s, total: 18.3 s\n",
      "Wall time: 19.8 s\n",
      "time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set optimization parameters\n",
    "epochs = 100\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0.\n",
    "\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Block.summary of MFBlock(\n",
       "  (user_embeddings): Embedding(10534 -> 64, float32)\n",
       "  (item_embeddings): Embedding(10242 -> 64, float32)\n",
       "  (dropout_user): Dropout(p = 0.5, axes=())\n",
       "  (dropout_item): Dropout(p = 0.5, axes=())\n",
       "  (dense_user): Dense(64 -> 64, Activation(relu))\n",
       "  (dense_item): Dense(64 -> 64, Activation(relu))\n",
       ")>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.35 ms\n"
     ]
    }
   ],
   "source": [
    "trained_net.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Validation\n",
    "\n",
    "We can see our training error going down, but our validation accuracy bounces around a bit.  Let's check how our model is predicting for an individual user.  We could pick randomly, but for this case, let's try user #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>u6_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>B0065SDTLS</td>\n",
       "      <td>5238</td>\n",
       "      <td>6.305015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686</th>\n",
       "      <td>B00RXI5EY4</td>\n",
       "      <td>7686</td>\n",
       "      <td>6.248839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>B00IE18AM2</td>\n",
       "      <td>2850</td>\n",
       "      <td>6.182717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>B00AZORUCW</td>\n",
       "      <td>10201</td>\n",
       "      <td>6.160579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>B00200K8KK</td>\n",
       "      <td>3540</td>\n",
       "      <td>6.135994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6744</th>\n",
       "      <td>B006ZNCODC</td>\n",
       "      <td>6744</td>\n",
       "      <td>6.071641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>B00C3EMFS6</td>\n",
       "      <td>3230</td>\n",
       "      <td>6.067893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8221</th>\n",
       "      <td>B0073VBBIY</td>\n",
       "      <td>8221</td>\n",
       "      <td>6.033623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>B000YYLKHE</td>\n",
       "      <td>5910</td>\n",
       "      <td>6.017969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4324</th>\n",
       "      <td>B0008MFDEI</td>\n",
       "      <td>4324</td>\n",
       "      <td>6.017260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>B0008G3KGM</td>\n",
       "      <td>6173</td>\n",
       "      <td>6.013392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>B00AWKBUF2</td>\n",
       "      <td>1322</td>\n",
       "      <td>5.996480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145</th>\n",
       "      <td>B0045E5KB6</td>\n",
       "      <td>10145</td>\n",
       "      <td>5.990868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>B005CVEVBG</td>\n",
       "      <td>5700</td>\n",
       "      <td>5.982640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>B008VEPDMC</td>\n",
       "      <td>9913</td>\n",
       "      <td>5.956645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7480</th>\n",
       "      <td>B00D037IRM</td>\n",
       "      <td>7480</td>\n",
       "      <td>5.948318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>B00FRIJA10</td>\n",
       "      <td>2213</td>\n",
       "      <td>5.944952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6877</th>\n",
       "      <td>B002ANRXQE</td>\n",
       "      <td>6877</td>\n",
       "      <td>5.900052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>B0046EJMAK</td>\n",
       "      <td>4225</td>\n",
       "      <td>5.895838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>B003UHUOW4</td>\n",
       "      <td>6059</td>\n",
       "      <td>5.887443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>B00D7IDIP6</td>\n",
       "      <td>5661</td>\n",
       "      <td>5.876620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>B005DNAYDW</td>\n",
       "      <td>3355</td>\n",
       "      <td>5.870559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>B00I41CR8K</td>\n",
       "      <td>9593</td>\n",
       "      <td>5.863083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>B001EJILKW</td>\n",
       "      <td>5984</td>\n",
       "      <td>5.849588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>B00GAWINLU</td>\n",
       "      <td>2696</td>\n",
       "      <td>5.827419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>B00BGF4IJC</td>\n",
       "      <td>3912</td>\n",
       "      <td>5.815767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>B00KW08I66</td>\n",
       "      <td>2975</td>\n",
       "      <td>5.815617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>B00C1BP9T8</td>\n",
       "      <td>3719</td>\n",
       "      <td>5.805032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7093</th>\n",
       "      <td>B002HOQY7K</td>\n",
       "      <td>7093</td>\n",
       "      <td>5.800601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>B003LMHHIW</td>\n",
       "      <td>8012</td>\n",
       "      <td>5.795860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>B00ALR6ZJM</td>\n",
       "      <td>229</td>\n",
       "      <td>2.581437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10159</th>\n",
       "      <td>B00MS4RNKG</td>\n",
       "      <td>10159</td>\n",
       "      <td>2.580465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>B0028JCS2E</td>\n",
       "      <td>8218</td>\n",
       "      <td>2.578832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>B0093KRA8I</td>\n",
       "      <td>1634</td>\n",
       "      <td>2.573797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>B00L4RZ0O4</td>\n",
       "      <td>1546</td>\n",
       "      <td>2.571727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>B0094G8XI2</td>\n",
       "      <td>1220</td>\n",
       "      <td>2.565691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>B0055FFM94</td>\n",
       "      <td>710</td>\n",
       "      <td>2.563601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>B00LWIAJ3M</td>\n",
       "      <td>3971</td>\n",
       "      <td>2.560508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>B0018OMIMK</td>\n",
       "      <td>1131</td>\n",
       "      <td>2.560162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>B00MNGK4SC</td>\n",
       "      <td>1280</td>\n",
       "      <td>2.557062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>B0069TG7KI</td>\n",
       "      <td>1688</td>\n",
       "      <td>2.535668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>B00GBT8QM8</td>\n",
       "      <td>2525</td>\n",
       "      <td>2.533018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>B00FRIJKQ0</td>\n",
       "      <td>4443</td>\n",
       "      <td>2.525148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>B0019WTVJY</td>\n",
       "      <td>1090</td>\n",
       "      <td>2.481442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>B007ZVE1XY</td>\n",
       "      <td>2864</td>\n",
       "      <td>2.481082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>B0013LSR3M</td>\n",
       "      <td>9101</td>\n",
       "      <td>2.465021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>B00EI4J9G0</td>\n",
       "      <td>4099</td>\n",
       "      <td>2.456750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>B00F2ZCQ28</td>\n",
       "      <td>4857</td>\n",
       "      <td>2.431052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>B0017KSRMA</td>\n",
       "      <td>4067</td>\n",
       "      <td>2.430842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>B008REY7B4</td>\n",
       "      <td>3090</td>\n",
       "      <td>2.413804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>B00DUZACIC</td>\n",
       "      <td>3015</td>\n",
       "      <td>2.412446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>B00887B7E0</td>\n",
       "      <td>3684</td>\n",
       "      <td>2.408190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>B00C2BZCD0</td>\n",
       "      <td>2147</td>\n",
       "      <td>2.396321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>B0058H04JW</td>\n",
       "      <td>1222</td>\n",
       "      <td>2.366128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>B00F9RNZGU</td>\n",
       "      <td>1604</td>\n",
       "      <td>2.349190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>B001EJN08K</td>\n",
       "      <td>1921</td>\n",
       "      <td>2.335151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6670</th>\n",
       "      <td>B003O2SLCA</td>\n",
       "      <td>6670</td>\n",
       "      <td>2.329344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8968</th>\n",
       "      <td>B00I5WNI18</td>\n",
       "      <td>8968</td>\n",
       "      <td>2.231607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>B004BP7R1U</td>\n",
       "      <td>1383</td>\n",
       "      <td>2.169455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222</th>\n",
       "      <td>B000W0ZZW6</td>\n",
       "      <td>9222</td>\n",
       "      <td>2.033525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10242 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id   item  u6_predictions\n",
       "5238   B0065SDTLS   5238        6.305015\n",
       "7686   B00RXI5EY4   7686        6.248839\n",
       "2850   B00IE18AM2   2850        6.182717\n",
       "10201  B00AZORUCW  10201        6.160579\n",
       "3540   B00200K8KK   3540        6.135994\n",
       "6744   B006ZNCODC   6744        6.071641\n",
       "3230   B00C3EMFS6   3230        6.067893\n",
       "8221   B0073VBBIY   8221        6.033623\n",
       "5910   B000YYLKHE   5910        6.017969\n",
       "4324   B0008MFDEI   4324        6.017260\n",
       "6173   B0008G3KGM   6173        6.013392\n",
       "1322   B00AWKBUF2   1322        5.996480\n",
       "10145  B0045E5KB6  10145        5.990868\n",
       "5700   B005CVEVBG   5700        5.982640\n",
       "9913   B008VEPDMC   9913        5.956645\n",
       "7480   B00D037IRM   7480        5.948318\n",
       "2213   B00FRIJA10   2213        5.944952\n",
       "6877   B002ANRXQE   6877        5.900052\n",
       "4225   B0046EJMAK   4225        5.895838\n",
       "6059   B003UHUOW4   6059        5.887443\n",
       "5661   B00D7IDIP6   5661        5.876620\n",
       "3355   B005DNAYDW   3355        5.870559\n",
       "9593   B00I41CR8K   9593        5.863083\n",
       "5984   B001EJILKW   5984        5.849588\n",
       "2696   B00GAWINLU   2696        5.827419\n",
       "3912   B00BGF4IJC   3912        5.815767\n",
       "2975   B00KW08I66   2975        5.815617\n",
       "3719   B00C1BP9T8   3719        5.805032\n",
       "7093   B002HOQY7K   7093        5.800601\n",
       "8012   B003LMHHIW   8012        5.795860\n",
       "...           ...    ...             ...\n",
       "229    B00ALR6ZJM    229        2.581437\n",
       "10159  B00MS4RNKG  10159        2.580465\n",
       "8218   B0028JCS2E   8218        2.578832\n",
       "1634   B0093KRA8I   1634        2.573797\n",
       "1546   B00L4RZ0O4   1546        2.571727\n",
       "1220   B0094G8XI2   1220        2.565691\n",
       "710    B0055FFM94    710        2.563601\n",
       "3971   B00LWIAJ3M   3971        2.560508\n",
       "1131   B0018OMIMK   1131        2.560162\n",
       "1280   B00MNGK4SC   1280        2.557062\n",
       "1688   B0069TG7KI   1688        2.535668\n",
       "2525   B00GBT8QM8   2525        2.533018\n",
       "4443   B00FRIJKQ0   4443        2.525148\n",
       "1090   B0019WTVJY   1090        2.481442\n",
       "2864   B007ZVE1XY   2864        2.481082\n",
       "9101   B0013LSR3M   9101        2.465021\n",
       "4099   B00EI4J9G0   4099        2.456750\n",
       "4857   B00F2ZCQ28   4857        2.431052\n",
       "4067   B0017KSRMA   4067        2.430842\n",
       "3090   B008REY7B4   3090        2.413804\n",
       "3015   B00DUZACIC   3015        2.412446\n",
       "3684   B00887B7E0   3684        2.408190\n",
       "2147   B00C2BZCD0   2147        2.396321\n",
       "1222   B0058H04JW   1222        2.366128\n",
       "1604   B00F9RNZGU   1604        2.349190\n",
       "1921   B001EJN08K   1921        2.335151\n",
       "6670   B003O2SLCA   6670        2.329344\n",
       "8968   B00I5WNI18   8968        2.231607\n",
       "1383   B004BP7R1U   1383        2.169455\n",
       "9222   B000W0ZZW6   9222        2.033525\n",
       "\n",
       "[10242 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 65.7 ms\n"
     ]
    }
   ],
   "source": [
    "product_index['u6_predictions'] = trained_net(nd.array([6] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u6_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this to the predictions for another user (we'll try user #7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>u6_predictions</th>\n",
       "      <th>u7_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>B005DNAYDW</td>\n",
       "      <td>3355</td>\n",
       "      <td>5.870559</td>\n",
       "      <td>6.312834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>B00K01FKLI</td>\n",
       "      <td>2641</td>\n",
       "      <td>5.153759</td>\n",
       "      <td>6.154777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>B00FFC56CK</td>\n",
       "      <td>9118</td>\n",
       "      <td>5.600296</td>\n",
       "      <td>6.154257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>B00200K8KK</td>\n",
       "      <td>3540</td>\n",
       "      <td>6.135994</td>\n",
       "      <td>6.137447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>B005GXUSQ2</td>\n",
       "      <td>3514</td>\n",
       "      <td>5.214823</td>\n",
       "      <td>6.053624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7480</th>\n",
       "      <td>B00D037IRM</td>\n",
       "      <td>7480</td>\n",
       "      <td>5.948318</td>\n",
       "      <td>6.036866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>B00OFHM37W</td>\n",
       "      <td>5191</td>\n",
       "      <td>5.478827</td>\n",
       "      <td>6.017517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>B0088X2IH4</td>\n",
       "      <td>206</td>\n",
       "      <td>5.498401</td>\n",
       "      <td>5.990711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>B0065SDTLS</td>\n",
       "      <td>5238</td>\n",
       "      <td>6.305015</td>\n",
       "      <td>5.989287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>B00NFV2U40</td>\n",
       "      <td>5882</td>\n",
       "      <td>5.770728</td>\n",
       "      <td>5.962039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>B00C3EMFS6</td>\n",
       "      <td>3230</td>\n",
       "      <td>6.067893</td>\n",
       "      <td>5.921239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>B00KRO0AJK</td>\n",
       "      <td>1560</td>\n",
       "      <td>5.238745</td>\n",
       "      <td>5.911787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>B00CKGB91G</td>\n",
       "      <td>6675</td>\n",
       "      <td>5.375556</td>\n",
       "      <td>5.910604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>B002G9U92C</td>\n",
       "      <td>9067</td>\n",
       "      <td>5.487152</td>\n",
       "      <td>5.906207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>B00CO97HZQ</td>\n",
       "      <td>1458</td>\n",
       "      <td>5.354620</td>\n",
       "      <td>5.835176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466</th>\n",
       "      <td>B008RL2X8Q</td>\n",
       "      <td>6466</td>\n",
       "      <td>5.547987</td>\n",
       "      <td>5.832728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686</th>\n",
       "      <td>B00RXI5EY4</td>\n",
       "      <td>7686</td>\n",
       "      <td>6.248839</td>\n",
       "      <td>5.832216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>B00IE18AM2</td>\n",
       "      <td>2850</td>\n",
       "      <td>6.182717</td>\n",
       "      <td>5.821761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8080</th>\n",
       "      <td>B0031Y8SPQ</td>\n",
       "      <td>8080</td>\n",
       "      <td>5.500389</td>\n",
       "      <td>5.811605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9354</th>\n",
       "      <td>B001KP6M4M</td>\n",
       "      <td>9354</td>\n",
       "      <td>4.962360</td>\n",
       "      <td>5.809180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>B000YYLKHE</td>\n",
       "      <td>5910</td>\n",
       "      <td>6.017969</td>\n",
       "      <td>5.808881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>B001TH7JZC</td>\n",
       "      <td>1698</td>\n",
       "      <td>5.422800</td>\n",
       "      <td>5.800221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9284</th>\n",
       "      <td>B00AE1J9K2</td>\n",
       "      <td>9284</td>\n",
       "      <td>5.522264</td>\n",
       "      <td>5.797209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>B00FO8VNOU</td>\n",
       "      <td>10037</td>\n",
       "      <td>5.602914</td>\n",
       "      <td>5.793911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>B008VEPDMC</td>\n",
       "      <td>9913</td>\n",
       "      <td>5.956645</td>\n",
       "      <td>5.792949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9358</th>\n",
       "      <td>B006E9CZE0</td>\n",
       "      <td>9358</td>\n",
       "      <td>5.004056</td>\n",
       "      <td>5.789439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>B005CVEVBG</td>\n",
       "      <td>5700</td>\n",
       "      <td>5.982640</td>\n",
       "      <td>5.783896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>B00AWKBUF2</td>\n",
       "      <td>1322</td>\n",
       "      <td>5.996480</td>\n",
       "      <td>5.781953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10092</th>\n",
       "      <td>B0040RG0BW</td>\n",
       "      <td>10092</td>\n",
       "      <td>5.175220</td>\n",
       "      <td>5.778448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>B003XH69PW</td>\n",
       "      <td>5071</td>\n",
       "      <td>4.893610</td>\n",
       "      <td>5.778280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>B000UJY3W2</td>\n",
       "      <td>1069</td>\n",
       "      <td>3.223467</td>\n",
       "      <td>2.671642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>B00EI4J9G0</td>\n",
       "      <td>4099</td>\n",
       "      <td>2.456750</td>\n",
       "      <td>2.663499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>B00GBT8QM8</td>\n",
       "      <td>2525</td>\n",
       "      <td>2.533018</td>\n",
       "      <td>2.663264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>B007HXMQXM</td>\n",
       "      <td>6428</td>\n",
       "      <td>3.137357</td>\n",
       "      <td>2.643669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10223</th>\n",
       "      <td>B001I9XWNE</td>\n",
       "      <td>10223</td>\n",
       "      <td>3.513884</td>\n",
       "      <td>2.640219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7087</th>\n",
       "      <td>B00BFZREW6</td>\n",
       "      <td>7087</td>\n",
       "      <td>2.847801</td>\n",
       "      <td>2.632975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3552</th>\n",
       "      <td>B0002M6OP6</td>\n",
       "      <td>3552</td>\n",
       "      <td>2.905818</td>\n",
       "      <td>2.620320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>B00K9OS2LG</td>\n",
       "      <td>8697</td>\n",
       "      <td>2.830197</td>\n",
       "      <td>2.610896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>B0013LSR3M</td>\n",
       "      <td>9101</td>\n",
       "      <td>2.465021</td>\n",
       "      <td>2.606248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>B0018OMIMK</td>\n",
       "      <td>1131</td>\n",
       "      <td>2.560162</td>\n",
       "      <td>2.597567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>B004IHNHQK</td>\n",
       "      <td>1478</td>\n",
       "      <td>2.608353</td>\n",
       "      <td>2.548534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>B00K6RSFCC</td>\n",
       "      <td>5364</td>\n",
       "      <td>3.122852</td>\n",
       "      <td>2.503456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>B0094G8XI2</td>\n",
       "      <td>1220</td>\n",
       "      <td>2.565691</td>\n",
       "      <td>2.500673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8968</th>\n",
       "      <td>B00I5WNI18</td>\n",
       "      <td>8968</td>\n",
       "      <td>2.231607</td>\n",
       "      <td>2.488419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>B00C2BZCD0</td>\n",
       "      <td>2147</td>\n",
       "      <td>2.396321</td>\n",
       "      <td>2.483181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>B0017KSRMA</td>\n",
       "      <td>4067</td>\n",
       "      <td>2.430842</td>\n",
       "      <td>2.447063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>B00DUZACIC</td>\n",
       "      <td>3015</td>\n",
       "      <td>2.412446</td>\n",
       "      <td>2.445989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>B008REY7B4</td>\n",
       "      <td>3090</td>\n",
       "      <td>2.413804</td>\n",
       "      <td>2.442016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>B0019WTVJY</td>\n",
       "      <td>1090</td>\n",
       "      <td>2.481442</td>\n",
       "      <td>2.425231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6670</th>\n",
       "      <td>B003O2SLCA</td>\n",
       "      <td>6670</td>\n",
       "      <td>2.329344</td>\n",
       "      <td>2.401949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>B00FRIJKQ0</td>\n",
       "      <td>4443</td>\n",
       "      <td>2.525148</td>\n",
       "      <td>2.395919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234</th>\n",
       "      <td>B0082TV3PM</td>\n",
       "      <td>8234</td>\n",
       "      <td>2.637444</td>\n",
       "      <td>2.368610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>B0055FFM94</td>\n",
       "      <td>710</td>\n",
       "      <td>2.563601</td>\n",
       "      <td>2.350166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>B005GUPUTA</td>\n",
       "      <td>1068</td>\n",
       "      <td>2.651070</td>\n",
       "      <td>2.293006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>B007ZVE1XY</td>\n",
       "      <td>2864</td>\n",
       "      <td>2.481082</td>\n",
       "      <td>2.280531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>B00F2ZCQ28</td>\n",
       "      <td>4857</td>\n",
       "      <td>2.431052</td>\n",
       "      <td>2.254833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>B00FDH2A9O</td>\n",
       "      <td>4180</td>\n",
       "      <td>2.759043</td>\n",
       "      <td>2.149966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222</th>\n",
       "      <td>B000W0ZZW6</td>\n",
       "      <td>9222</td>\n",
       "      <td>2.033525</td>\n",
       "      <td>2.023126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>B001EJN08K</td>\n",
       "      <td>1921</td>\n",
       "      <td>2.335151</td>\n",
       "      <td>1.980628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>B004BP7R1U</td>\n",
       "      <td>1383</td>\n",
       "      <td>2.169455</td>\n",
       "      <td>1.964991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10242 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id   item  u6_predictions  u7_predictions\n",
       "3355   B005DNAYDW   3355        5.870559        6.312834\n",
       "2641   B00K01FKLI   2641        5.153759        6.154777\n",
       "9118   B00FFC56CK   9118        5.600296        6.154257\n",
       "3540   B00200K8KK   3540        6.135994        6.137447\n",
       "3514   B005GXUSQ2   3514        5.214823        6.053624\n",
       "7480   B00D037IRM   7480        5.948318        6.036866\n",
       "5191   B00OFHM37W   5191        5.478827        6.017517\n",
       "206    B0088X2IH4    206        5.498401        5.990711\n",
       "5238   B0065SDTLS   5238        6.305015        5.989287\n",
       "5882   B00NFV2U40   5882        5.770728        5.962039\n",
       "3230   B00C3EMFS6   3230        6.067893        5.921239\n",
       "1560   B00KRO0AJK   1560        5.238745        5.911787\n",
       "6675   B00CKGB91G   6675        5.375556        5.910604\n",
       "9067   B002G9U92C   9067        5.487152        5.906207\n",
       "1458   B00CO97HZQ   1458        5.354620        5.835176\n",
       "6466   B008RL2X8Q   6466        5.547987        5.832728\n",
       "7686   B00RXI5EY4   7686        6.248839        5.832216\n",
       "2850   B00IE18AM2   2850        6.182717        5.821761\n",
       "8080   B0031Y8SPQ   8080        5.500389        5.811605\n",
       "9354   B001KP6M4M   9354        4.962360        5.809180\n",
       "5910   B000YYLKHE   5910        6.017969        5.808881\n",
       "1698   B001TH7JZC   1698        5.422800        5.800221\n",
       "9284   B00AE1J9K2   9284        5.522264        5.797209\n",
       "10037  B00FO8VNOU  10037        5.602914        5.793911\n",
       "9913   B008VEPDMC   9913        5.956645        5.792949\n",
       "9358   B006E9CZE0   9358        5.004056        5.789439\n",
       "5700   B005CVEVBG   5700        5.982640        5.783896\n",
       "1322   B00AWKBUF2   1322        5.996480        5.781953\n",
       "10092  B0040RG0BW  10092        5.175220        5.778448\n",
       "5071   B003XH69PW   5071        4.893610        5.778280\n",
       "...           ...    ...             ...             ...\n",
       "1069   B000UJY3W2   1069        3.223467        2.671642\n",
       "4099   B00EI4J9G0   4099        2.456750        2.663499\n",
       "2525   B00GBT8QM8   2525        2.533018        2.663264\n",
       "6428   B007HXMQXM   6428        3.137357        2.643669\n",
       "10223  B001I9XWNE  10223        3.513884        2.640219\n",
       "7087   B00BFZREW6   7087        2.847801        2.632975\n",
       "3552   B0002M6OP6   3552        2.905818        2.620320\n",
       "8697   B00K9OS2LG   8697        2.830197        2.610896\n",
       "9101   B0013LSR3M   9101        2.465021        2.606248\n",
       "1131   B0018OMIMK   1131        2.560162        2.597567\n",
       "1478   B004IHNHQK   1478        2.608353        2.548534\n",
       "5364   B00K6RSFCC   5364        3.122852        2.503456\n",
       "1220   B0094G8XI2   1220        2.565691        2.500673\n",
       "8968   B00I5WNI18   8968        2.231607        2.488419\n",
       "2147   B00C2BZCD0   2147        2.396321        2.483181\n",
       "4067   B0017KSRMA   4067        2.430842        2.447063\n",
       "3015   B00DUZACIC   3015        2.412446        2.445989\n",
       "3090   B008REY7B4   3090        2.413804        2.442016\n",
       "1090   B0019WTVJY   1090        2.481442        2.425231\n",
       "6670   B003O2SLCA   6670        2.329344        2.401949\n",
       "4443   B00FRIJKQ0   4443        2.525148        2.395919\n",
       "8234   B0082TV3PM   8234        2.637444        2.368610\n",
       "710    B0055FFM94    710        2.563601        2.350166\n",
       "1068   B005GUPUTA   1068        2.651070        2.293006\n",
       "2864   B007ZVE1XY   2864        2.481082        2.280531\n",
       "4857   B00F2ZCQ28   4857        2.431052        2.254833\n",
       "4180   B00FDH2A9O   4180        2.759043        2.149966\n",
       "9222   B000W0ZZW6   9222        2.033525        2.023126\n",
       "1921   B001EJN08K   1921        2.335151        1.980628\n",
       "1383   B004BP7R1U   1383        2.169455        1.964991\n",
       "\n",
       "[10242 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.9 ms\n"
     ]
    }
   ],
   "source": [
    "product_index['u7_predictions'] = trained_net(nd.array([7] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u7_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted ratings are different between the two users, but the same top (and bottom) items for user #6 appear for #7 as well.  Let's look at the correlation across the full set of 38K items to see if this relationship holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 352 ms\n"
     ]
    }
   ],
   "source": [
    "product_index[['u6_predictions', 'u7_predictions']].plot.scatter('u6_predictions', 'u7_predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this correlation is nearly perfect.  Essentially the average rating of items dominates across users and we'll recommend the same well-reviewed items to everyone.  As it turns out, we can add more embeddings and this relationship will go away since we're better able to capture differential preferences across users.\n",
    "\n",
    "However, with just a 64 dimensional embedding, it took 7 minutes to run just 3 epochs.  If we ran this outside of our Notebook Instance we could run larger jobs and move on to other work would improve productivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for all users in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 0.24907028675079346. 0.2485296532511711\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 0.24842677712440492. 0.24876000434160234\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 0.23891172856092452. 0.2381374195218086\n",
      "epoch: 3\n",
      "EPOCH 3: MSE ON TRAINING and TEST: 0.23960698544979095. 0.24003330916166304\n",
      "epoch: 4\n",
      "EPOCH 4: MSE ON TRAINING and TEST: 0.23626965284347534. 0.23847542703151703\n",
      "epoch: 5\n",
      "EPOCH 5: MSE ON TRAINING and TEST: 0.22508383880962024. 0.224029465155168\n",
      "epoch: 6\n",
      "EPOCH 6: MSE ON TRAINING and TEST: 0.22322055995464324. 0.2231897696852684\n",
      "epoch: 7\n",
      "EPOCH 7: MSE ON TRAINING and TEST: 0.2350229561328888. 0.23562388718128205\n",
      "epoch: 8\n",
      "EPOCH 8: MSE ON TRAINING and TEST: 0.22506736083464188. 0.22534690499305726\n",
      "epoch: 9\n",
      "EPOCH 9: MSE ON TRAINING and TEST: 0.26205761134624483. 0.2622961729764938\n",
      "epoch: 10\n",
      "EPOCH 10: MSE ON TRAINING and TEST: 0.20537285953760148. 0.2040727972984314\n",
      "epoch: 11\n",
      "EPOCH 11: MSE ON TRAINING and TEST: 0.2570281535387039. 0.25651352405548095\n",
      "epoch: 12\n",
      "EPOCH 12: MSE ON TRAINING and TEST: 0.2274704858660698. 0.22774574837901376\n",
      "epoch: 13\n",
      "EPOCH 13: MSE ON TRAINING and TEST: 0.23284805417060853. 0.23210398256778716\n",
      "epoch: 14\n",
      "EPOCH 14: MSE ON TRAINING and TEST: 0.21265202015638351. 0.21391966193914413\n",
      "epoch: 15\n",
      "EPOCH 15: MSE ON TRAINING and TEST: 0.2247584340247241. 0.22449539452791215\n",
      "epoch: 16\n",
      "EPOCH 16: MSE ON TRAINING and TEST: 0.2616056874394417. 0.2622581422328949\n",
      "epoch: 17\n",
      "EPOCH 17: MSE ON TRAINING and TEST: 0.20626336634159087. 0.206129951775074\n",
      "epoch: 18\n",
      "EPOCH 18: MSE ON TRAINING and TEST: 0.24230225533246993. 0.2439337983727455\n",
      "epoch: 19\n",
      "EPOCH 19: MSE ON TRAINING and TEST: 0.23358158767223358. 0.2330276776443828\n",
      "epoch: 20\n",
      "EPOCH 20: MSE ON TRAINING and TEST: 0.2681140750646591. 0.2695493459701538\n",
      "epoch: 21\n",
      "EPOCH 21: MSE ON TRAINING and TEST: 0.20200316458940507. 0.20174529999494553\n",
      "epoch: 22\n",
      "EPOCH 22: MSE ON TRAINING and TEST: 0.2275116488337517. 0.22684732526540757\n",
      "epoch: 23\n",
      "EPOCH 23: MSE ON TRAINING and TEST: 0.21774738729000093. 0.2197812616825104\n",
      "epoch: 24\n",
      "EPOCH 24: MSE ON TRAINING and TEST: 0.2470193013548851. 0.24582747966051102\n",
      "epoch: 25\n",
      "EPOCH 25: MSE ON TRAINING and TEST: 0.1945061507550153. 0.19361550428650595\n",
      "epoch: 26\n",
      "EPOCH 26: MSE ON TRAINING and TEST: 0.24632624089717864. 0.24625857174396515\n",
      "epoch: 27\n",
      "EPOCH 27: MSE ON TRAINING and TEST: 0.2088342323899269. 0.20756344348192216\n",
      "epoch: 28\n",
      "EPOCH 28: MSE ON TRAINING and TEST: 0.23874001652002336. 0.23971263468265533\n",
      "epoch: 29\n",
      "EPOCH 29: MSE ON TRAINING and TEST: 0.21428870409727097. 0.21415905207395552\n",
      "epoch: 30\n",
      "EPOCH 30: MSE ON TRAINING and TEST: 0.22082343101501464. 0.2205436959862709\n",
      "epoch: 31\n",
      "EPOCH 31: MSE ON TRAINING and TEST: 0.253487628698349. 0.25320900082588194\n",
      "epoch: 32\n",
      "EPOCH 32: MSE ON TRAINING and TEST: 0.19337216155095535. 0.19288324361497705\n",
      "epoch: 33\n",
      "EPOCH 33: MSE ON TRAINING and TEST: 0.24263186901807784. 0.24227402359247208\n",
      "epoch: 34\n",
      "EPOCH 34: MSE ON TRAINING and TEST: 0.19706657528877258. 0.19459641724824905\n",
      "epoch: 35\n",
      "EPOCH 35: MSE ON TRAINING and TEST: 0.2309682235121727. 0.2313686802983284\n",
      "epoch: 36\n",
      "EPOCH 36: MSE ON TRAINING and TEST: 0.19799090325832366. 0.19655799269676208\n",
      "epoch: 37\n",
      "EPOCH 37: MSE ON TRAINING and TEST: 0.21048340648412706. 0.21071637570858\n",
      "epoch: 38\n",
      "EPOCH 38: MSE ON TRAINING and TEST: 0.22363518476486205. 0.22457476556301117\n",
      "epoch: 39\n",
      "EPOCH 39: MSE ON TRAINING and TEST: 0.20278251618146897. 0.20345515283671292\n",
      "epoch: 40\n",
      "EPOCH 40: MSE ON TRAINING and TEST: 0.200567889213562. 0.20054235458374023\n",
      "epoch: 41\n",
      "EPOCH 41: MSE ON TRAINING and TEST: 0.19735856354236603. 0.19765419661998748\n",
      "epoch: 42\n",
      "EPOCH 42: MSE ON TRAINING and TEST: 0.24201865223321048. 0.2420771151781082\n",
      "epoch: 43\n",
      "EPOCH 43: MSE ON TRAINING and TEST: 0.2313190370798111. 0.23195355832576753\n",
      "epoch: 44\n",
      "EPOCH 44: MSE ON TRAINING and TEST: 0.18314878791570663. 0.18284715116024017\n",
      "epoch: 45\n",
      "EPOCH 45: MSE ON TRAINING and TEST: 0.23297516256570816. 0.23298247009515763\n",
      "epoch: 46\n",
      "EPOCH 46: MSE ON TRAINING and TEST: 0.18709370493888855. 0.18691784143447876\n",
      "epoch: 47\n",
      "EPOCH 47: MSE ON TRAINING and TEST: 0.2379774883389473. 0.23701777458190917\n",
      "epoch: 48\n",
      "EPOCH 48: MSE ON TRAINING and TEST: 0.1722474217414856. 0.1721082001924515\n",
      "epoch: 49\n",
      "EPOCH 49: MSE ON TRAINING and TEST: 0.23326039585200223. 0.23350551277399062\n",
      "epoch: 50\n",
      "EPOCH 50: MSE ON TRAINING and TEST: 0.18327031284570694. 0.18395834267139435\n",
      "epoch: 51\n",
      "EPOCH 51: MSE ON TRAINING and TEST: 0.20505882203578948. 0.20510659813880922\n",
      "epoch: 52\n",
      "EPOCH 52: MSE ON TRAINING and TEST: 0.2042736980048093. 0.20362242649901996\n",
      "epoch: 53\n",
      "EPOCH 53: MSE ON TRAINING and TEST: 0.19115663468837737. 0.19042833298444747\n",
      "epoch: 54\n",
      "EPOCH 54: MSE ON TRAINING and TEST: 0.22017209082841874. 0.21979366838932038\n",
      "epoch: 55\n",
      "EPOCH 55: MSE ON TRAINING and TEST: 0.17314032465219498. 0.17200383692979812\n",
      "epoch: 56\n",
      "EPOCH 56: MSE ON TRAINING and TEST: 0.2033534377813339. 0.20415641814470292\n",
      "epoch: 57\n",
      "EPOCH 57: MSE ON TRAINING and TEST: 0.19429201781749725. 0.1925269514322281\n",
      "epoch: 58\n",
      "EPOCH 58: MSE ON TRAINING and TEST: 0.1884549543261528. 0.1873266205191612\n",
      "epoch: 59\n",
      "EPOCH 59: MSE ON TRAINING and TEST: 0.21724248745224692. 0.21743404052474283\n",
      "epoch: 60\n",
      "EPOCH 60: MSE ON TRAINING and TEST: 0.1929246038198471. 0.19371989816427232\n",
      "epoch: 61\n",
      "EPOCH 61: MSE ON TRAINING and TEST: 0.20009536743164064. 0.1986558258533478\n",
      "epoch: 62\n",
      "EPOCH 62: MSE ON TRAINING and TEST: 0.20085070729255677. 0.1994012027978897\n",
      "epoch: 63\n",
      "EPOCH 63: MSE ON TRAINING and TEST: 0.19958489537239074. 0.19964857697486876\n",
      "epoch: 64\n",
      "EPOCH 64: MSE ON TRAINING and TEST: 0.21715168952941893. 0.21714143306016923\n",
      "epoch: 65\n",
      "EPOCH 65: MSE ON TRAINING and TEST: 0.21095816791057587. 0.21062284409999849\n",
      "epoch: 66\n",
      "EPOCH 66: MSE ON TRAINING and TEST: 0.20394544709812512. 0.20428348806771365\n",
      "epoch: 67\n",
      "EPOCH 67: MSE ON TRAINING and TEST: 0.21337658613920213. 0.21415406316518784\n",
      "epoch: 68\n",
      "EPOCH 68: MSE ON TRAINING and TEST: 0.16147441864013673. 0.16302990019321442\n",
      "epoch: 69\n",
      "EPOCH 69: MSE ON TRAINING and TEST: 0.20546623793515292. 0.20526187121868134\n",
      "epoch: 70\n",
      "EPOCH 70: MSE ON TRAINING and TEST: 0.23244558721780778. 0.23151905536651612\n",
      "epoch: 71\n",
      "EPOCH 71: MSE ON TRAINING and TEST: 0.1600891798734665. 0.1605520263314247\n",
      "epoch: 72\n",
      "EPOCH 72: MSE ON TRAINING and TEST: 0.18632542341947556. 0.18851443827152253\n",
      "epoch: 73\n",
      "EPOCH 73: MSE ON TRAINING and TEST: 0.19592541605234146. 0.19513018835674634\n",
      "epoch: 74\n",
      "EPOCH 74: MSE ON TRAINING and TEST: 0.20559138506650926. 0.20402678698301316\n",
      "epoch: 75\n",
      "EPOCH 75: MSE ON TRAINING and TEST: 0.19127022475004196. 0.19196935892105102\n",
      "epoch: 76\n",
      "EPOCH 76: MSE ON TRAINING and TEST: 0.19260281324386597. 0.19231465458869934\n",
      "epoch: 77\n",
      "EPOCH 77: MSE ON TRAINING and TEST: 0.20399858951568603. 0.20502739846706391\n",
      "epoch: 78\n",
      "EPOCH 78: MSE ON TRAINING and TEST: 0.16883208453655243. 0.16803057342767716\n",
      "epoch: 79\n",
      "EPOCH 79: MSE ON TRAINING and TEST: 0.23597093522548676. 0.23451147824525834\n",
      "epoch: 80\n",
      "EPOCH 80: MSE ON TRAINING and TEST: 0.16270358115434647. 0.1646804470907558\n",
      "epoch: 81\n",
      "EPOCH 81: MSE ON TRAINING and TEST: 0.17775199562311172. 0.17825645655393602\n",
      "epoch: 82\n",
      "EPOCH 82: MSE ON TRAINING and TEST: 0.18644275069236754. 0.18842583447694777\n",
      "epoch: 83\n",
      "EPOCH 83: MSE ON TRAINING and TEST: 0.20056183487176896. 0.20139494389295579\n",
      "epoch: 84\n",
      "EPOCH 84: MSE ON TRAINING and TEST: 0.16958785951137542. 0.169975845515728\n",
      "epoch: 85\n",
      "EPOCH 85: MSE ON TRAINING and TEST: 0.19648006558418274. 0.19654266387224198\n",
      "epoch: 86\n",
      "EPOCH 86: MSE ON TRAINING and TEST: 0.18891313807530838. 0.18844777616587552\n",
      "epoch: 87\n",
      "EPOCH 87: MSE ON TRAINING and TEST: 0.18385377675294876. 0.18339966833591462\n",
      "epoch: 88\n",
      "EPOCH 88: MSE ON TRAINING and TEST: 0.18449611514806746. 0.18439015746116638\n",
      "epoch: 89\n",
      "EPOCH 89: MSE ON TRAINING and TEST: 0.16394819021224977. 0.16432568728923796\n",
      "epoch: 90\n",
      "EPOCH 90: MSE ON TRAINING and TEST: 0.1937684014439583. 0.1930818259716034\n",
      "epoch: 91\n",
      "EPOCH 91: MSE ON TRAINING and TEST: 0.19880489259958267. 0.19938042312860488\n",
      "epoch: 92\n",
      "EPOCH 92: MSE ON TRAINING and TEST: 0.17251518368721008. 0.1712871566414833\n",
      "epoch: 93\n",
      "EPOCH 93: MSE ON TRAINING and TEST: 0.1814373474229466. 0.18080612475221808\n",
      "epoch: 94\n",
      "EPOCH 94: MSE ON TRAINING and TEST: 0.17830436080694198. 0.17780787944793702\n",
      "epoch: 95\n",
      "EPOCH 95: MSE ON TRAINING and TEST: 0.17250459492206574. 0.1728458434343338\n",
      "epoch: 96\n",
      "EPOCH 96: MSE ON TRAINING and TEST: 0.17320251166820527. 0.1730598658323288\n",
      "epoch: 97\n",
      "EPOCH 97: MSE ON TRAINING and TEST: 0.16964332461357118. 0.170443394780159\n",
      "epoch: 98\n",
      "EPOCH 98: MSE ON TRAINING and TEST: 0.21935425847768783. 0.2192196249961853\n",
      "epoch: 99\n",
      "EPOCH 99: MSE ON TRAINING and TEST: 0.14261301159858703. 0.14334083795547486\n",
      "epoch: 100\n",
      "EPOCH 100: MSE ON TRAINING and TEST: 0.1833879381418228. 0.18239939619194379\n",
      "epoch: 101\n",
      "EPOCH 101: MSE ON TRAINING and TEST: 0.19046992659568787. 0.19014460742473602\n",
      "epoch: 102\n",
      "EPOCH 102: MSE ON TRAINING and TEST: 0.13779079169034958. 0.1390822172164917\n",
      "epoch: 103\n",
      "EPOCH 103: MSE ON TRAINING and TEST: 0.19783760065382178. 0.19848971515893937\n",
      "epoch: 104\n",
      "EPOCH 104: MSE ON TRAINING and TEST: 0.16195827573537827. 0.16052750051021575\n",
      "epoch: 105\n",
      "EPOCH 105: MSE ON TRAINING and TEST: 0.18452685326337814. 0.18482180535793305\n",
      "epoch: 106\n",
      "EPOCH 106: MSE ON TRAINING and TEST: 0.18203005343675613. 0.18106778115034103\n",
      "epoch: 107\n",
      "EPOCH 107: MSE ON TRAINING and TEST: 0.19178036451339722. 0.19104222817854447\n",
      "epoch: 108\n",
      "EPOCH 108: MSE ON TRAINING and TEST: 0.1811034455895424. 0.1807301938533783\n",
      "epoch: 109\n",
      "EPOCH 109: MSE ON TRAINING and TEST: 0.197996886074543. 0.19706626385450363\n",
      "epoch: 110\n",
      "EPOCH 110: MSE ON TRAINING and TEST: 0.15370641377839175. 0.15407752692699433\n",
      "epoch: 111\n",
      "EPOCH 111: MSE ON TRAINING and TEST: 0.19198973774909972. 0.19108536392450332\n",
      "epoch: 112\n",
      "EPOCH 112: MSE ON TRAINING and TEST: 0.18498758971691132. 0.18667925298213958\n",
      "epoch: 113\n",
      "EPOCH 113: MSE ON TRAINING and TEST: 0.17820363938808442. 0.1780593976378441\n",
      "epoch: 114\n",
      "EPOCH 114: MSE ON TRAINING and TEST: 0.16234252154827117. 0.16171920028599826\n",
      "epoch: 115\n",
      "EPOCH 115: MSE ON TRAINING and TEST: 0.16207290738821029. 0.16240058839321136\n",
      "epoch: 116\n",
      "EPOCH 116: MSE ON TRAINING and TEST: 0.1731834441423416. 0.17397089600563048\n",
      "epoch: 117\n",
      "EPOCH 117: MSE ON TRAINING and TEST: 0.17876841276884078. 0.17953677475452423\n",
      "epoch: 118\n",
      "EPOCH 118: MSE ON TRAINING and TEST: 0.15865286588668823. 0.15790810585021972\n",
      "epoch: 119\n",
      "EPOCH 119: MSE ON TRAINING and TEST: 0.22503423690795898. 0.22519743740558623\n",
      "epoch: 120\n",
      "EPOCH 120: MSE ON TRAINING and TEST: 0.1573102189735933. 0.15705049444328656\n",
      "epoch: 121\n",
      "EPOCH 121: MSE ON TRAINING and TEST: 0.15978978127241134. 0.15959638804197313\n",
      "epoch: 122\n",
      "EPOCH 122: MSE ON TRAINING and TEST: 0.17390861362218857. 0.17538973093032836\n",
      "epoch: 123\n",
      "EPOCH 123: MSE ON TRAINING and TEST: 0.18281684666872025. 0.1810440018773079\n",
      "epoch: 124\n",
      "EPOCH 124: MSE ON TRAINING and TEST: 0.13617797046899796. 0.13692157566547394\n",
      "epoch: 125\n",
      "EPOCH 125: MSE ON TRAINING and TEST: 0.20023612231016158. 0.1979873850941658\n",
      "epoch: 126\n",
      "EPOCH 126: MSE ON TRAINING and TEST: 0.14271339923143386. 0.14204079359769822\n",
      "epoch: 127\n",
      "EPOCH 127: MSE ON TRAINING and TEST: 0.18619564446535977. 0.1856955804608085\n",
      "epoch: 128\n",
      "EPOCH 128: MSE ON TRAINING and TEST: 0.1555394172668457. 0.15531360805034639\n",
      "epoch: 129\n",
      "EPOCH 129: MSE ON TRAINING and TEST: 0.18337659537792206. 0.1819237634539604\n",
      "epoch: 130\n",
      "EPOCH 130: MSE ON TRAINING and TEST: 0.14769169823689896. 0.1479975998401642\n",
      "epoch: 131\n",
      "EPOCH 131: MSE ON TRAINING and TEST: 0.19548170566558837. 0.19379338175058364\n",
      "epoch: 132\n",
      "EPOCH 132: MSE ON TRAINING and TEST: 0.13394276350736617. 0.13497245013713838\n",
      "epoch: 133\n",
      "EPOCH 133: MSE ON TRAINING and TEST: 0.18557592183351518. 0.18502627611160277\n",
      "epoch: 134\n",
      "EPOCH 134: MSE ON TRAINING and TEST: 0.17768929600715638. 0.17757054486057974\n",
      "epoch: 135\n",
      "EPOCH 135: MSE ON TRAINING and TEST: 0.16221911758184432. 0.16235072910785675\n",
      "epoch: 136\n",
      "EPOCH 136: MSE ON TRAINING and TEST: 0.1759838193655014. 0.1769839569926262\n",
      "epoch: 137\n",
      "EPOCH 137: MSE ON TRAINING and TEST: 0.17649956183000046. 0.1766317218542099\n",
      "epoch: 138\n",
      "EPOCH 138: MSE ON TRAINING and TEST: 0.16699943095445632. 0.1649127036333084\n",
      "epoch: 139\n",
      "EPOCH 139: MSE ON TRAINING and TEST: 0.16070002168416977. 0.16115881949663163\n",
      "epoch: 140\n",
      "EPOCH 140: MSE ON TRAINING and TEST: 0.15660754293203355. 0.1569513887166977\n",
      "epoch: 141\n",
      "EPOCH 141: MSE ON TRAINING and TEST: 0.15607702136039733. 0.15591022914106195\n",
      "epoch: 142\n",
      "EPOCH 142: MSE ON TRAINING and TEST: 0.1611786365509033. 0.162651327252388\n",
      "epoch: 143\n",
      "EPOCH 143: MSE ON TRAINING and TEST: 0.15793168395757676. 0.15648898780345916\n",
      "epoch: 144\n",
      "EPOCH 144: MSE ON TRAINING and TEST: 0.18995162844657898. 0.19132114052772523\n",
      "epoch: 145\n",
      "EPOCH 145: MSE ON TRAINING and TEST: 0.1480153724551201. 0.14783771932125092\n",
      "epoch: 146\n",
      "EPOCH 146: MSE ON TRAINING and TEST: 0.18475383371114731. 0.18501464575529097\n",
      "epoch: 147\n",
      "EPOCH 147: MSE ON TRAINING and TEST: 0.16624765639955347. 0.16589924828572708\n",
      "epoch: 148\n",
      "EPOCH 148: MSE ON TRAINING and TEST: 0.14347986727952958. 0.143442502617836\n",
      "epoch: 149\n",
      "EPOCH 149: MSE ON TRAINING and TEST: 0.16528114080429077. 0.16584827601909638\n",
      "epoch: 150\n",
      "EPOCH 150: MSE ON TRAINING and TEST: 0.17720333635807037. 0.17721399366855622\n",
      "epoch: 151\n",
      "EPOCH 151: MSE ON TRAINING and TEST: 0.15763169378042222. 0.15731956213712692\n",
      "epoch: 152\n",
      "EPOCH 152: MSE ON TRAINING and TEST: 0.16625443398952483. 0.1673559367656708\n",
      "epoch: 153\n",
      "EPOCH 153: MSE ON TRAINING and TEST: 0.15042293965816497. 0.15028240531682968\n",
      "epoch: 154\n",
      "EPOCH 154: MSE ON TRAINING and TEST: 0.15655437789180063. 0.15666325796734204\n",
      "epoch: 155\n",
      "EPOCH 155: MSE ON TRAINING and TEST: 0.1635837122797966. 0.16309988349676133\n",
      "epoch: 156\n",
      "EPOCH 156: MSE ON TRAINING and TEST: 0.15540367513895034. 0.1557295024394989\n",
      "epoch: 157\n",
      "EPOCH 157: MSE ON TRAINING and TEST: 0.16417538970708848. 0.16530755907297134\n",
      "epoch: 158\n",
      "EPOCH 158: MSE ON TRAINING and TEST: 0.17187861502170562. 0.1704764574766159\n",
      "epoch: 159\n",
      "EPOCH 159: MSE ON TRAINING and TEST: 0.16377077102661133. 0.16467218548059465\n",
      "epoch: 160\n",
      "EPOCH 160: MSE ON TRAINING and TEST: 0.19885820150375366. 0.19858432859182357\n",
      "epoch: 161\n",
      "EPOCH 161: MSE ON TRAINING and TEST: 0.15009908825159074. 0.15007446164434607\n",
      "epoch: 162\n",
      "EPOCH 162: MSE ON TRAINING and TEST: 0.15165754705667495. 0.1518130734562874\n",
      "epoch: 163\n",
      "EPOCH 163: MSE ON TRAINING and TEST: 0.15849753916263581. 0.15829868912696837\n",
      "epoch: 164\n",
      "EPOCH 164: MSE ON TRAINING and TEST: 0.13371154259551654. 0.1340036064386368\n",
      "epoch: 165\n",
      "EPOCH 165: MSE ON TRAINING and TEST: 0.2159689947962761. 0.21596481949090957\n",
      "epoch: 166\n",
      "EPOCH 166: MSE ON TRAINING and TEST: 0.14242605715990067. 0.1409383788704872\n",
      "epoch: 167\n",
      "EPOCH 167: MSE ON TRAINING and TEST: 0.17484909147024155. 0.17530752420425416\n",
      "epoch: 168\n",
      "EPOCH 168: MSE ON TRAINING and TEST: 0.16798937916755677. 0.16780825501138513\n",
      "epoch: 169\n",
      "EPOCH 169: MSE ON TRAINING and TEST: 0.1520790621638298. 0.15109920650720596\n",
      "epoch: 170\n",
      "EPOCH 170: MSE ON TRAINING and TEST: 0.16401937007904052. 0.1644668996334076\n",
      "epoch: 171\n",
      "EPOCH 171: MSE ON TRAINING and TEST: 0.1571975282647393. 0.1568990409374237\n",
      "epoch: 172\n",
      "EPOCH 172: MSE ON TRAINING and TEST: 0.1819680690765381. 0.1800386905670166\n",
      "epoch: 173\n",
      "EPOCH 173: MSE ON TRAINING and TEST: 0.13732487857341766. 0.13898109048604965\n",
      "epoch: 174\n",
      "EPOCH 174: MSE ON TRAINING and TEST: 0.17230420261621476. 0.17219015657901765\n",
      "epoch: 175\n",
      "EPOCH 175: MSE ON TRAINING and TEST: 0.1467020869255066. 0.1458453806963834\n",
      "epoch: 176\n",
      "EPOCH 176: MSE ON TRAINING and TEST: 0.17772987484931946. 0.1780621364712715\n",
      "epoch: 177\n",
      "EPOCH 177: MSE ON TRAINING and TEST: 0.17507219165563584. 0.17335072308778762\n",
      "epoch: 178\n",
      "EPOCH 178: MSE ON TRAINING and TEST: 0.13849211037158965. 0.138157020509243\n",
      "epoch: 179\n",
      "EPOCH 179: MSE ON TRAINING and TEST: 0.17530427277088165. 0.175030979514122\n",
      "epoch: 180\n",
      "EPOCH 180: MSE ON TRAINING and TEST: 0.16412990242242814. 0.16372118294239044\n",
      "epoch: 181\n",
      "EPOCH 181: MSE ON TRAINING and TEST: 0.15544894473119217. 0.1551987108859149\n",
      "epoch: 182\n",
      "EPOCH 182: MSE ON TRAINING and TEST: 0.1470586061477661. 0.1470186620950699\n",
      "epoch: 183\n",
      "EPOCH 183: MSE ON TRAINING and TEST: 0.15660310089588164. 0.15624397099018097\n",
      "epoch: 184\n",
      "EPOCH 184: MSE ON TRAINING and TEST: 0.19261111468076705. 0.19560734629631044\n",
      "epoch: 185\n",
      "EPOCH 185: MSE ON TRAINING and TEST: 0.1245400682091713. 0.12459141612052918\n",
      "epoch: 186\n",
      "EPOCH 186: MSE ON TRAINING and TEST: 0.15865911096334456. 0.15879907757043837\n",
      "epoch: 187\n",
      "EPOCH 187: MSE ON TRAINING and TEST: 0.17051606327295304. 0.17130475342273713\n",
      "epoch: 188\n",
      "EPOCH 188: MSE ON TRAINING and TEST: 0.18456831168044696. 0.18481531197374518\n",
      "epoch: 189\n",
      "EPOCH 189: MSE ON TRAINING and TEST: 0.1535600945353508. 0.15332192331552505\n",
      "epoch: 190\n",
      "EPOCH 190: MSE ON TRAINING and TEST: 0.1269514389336109. 0.1278109259903431\n",
      "epoch: 191\n",
      "EPOCH 191: MSE ON TRAINING and TEST: 0.15965970808809454. 0.1601867526769638\n",
      "epoch: 192\n",
      "EPOCH 192: MSE ON TRAINING and TEST: 0.16957084983587264. 0.1684267520904541\n",
      "epoch: 193\n",
      "EPOCH 193: MSE ON TRAINING and TEST: 0.1642976686358452. 0.16420216411352156\n",
      "epoch: 194\n",
      "EPOCH 194: MSE ON TRAINING and TEST: 0.1528458908200264. 0.15303882211446762\n",
      "epoch: 195\n",
      "EPOCH 195: MSE ON TRAINING and TEST: 0.14355682730674743. 0.14385430921207776\n",
      "epoch: 196\n",
      "EPOCH 196: MSE ON TRAINING and TEST: 0.1618632197380066. 0.16199442893266677\n",
      "epoch: 197\n",
      "EPOCH 197: MSE ON TRAINING and TEST: 0.16886920630931854. 0.16926346719264984\n",
      "epoch: 198\n",
      "EPOCH 198: MSE ON TRAINING and TEST: 0.15749250894243066. 0.15730054229497908\n",
      "epoch: 199\n",
      "EPOCH 199: MSE ON TRAINING and TEST: 0.1597232088446617. 0.1596459448337555\n",
      "epoch: 200\n",
      "EPOCH 200: MSE ON TRAINING and TEST: 0.15982797145843505. 0.15946411937475205\n",
      "epoch: 201\n",
      "EPOCH 201: MSE ON TRAINING and TEST: 0.12788544073700905. 0.12665072083473206\n",
      "epoch: 202\n",
      "EPOCH 202: MSE ON TRAINING and TEST: 0.1411380335688591. 0.14114809307185086\n",
      "epoch: 203\n",
      "EPOCH 203: MSE ON TRAINING and TEST: 0.17834677994251252. 0.17791847884655\n",
      "epoch: 204\n",
      "EPOCH 204: MSE ON TRAINING and TEST: 0.13626248240470887. 0.1370047703385353\n",
      "epoch: 205\n",
      "EPOCH 205: MSE ON TRAINING and TEST: 0.18043040280992334. 0.18142469823360444\n",
      "epoch: 206\n",
      "EPOCH 206: MSE ON TRAINING and TEST: 0.12946284264326097. 0.12996907979249955\n",
      "epoch: 207\n",
      "EPOCH 207: MSE ON TRAINING and TEST: 0.19085384756326676. 0.19140473902225494\n",
      "epoch: 208\n",
      "EPOCH 208: MSE ON TRAINING and TEST: 0.12614880773154172. 0.12572036548094315\n",
      "epoch: 209\n",
      "EPOCH 209: MSE ON TRAINING and TEST: 0.16962939649820327. 0.1695174217224121\n",
      "epoch: 210\n",
      "EPOCH 210: MSE ON TRAINING and TEST: 0.12744035422801972. 0.12749015241861344\n",
      "epoch: 211\n",
      "EPOCH 211: MSE ON TRAINING and TEST: 0.13938270509243011. 0.13935731798410417\n",
      "epoch: 212\n",
      "EPOCH 212: MSE ON TRAINING and TEST: 0.1608974039554596. 0.16145417243242263\n",
      "epoch: 213\n",
      "EPOCH 213: MSE ON TRAINING and TEST: 0.16356011033058165. 0.1636180341243744\n",
      "epoch: 214\n",
      "EPOCH 214: MSE ON TRAINING and TEST: 0.13324786573648453. 0.13363137543201448\n",
      "epoch: 215\n",
      "EPOCH 215: MSE ON TRAINING and TEST: 0.1650689122351733. 0.1660325608470223\n",
      "epoch: 216\n",
      "EPOCH 216: MSE ON TRAINING and TEST: 0.14039825648069382. 0.14015723615884781\n",
      "epoch: 217\n",
      "EPOCH 217: MSE ON TRAINING and TEST: 0.1592714488506317. 0.1585294187068939\n",
      "epoch: 218\n",
      "EPOCH 218: MSE ON TRAINING and TEST: 0.15470200330018996. 0.15548412650823593\n",
      "epoch: 219\n",
      "EPOCH 219: MSE ON TRAINING and TEST: 0.15970297604799272. 0.15983589142560958\n",
      "epoch: 220\n",
      "EPOCH 220: MSE ON TRAINING and TEST: 0.16730111688375474. 0.16677779257297515\n",
      "epoch: 221\n",
      "EPOCH 221: MSE ON TRAINING and TEST: 0.12007748186588288. 0.12014614939689636\n",
      "epoch: 222\n",
      "EPOCH 222: MSE ON TRAINING and TEST: 0.14388261870904404. 0.14378168230706995\n",
      "epoch: 223\n",
      "EPOCH 223: MSE ON TRAINING and TEST: 0.15700095146894455. 0.15762205123901368\n",
      "epoch: 224\n",
      "EPOCH 224: MSE ON TRAINING and TEST: 0.14266560673713685. 0.14247069954872132\n",
      "epoch: 225\n",
      "EPOCH 225: MSE ON TRAINING and TEST: 0.16385990245775742. 0.16498291492462158\n",
      "epoch: 226\n",
      "EPOCH 226: MSE ON TRAINING and TEST: 0.1606006756424904. 0.15999984741210938\n",
      "epoch: 227\n",
      "EPOCH 227: MSE ON TRAINING and TEST: 0.1485041171312332. 0.14886126667261124\n",
      "epoch: 228\n",
      "EPOCH 228: MSE ON TRAINING and TEST: 0.1571975275874138. 0.15636834055185317\n",
      "epoch: 229\n",
      "EPOCH 229: MSE ON TRAINING and TEST: 0.11226706877350807. 0.11193598129532555\n",
      "epoch: 230\n",
      "EPOCH 230: MSE ON TRAINING and TEST: 0.1837775856256485. 0.18336839973926544\n",
      "epoch: 231\n",
      "EPOCH 231: MSE ON TRAINING and TEST: 0.12508627846837045. 0.12474444061517716\n",
      "epoch: 232\n",
      "EPOCH 232: MSE ON TRAINING and TEST: 0.16182624345475977. 0.16171717196702956\n",
      "epoch: 233\n",
      "EPOCH 233: MSE ON TRAINING and TEST: 0.13103332445025445. 0.13174408972263335\n",
      "epoch: 234\n",
      "EPOCH 234: MSE ON TRAINING and TEST: 0.160721555352211. 0.16055146008729934\n",
      "epoch: 235\n",
      "EPOCH 235: MSE ON TRAINING and TEST: 0.12911437898874284. 0.13015949353575706\n",
      "epoch: 236\n",
      "EPOCH 236: MSE ON TRAINING and TEST: 0.13603894263505936. 0.13541769304058768\n",
      "epoch: 237\n",
      "EPOCH 237: MSE ON TRAINING and TEST: 0.1678729847073555. 0.1680453673005104\n",
      "epoch: 238\n",
      "EPOCH 238: MSE ON TRAINING and TEST: 0.11638440191745758. 0.11610590070486068\n",
      "epoch: 239\n",
      "EPOCH 239: MSE ON TRAINING and TEST: 0.12112337127327918. 0.12099865451455116\n",
      "epoch: 240\n",
      "EPOCH 240: MSE ON TRAINING and TEST: 0.18648047000169754. 0.18568530827760696\n",
      "epoch: 241\n",
      "EPOCH 241: MSE ON TRAINING and TEST: 0.10883871093392372. 0.10999001264572143\n",
      "epoch: 242\n",
      "EPOCH 242: MSE ON TRAINING and TEST: 0.18029532513835214. 0.17976198142225092\n",
      "epoch: 243\n",
      "EPOCH 243: MSE ON TRAINING and TEST: 0.16070614457130433. 0.16058283299207687\n",
      "epoch: 244\n",
      "EPOCH 244: MSE ON TRAINING and TEST: 0.1694195345044136. 0.169312983751297\n",
      "epoch: 245\n",
      "EPOCH 245: MSE ON TRAINING and TEST: 0.14411624073982238. 0.1447647050023079\n",
      "epoch: 246\n",
      "EPOCH 246: MSE ON TRAINING and TEST: 0.12616459801793098. 0.1263272136449814\n",
      "epoch: 247\n",
      "EPOCH 247: MSE ON TRAINING and TEST: 0.15107968598604202. 0.15000321120023727\n",
      "epoch: 248\n",
      "EPOCH 248: MSE ON TRAINING and TEST: 0.15775531381368638. 0.15686124712228774\n",
      "epoch: 249\n",
      "EPOCH 249: MSE ON TRAINING and TEST: 0.1688606549393047. 0.16876719892024994\n",
      "epoch: 250\n",
      "EPOCH 250: MSE ON TRAINING and TEST: 0.12236492782831192. 0.12230115681886673\n",
      "epoch: 251\n",
      "EPOCH 251: MSE ON TRAINING and TEST: 0.15553027838468553. 0.1558910846710205\n",
      "epoch: 252\n",
      "EPOCH 252: MSE ON TRAINING and TEST: 0.1318065941333771. 0.1308773897588253\n",
      "epoch: 253\n",
      "EPOCH 253: MSE ON TRAINING and TEST: 0.1478837564587593. 0.1471000373363495\n",
      "epoch: 254\n",
      "EPOCH 254: MSE ON TRAINING and TEST: 0.15306951999664306. 0.15361370891332626\n",
      "epoch: 255\n",
      "EPOCH 255: MSE ON TRAINING and TEST: 0.13847324550151824. 0.13887790739536285\n",
      "epoch: 256\n",
      "EPOCH 256: MSE ON TRAINING and TEST: 0.12303054854273796. 0.1229961866682226\n",
      "epoch: 257\n",
      "EPOCH 257: MSE ON TRAINING and TEST: 0.15251120030879975. 0.1527504399418831\n",
      "epoch: 258\n",
      "EPOCH 258: MSE ON TRAINING and TEST: 0.16824901849031448. 0.16910445541143418\n",
      "epoch: 259\n",
      "EPOCH 259: MSE ON TRAINING and TEST: 0.12275986915284937. 0.12267054468393326\n",
      "epoch: 260\n",
      "EPOCH 260: MSE ON TRAINING and TEST: 0.1719588115811348. 0.17312761694192885\n",
      "epoch: 261\n",
      "EPOCH 261: MSE ON TRAINING and TEST: 0.1396732285618782. 0.139543317258358\n",
      "epoch: 262\n",
      "EPOCH 262: MSE ON TRAINING and TEST: 0.1288143962621689. 0.1291604682803154\n",
      "epoch: 263\n",
      "EPOCH 263: MSE ON TRAINING and TEST: 0.16067972928285598. 0.1602588092738932\n",
      "epoch: 264\n",
      "EPOCH 264: MSE ON TRAINING and TEST: 0.10912864357233047. 0.11007178276777267\n",
      "epoch: 265\n",
      "EPOCH 265: MSE ON TRAINING and TEST: 0.15967268496751785. 0.15790939182043076\n",
      "epoch: 266\n",
      "EPOCH 266: MSE ON TRAINING and TEST: 0.1654937362129038. 0.16606846153736116\n",
      "epoch: 267\n",
      "EPOCH 267: MSE ON TRAINING and TEST: 0.1129329264163971. 0.11260305643081665\n",
      "epoch: 268\n",
      "EPOCH 268: MSE ON TRAINING and TEST: 0.18103563636541367. 0.18186345398426057\n",
      "epoch: 269\n",
      "EPOCH 269: MSE ON TRAINING and TEST: 0.16914661635052075. 0.16856164011088284\n",
      "epoch: 270\n",
      "EPOCH 270: MSE ON TRAINING and TEST: 0.13282618969678878. 0.13231129571795464\n",
      "epoch: 271\n",
      "EPOCH 271: MSE ON TRAINING and TEST: 0.1715501144528389. 0.17118187248706818\n",
      "epoch: 272\n",
      "EPOCH 272: MSE ON TRAINING and TEST: 0.13555950671434402. 0.13564552515745162\n",
      "epoch: 273\n",
      "EPOCH 273: MSE ON TRAINING and TEST: 0.1342264547944069. 0.1339401736855507\n",
      "epoch: 274\n",
      "EPOCH 274: MSE ON TRAINING and TEST: 0.16986175775527954. 0.17130212485790253\n",
      "epoch: 275\n",
      "EPOCH 275: MSE ON TRAINING and TEST: 0.15081543773412703. 0.14958925247192384\n",
      "epoch: 276\n",
      "EPOCH 276: MSE ON TRAINING and TEST: 0.12860831279646268. 0.12947996163910086\n",
      "epoch: 277\n",
      "EPOCH 277: MSE ON TRAINING and TEST: 0.12800249755382537. 0.12789890989661218\n",
      "epoch: 278\n",
      "EPOCH 278: MSE ON TRAINING and TEST: 0.14682947099208832. 0.14790648669004441\n",
      "epoch: 279\n",
      "EPOCH 279: MSE ON TRAINING and TEST: 0.14758842587471008. 0.14688294380903244\n",
      "epoch: 280\n",
      "EPOCH 280: MSE ON TRAINING and TEST: 0.13721131831407546. 0.13839946538209916\n",
      "epoch: 281\n",
      "EPOCH 281: MSE ON TRAINING and TEST: 0.13748789131641387. 0.13674216121435165\n",
      "epoch: 282\n",
      "EPOCH 282: MSE ON TRAINING and TEST: 0.14930259585380554. 0.1487228289246559\n",
      "epoch: 283\n",
      "EPOCH 283: MSE ON TRAINING and TEST: 0.14895377375862814. 0.14939262243834409\n",
      "epoch: 284\n",
      "EPOCH 284: MSE ON TRAINING and TEST: 0.1372387781739235. 0.13724371939897537\n",
      "epoch: 285\n",
      "EPOCH 285: MSE ON TRAINING and TEST: 0.15221508145332335. 0.15198373794555664\n",
      "epoch: 286\n",
      "EPOCH 286: MSE ON TRAINING and TEST: 0.15153319320895456. 0.1521218553185463\n",
      "epoch: 287\n",
      "EPOCH 287: MSE ON TRAINING and TEST: 0.13403165340423584. 0.13418419957160949\n",
      "epoch: 288\n",
      "EPOCH 288: MSE ON TRAINING and TEST: 0.1653023824095726. 0.16563875377178192\n",
      "epoch: 289\n",
      "EPOCH 289: MSE ON TRAINING and TEST: 0.11757411733269692. 0.11931323930621147\n",
      "epoch: 290\n",
      "EPOCH 290: MSE ON TRAINING and TEST: 0.1772461175918579. 0.17718414962291718\n",
      "epoch: 291\n",
      "EPOCH 291: MSE ON TRAINING and TEST: 0.14183350950479506. 0.14070869237184525\n",
      "epoch: 292\n",
      "EPOCH 292: MSE ON TRAINING and TEST: 0.1297334022819996. 0.13004220575094222\n",
      "epoch: 293\n",
      "EPOCH 293: MSE ON TRAINING and TEST: 0.1761284280907024. 0.17580214142799377\n",
      "epoch: 294\n",
      "EPOCH 294: MSE ON TRAINING and TEST: 0.14138432294130326. 0.14152235984802247\n",
      "epoch: 295\n",
      "EPOCH 295: MSE ON TRAINING and TEST: 0.1404072731733322. 0.14035642743110657\n",
      "epoch: 296\n",
      "EPOCH 296: MSE ON TRAINING and TEST: 0.15281948894262315. 0.1535530135035515\n",
      "epoch: 297\n",
      "EPOCH 297: MSE ON TRAINING and TEST: 0.12127647399902344. 0.12149966711347754\n",
      "epoch: 298\n",
      "EPOCH 298: MSE ON TRAINING and TEST: 0.16490066647529603. 0.1634513646364212\n",
      "epoch: 299\n",
      "EPOCH 299: MSE ON TRAINING and TEST: 0.13255655020475388. 0.13282574564218522\n",
      "epoch: 300\n",
      "EPOCH 300: MSE ON TRAINING and TEST: 0.14618681669235228. 0.14621503204107283\n",
      "epoch: 301\n",
      "EPOCH 301: MSE ON TRAINING and TEST: 0.18887244760990143. 0.18947431147098542\n",
      "epoch: 302\n",
      "EPOCH 302: MSE ON TRAINING and TEST: 0.12445120438933373. 0.12429955303668976\n",
      "epoch: 303\n",
      "EPOCH 303: MSE ON TRAINING and TEST: 0.1446385451338508. 0.14458915049379523\n",
      "epoch: 304\n",
      "EPOCH 304: MSE ON TRAINING and TEST: 0.12080472633242607. 0.12096395045518875\n",
      "epoch: 305\n",
      "EPOCH 305: MSE ON TRAINING and TEST: 0.13136913552880286. 0.13102029711008073\n",
      "epoch: 306\n",
      "EPOCH 306: MSE ON TRAINING and TEST: 0.15183669179677964. 0.1514143243432045\n",
      "epoch: 307\n",
      "EPOCH 307: MSE ON TRAINING and TEST: 0.14688578993082047. 0.1469864383339882\n",
      "epoch: 308\n",
      "EPOCH 308: MSE ON TRAINING and TEST: 0.14046449363231658. 0.14125787019729613\n",
      "epoch: 309\n",
      "EPOCH 309: MSE ON TRAINING and TEST: 0.18780779242515563. 0.18725767582654954\n",
      "epoch: 310\n",
      "EPOCH 310: MSE ON TRAINING and TEST: 0.11953743682666258. 0.1179621775041927\n",
      "epoch: 311\n",
      "EPOCH 311: MSE ON TRAINING and TEST: 0.14007814824581147. 0.13973720818758012\n",
      "epoch: 312\n",
      "EPOCH 312: MSE ON TRAINING and TEST: 0.16213953346014023. 0.16194200217723848\n",
      "epoch: 313\n",
      "EPOCH 313: MSE ON TRAINING and TEST: 0.11683435142040252. 0.1163918599486351\n",
      "epoch: 314\n",
      "EPOCH 314: MSE ON TRAINING and TEST: 0.13580804616212844. 0.13628112226724626\n",
      "epoch: 315\n",
      "EPOCH 315: MSE ON TRAINING and TEST: 0.12655342742800713. 0.12586021795868874\n",
      "epoch: 316\n",
      "EPOCH 316: MSE ON TRAINING and TEST: 0.14241391271352768. 0.14393801987171173\n",
      "epoch: 317\n",
      "EPOCH 317: MSE ON TRAINING and TEST: 0.13706879168748856. 0.13705976036461917\n",
      "epoch: 318\n",
      "EPOCH 318: MSE ON TRAINING and TEST: 0.11294769644737243. 0.1125509925186634\n",
      "epoch: 319\n",
      "EPOCH 319: MSE ON TRAINING and TEST: 0.19233104586601257. 0.19231926947832106\n",
      "epoch: 320\n",
      "EPOCH 320: MSE ON TRAINING and TEST: 0.10462052713740956. 0.10437215492129326\n",
      "epoch: 321\n",
      "EPOCH 321: MSE ON TRAINING and TEST: 0.16617043316364288. 0.16654199212789536\n",
      "epoch: 322\n",
      "EPOCH 322: MSE ON TRAINING and TEST: 0.15715851932764052. 0.15657704919576645\n",
      "epoch: 323\n",
      "EPOCH 323: MSE ON TRAINING and TEST: 0.11381525322794914. 0.11414716243743897\n",
      "epoch: 324\n",
      "EPOCH 324: MSE ON TRAINING and TEST: 0.15932039320468902. 0.15853819657455792\n",
      "epoch: 325\n",
      "EPOCH 325: MSE ON TRAINING and TEST: 0.13803979605436326. 0.13860248178243637\n",
      "epoch: 326\n",
      "EPOCH 326: MSE ON TRAINING and TEST: 0.1314672887325287. 0.1299092635512352\n",
      "epoch: 327\n",
      "EPOCH 327: MSE ON TRAINING and TEST: 0.14393459395928818. 0.14351170957088472\n",
      "epoch: 328\n",
      "EPOCH 328: MSE ON TRAINING and TEST: 0.15770612508058549. 0.1572565972805023\n",
      "epoch: 329\n",
      "EPOCH 329: MSE ON TRAINING and TEST: 0.12615863978862762. 0.12567580342292786\n",
      "epoch: 330\n",
      "EPOCH 330: MSE ON TRAINING and TEST: 0.12621276304125786. 0.1257382594048977\n",
      "epoch: 331\n",
      "EPOCH 331: MSE ON TRAINING and TEST: 0.15555285960435866. 0.15542379292574796\n",
      "epoch: 332\n",
      "EPOCH 332: MSE ON TRAINING and TEST: 0.10740423053503037. 0.10717694610357284\n",
      "epoch: 333\n",
      "EPOCH 333: MSE ON TRAINING and TEST: 0.1782855734229088. 0.17726660668849945\n",
      "epoch: 334\n",
      "EPOCH 334: MSE ON TRAINING and TEST: 0.1176208235323429. 0.11692241430282593\n",
      "epoch: 335\n",
      "EPOCH 335: MSE ON TRAINING and TEST: 0.14097387343645096. 0.14148537963628768\n",
      "epoch: 336\n",
      "EPOCH 336: MSE ON TRAINING and TEST: 0.1295248582959175. 0.13031746298074723\n",
      "epoch: 337\n",
      "EPOCH 337: MSE ON TRAINING and TEST: 0.2087931294332851. 0.20953583175485785\n",
      "epoch: 338\n",
      "EPOCH 338: MSE ON TRAINING and TEST: 0.10102880969643593. 0.10122876167297364\n",
      "epoch: 339\n",
      "EPOCH 339: MSE ON TRAINING and TEST: 0.1473904624581337. 0.14622345119714736\n",
      "epoch: 340\n",
      "EPOCH 340: MSE ON TRAINING and TEST: 0.13725236654281617. 0.13902631253004075\n",
      "epoch: 341\n",
      "EPOCH 341: MSE ON TRAINING and TEST: 0.13751530200242995. 0.137694875895977\n",
      "epoch: 342\n",
      "EPOCH 342: MSE ON TRAINING and TEST: 0.12837077528238297. 0.12821606248617173\n",
      "epoch: 343\n",
      "EPOCH 343: MSE ON TRAINING and TEST: 0.13652168959379196. 0.13559816926717758\n",
      "epoch: 344\n",
      "EPOCH 344: MSE ON TRAINING and TEST: 0.12961401722647928. 0.13000791316682642\n",
      "epoch: 345\n",
      "EPOCH 345: MSE ON TRAINING and TEST: 0.1346002086997032. 0.13514291048049926\n",
      "epoch: 346\n",
      "EPOCH 346: MSE ON TRAINING and TEST: 0.14357481747865677. 0.14343150407075883\n",
      "epoch: 347\n",
      "EPOCH 347: MSE ON TRAINING and TEST: 0.12266651811924847. 0.12126091495156288\n",
      "epoch: 348\n",
      "EPOCH 348: MSE ON TRAINING and TEST: 0.15091730356216432. 0.1527047723531723\n",
      "epoch: 349\n",
      "EPOCH 349: MSE ON TRAINING and TEST: 0.11327394470572472. 0.11353406310081482\n",
      "epoch: 350\n",
      "EPOCH 350: MSE ON TRAINING and TEST: 0.1329556629061699. 0.13312666416168212\n",
      "epoch: 351\n",
      "EPOCH 351: MSE ON TRAINING and TEST: 0.12290697395801545. 0.12313900142908096\n",
      "epoch: 352\n",
      "EPOCH 352: MSE ON TRAINING and TEST: 0.1411111131310463. 0.1414685919880867\n",
      "epoch: 353\n",
      "EPOCH 353: MSE ON TRAINING and TEST: 0.1356945738196373. 0.13534095287322997\n",
      "epoch: 354\n",
      "EPOCH 354: MSE ON TRAINING and TEST: 0.14798042584549298. 0.14790780544281007\n",
      "epoch: 355\n",
      "EPOCH 355: MSE ON TRAINING and TEST: 0.14138050824403764. 0.14108335524797438\n",
      "epoch: 356\n",
      "EPOCH 356: MSE ON TRAINING and TEST: 0.1401126876473427. 0.13984753787517548\n",
      "epoch: 357\n",
      "EPOCH 357: MSE ON TRAINING and TEST: 0.12650250867009163. 0.12719943672418593\n",
      "epoch: 358\n",
      "EPOCH 358: MSE ON TRAINING and TEST: 0.13463100492954255. 0.13436338305473328\n",
      "epoch: 359\n",
      "EPOCH 359: MSE ON TRAINING and TEST: 0.15024187713861464. 0.15082736313343048\n",
      "epoch: 360\n",
      "EPOCH 360: MSE ON TRAINING and TEST: 0.12083387225866318. 0.12107936143875123\n",
      "epoch: 361\n",
      "EPOCH 361: MSE ON TRAINING and TEST: 0.12942543625831604. 0.12851303368806838\n",
      "epoch: 362\n",
      "EPOCH 362: MSE ON TRAINING and TEST: 0.16011369824409485. 0.15969381332397461\n",
      "epoch: 363\n",
      "EPOCH 363: MSE ON TRAINING and TEST: 0.12023248374462128. 0.12030928060412407\n",
      "epoch: 364\n",
      "EPOCH 364: MSE ON TRAINING and TEST: 0.14955032549121164. 0.14957584034312854\n",
      "epoch: 365\n",
      "EPOCH 365: MSE ON TRAINING and TEST: 0.1433144897222519. 0.14364158511161804\n",
      "epoch: 366\n",
      "EPOCH 366: MSE ON TRAINING and TEST: 0.1298166260123253. 0.1292218118906021\n",
      "epoch: 367\n",
      "EPOCH 367: MSE ON TRAINING and TEST: 0.13596789240837098. 0.1367124944925308\n",
      "epoch: 368\n",
      "EPOCH 368: MSE ON TRAINING and TEST: 0.16798446476459503. 0.16907952725887299\n",
      "epoch: 369\n",
      "EPOCH 369: MSE ON TRAINING and TEST: 0.11363157704472542. 0.1122253842651844\n",
      "epoch: 370\n",
      "EPOCH 370: MSE ON TRAINING and TEST: 0.13348509967327118. 0.13349849581718445\n",
      "epoch: 371\n",
      "EPOCH 371: MSE ON TRAINING and TEST: 0.14602618596770547. 0.14482654224742542\n",
      "epoch: 372\n",
      "EPOCH 372: MSE ON TRAINING and TEST: 0.12710330188274382. 0.1266460932791233\n",
      "epoch: 373\n",
      "EPOCH 373: MSE ON TRAINING and TEST: 0.11242503076791763. 0.11224397942423821\n",
      "epoch: 374\n",
      "EPOCH 374: MSE ON TRAINING and TEST: 0.1309642881155014. 0.13039663881063462\n",
      "epoch: 375\n",
      "EPOCH 375: MSE ON TRAINING and TEST: 0.1411712110042572. 0.14069363623857498\n",
      "epoch: 376\n",
      "EPOCH 376: MSE ON TRAINING and TEST: 0.1233180157840252. 0.1234903760254383\n",
      "epoch: 377\n",
      "EPOCH 377: MSE ON TRAINING and TEST: 0.14452361911535264. 0.14440331012010574\n",
      "epoch: 378\n",
      "EPOCH 378: MSE ON TRAINING and TEST: 0.12982504069805145. 0.12947491895068775\n",
      "epoch: 379\n",
      "EPOCH 379: MSE ON TRAINING and TEST: 0.12217666283249855. 0.12184403985738754\n",
      "epoch: 380\n",
      "EPOCH 380: MSE ON TRAINING and TEST: 0.12223268076777458. 0.12253837138414383\n",
      "epoch: 381\n",
      "EPOCH 381: MSE ON TRAINING and TEST: 0.10919462618502704. 0.10919162929058075\n",
      "epoch: 382\n",
      "EPOCH 382: MSE ON TRAINING and TEST: 0.140377876162529. 0.1405835583806038\n",
      "epoch: 383\n",
      "EPOCH 383: MSE ON TRAINING and TEST: 0.10308523550629616. 0.10306736007332802\n",
      "epoch: 384\n",
      "EPOCH 384: MSE ON TRAINING and TEST: 0.15879273116588594. 0.1588105320930481\n",
      "epoch: 385\n",
      "EPOCH 385: MSE ON TRAINING and TEST: 0.11842138543725014. 0.11820686608552933\n",
      "epoch: 386\n",
      "EPOCH 386: MSE ON TRAINING and TEST: 0.13915076702833176. 0.138828606903553\n",
      "epoch: 387\n",
      "EPOCH 387: MSE ON TRAINING and TEST: 0.11942562758922577. 0.12024202644824981\n",
      "epoch: 388\n",
      "EPOCH 388: MSE ON TRAINING and TEST: 0.16788039424202658. 0.16730067878961563\n",
      "epoch: 389\n",
      "EPOCH 389: MSE ON TRAINING and TEST: 0.10703101009130478. 0.10718107298016548\n",
      "epoch: 390\n",
      "EPOCH 390: MSE ON TRAINING and TEST: 0.11956379786133767. 0.12061224728822709\n",
      "epoch: 391\n",
      "EPOCH 391: MSE ON TRAINING and TEST: 0.13397911041975022. 0.13297916799783707\n",
      "epoch: 392\n",
      "EPOCH 392: MSE ON TRAINING and TEST: 0.15859124064445496. 0.15789284218441357\n",
      "epoch: 393\n",
      "EPOCH 393: MSE ON TRAINING and TEST: 0.12724588960409164. 0.12668794840574266\n",
      "epoch: 394\n",
      "EPOCH 394: MSE ON TRAINING and TEST: 0.12233033776283264. 0.12271184176206588\n",
      "epoch: 395\n",
      "EPOCH 395: MSE ON TRAINING and TEST: 0.14711440801620485. 0.14704395085573196\n",
      "epoch: 396\n",
      "EPOCH 396: MSE ON TRAINING and TEST: 0.11154300272464752. 0.11098062619566917\n",
      "epoch: 397\n",
      "EPOCH 397: MSE ON TRAINING and TEST: 0.13904857039451599. 0.13843872100114823\n",
      "epoch: 398\n",
      "EPOCH 398: MSE ON TRAINING and TEST: 0.12805349379777908. 0.1286269880153916\n",
      "epoch: 399\n",
      "EPOCH 399: MSE ON TRAINING and TEST: 0.15741641372442244. 0.1569024682044983\n",
      "epoch: 400\n",
      "EPOCH 400: MSE ON TRAINING and TEST: 0.12310955151915551. 0.12289468720555305\n",
      "epoch: 401\n",
      "EPOCH 401: MSE ON TRAINING and TEST: 0.13931878954172133. 0.1393369808793068\n",
      "epoch: 402\n",
      "EPOCH 402: MSE ON TRAINING and TEST: 0.1169558160007. 0.11658051908016205\n",
      "epoch: 403\n",
      "EPOCH 403: MSE ON TRAINING and TEST: 0.1359997108578682. 0.13627829402685165\n",
      "epoch: 404\n",
      "EPOCH 404: MSE ON TRAINING and TEST: 0.1468667358160019. 0.14659532606601716\n",
      "epoch: 405\n",
      "EPOCH 405: MSE ON TRAINING and TEST: 0.11207825690507889. 0.1115847873416814\n",
      "epoch: 406\n",
      "EPOCH 406: MSE ON TRAINING and TEST: 0.14785952419042586. 0.14834535717964173\n",
      "epoch: 407\n",
      "EPOCH 407: MSE ON TRAINING and TEST: 0.10821809321641922. 0.10873335599899292\n",
      "epoch: 408\n",
      "EPOCH 408: MSE ON TRAINING and TEST: 0.11016445877877148. 0.10967327654361725\n",
      "epoch: 409\n",
      "EPOCH 409: MSE ON TRAINING and TEST: 0.15308717340230943. 0.15387225896120071\n",
      "epoch: 410\n",
      "EPOCH 410: MSE ON TRAINING and TEST: 0.112179746478796. 0.11194807812571525\n",
      "epoch: 411\n",
      "EPOCH 411: MSE ON TRAINING and TEST: 0.15515689104795455. 0.15495577901601792\n",
      "epoch: 412\n",
      "EPOCH 412: MSE ON TRAINING and TEST: 0.11715354695916176. 0.11738996072248979\n",
      "epoch: 413\n",
      "EPOCH 413: MSE ON TRAINING and TEST: 0.15611114352941513. 0.1557919055223465\n",
      "epoch: 414\n",
      "EPOCH 414: MSE ON TRAINING and TEST: 0.11307545006275177. 0.1141499936580658\n",
      "epoch: 415\n",
      "EPOCH 415: MSE ON TRAINING and TEST: 0.13817846910520035. 0.13788634687662124\n",
      "epoch: 416\n",
      "EPOCH 416: MSE ON TRAINING and TEST: 0.1314917877316475. 0.1318353794515133\n",
      "epoch: 417\n",
      "EPOCH 417: MSE ON TRAINING and TEST: 0.13859350383281707. 0.13746214509010315\n",
      "epoch: 418\n",
      "EPOCH 418: MSE ON TRAINING and TEST: 0.10197678282856941. 0.10331621542572975\n",
      "epoch: 419\n",
      "EPOCH 419: MSE ON TRAINING and TEST: 0.14758279621601106. 0.14687651124867526\n",
      "epoch: 420\n",
      "EPOCH 420: MSE ON TRAINING and TEST: 0.16598174422979356. 0.1648497223854065\n",
      "epoch: 421\n",
      "EPOCH 421: MSE ON TRAINING and TEST: 0.08893354386091232. 0.0893437348306179\n",
      "epoch: 422\n",
      "EPOCH 422: MSE ON TRAINING and TEST: 0.17001811482689597. 0.1715060442686081\n",
      "epoch: 423\n",
      "EPOCH 423: MSE ON TRAINING and TEST: 0.11170670762658119. 0.11124081313610076\n",
      "epoch: 424\n",
      "EPOCH 424: MSE ON TRAINING and TEST: 0.13392175883054733. 0.13394420295953752\n",
      "epoch: 425\n",
      "EPOCH 425: MSE ON TRAINING and TEST: 0.12495689297264273. 0.12512696060267361\n",
      "epoch: 426\n",
      "EPOCH 426: MSE ON TRAINING and TEST: 0.1261147990822792. 0.12617128863930702\n",
      "epoch: 427\n",
      "EPOCH 427: MSE ON TRAINING and TEST: 0.11744516715407372. 0.11684591919183732\n",
      "epoch: 428\n",
      "EPOCH 428: MSE ON TRAINING and TEST: 0.10426715314388275. 0.10547983869910241\n",
      "epoch: 429\n",
      "EPOCH 429: MSE ON TRAINING and TEST: 0.15799941569566728. 0.15846498012542726\n",
      "epoch: 430\n",
      "EPOCH 430: MSE ON TRAINING and TEST: 0.11828003823757172. 0.11870013251900673\n",
      "epoch: 431\n",
      "EPOCH 431: MSE ON TRAINING and TEST: 0.11240623518824577. 0.11222907602787018\n",
      "epoch: 432\n",
      "EPOCH 432: MSE ON TRAINING and TEST: 0.13787115703929553. 0.13827864690260452\n",
      "epoch: 433\n",
      "EPOCH 433: MSE ON TRAINING and TEST: 0.0966147281229496. 0.096537284553051\n",
      "epoch: 434\n",
      "EPOCH 434: MSE ON TRAINING and TEST: 0.1374948278069496. 0.13891176134347916\n",
      "epoch: 435\n",
      "EPOCH 435: MSE ON TRAINING and TEST: 0.1221623569726944. 0.12105305567383766\n",
      "epoch: 436\n",
      "EPOCH 436: MSE ON TRAINING and TEST: 0.12441027611494064. 0.125119186937809\n",
      "epoch: 437\n",
      "EPOCH 437: MSE ON TRAINING and TEST: 0.11771033927798272. 0.11881740912795066\n",
      "epoch: 438\n",
      "EPOCH 438: MSE ON TRAINING and TEST: 0.09720081463456154. 0.09698123335838318\n",
      "epoch: 439\n",
      "EPOCH 439: MSE ON TRAINING and TEST: 0.1568403812971982. 0.1569720980795947\n",
      "epoch: 440\n",
      "EPOCH 440: MSE ON TRAINING and TEST: 0.10256858170032501. 0.10288577750325203\n",
      "epoch: 441\n",
      "EPOCH 441: MSE ON TRAINING and TEST: 0.14818551391363144. 0.14864721149206161\n",
      "epoch: 442\n",
      "EPOCH 442: MSE ON TRAINING and TEST: 0.1117250221696767. 0.11106941401958466\n",
      "epoch: 443\n",
      "EPOCH 443: MSE ON TRAINING and TEST: 0.13258927166461945. 0.1327617347240448\n",
      "epoch: 444\n",
      "EPOCH 444: MSE ON TRAINING and TEST: 0.13841935694217683. 0.1378521591424942\n",
      "epoch: 445\n",
      "EPOCH 445: MSE ON TRAINING and TEST: 0.13393197655677797. 0.13335456028580667\n",
      "epoch: 446\n",
      "EPOCH 446: MSE ON TRAINING and TEST: 0.13258901312947274. 0.13287962702187625\n",
      "epoch: 447\n",
      "EPOCH 447: MSE ON TRAINING and TEST: 0.09175453782081604. 0.09203398674726486\n",
      "epoch: 448\n",
      "EPOCH 448: MSE ON TRAINING and TEST: 0.13221458345651627. 0.1330426350235939\n",
      "epoch: 449\n",
      "EPOCH 449: MSE ON TRAINING and TEST: 0.12254280055111105. 0.12253357172012329\n",
      "epoch: 450\n",
      "EPOCH 450: MSE ON TRAINING and TEST: 0.09831287115812301. 0.09856473281979561\n",
      "epoch: 451\n",
      "EPOCH 451: MSE ON TRAINING and TEST: 0.13176047652959824. 0.13158167377114297\n",
      "epoch: 452\n",
      "EPOCH 452: MSE ON TRAINING and TEST: 0.13454133570194243. 0.1345243290066719\n",
      "epoch: 453\n",
      "EPOCH 453: MSE ON TRAINING and TEST: 0.1182190641760826. 0.11801154979250648\n",
      "epoch: 454\n",
      "EPOCH 454: MSE ON TRAINING and TEST: 0.12620902955532073. 0.1266454689204693\n",
      "epoch: 455\n",
      "EPOCH 455: MSE ON TRAINING and TEST: 0.13742434233427048. 0.1372670754790306\n",
      "epoch: 456\n",
      "EPOCH 456: MSE ON TRAINING and TEST: 0.1289385214447975. 0.1296265661716461\n",
      "epoch: 457\n",
      "EPOCH 457: MSE ON TRAINING and TEST: 0.1473459705710411. 0.14766718745231627\n",
      "epoch: 458\n",
      "EPOCH 458: MSE ON TRAINING and TEST: 0.10616866350173951. 0.10464898198843002\n",
      "epoch: 459\n",
      "EPOCH 459: MSE ON TRAINING and TEST: 0.13350458578629928. 0.13360477035695856\n",
      "epoch: 460\n",
      "EPOCH 460: MSE ON TRAINING and TEST: 0.11689925491809845. 0.11669130772352218\n",
      "epoch: 461\n",
      "EPOCH 461: MSE ON TRAINING and TEST: 0.11359860002994537. 0.11293463334441185\n",
      "epoch: 462\n",
      "EPOCH 462: MSE ON TRAINING and TEST: 0.12538216188549994. 0.1261775016784668\n",
      "epoch: 463\n",
      "EPOCH 463: MSE ON TRAINING and TEST: 0.11893864646553994. 0.11936506927013397\n",
      "epoch: 464\n",
      "EPOCH 464: MSE ON TRAINING and TEST: 0.11413834765553474. 0.11341218650341034\n",
      "epoch: 465\n",
      "EPOCH 465: MSE ON TRAINING and TEST: 0.15260937362909316. 0.1523296445608139\n",
      "epoch: 466\n",
      "EPOCH 466: MSE ON TRAINING and TEST: 0.10133924944834276. 0.09962441094897011\n",
      "epoch: 467\n",
      "EPOCH 467: MSE ON TRAINING and TEST: 0.12551172450184822. 0.12552991956472398\n",
      "epoch: 468\n",
      "EPOCH 468: MSE ON TRAINING and TEST: 0.1514035642147064. 0.15057222694158554\n",
      "epoch: 469\n",
      "EPOCH 469: MSE ON TRAINING and TEST: 0.12200978472828865. 0.12159469723701477\n",
      "epoch: 470\n",
      "EPOCH 470: MSE ON TRAINING and TEST: 0.13443072140216827. 0.13398265540599824\n",
      "epoch: 471\n",
      "EPOCH 471: MSE ON TRAINING and TEST: 0.12979864031076432. 0.12898845747113227\n",
      "epoch: 472\n",
      "EPOCH 472: MSE ON TRAINING and TEST: 0.15513043850660324. 0.15450028628110885\n",
      "epoch: 473\n",
      "EPOCH 473: MSE ON TRAINING and TEST: 0.0894647292792797. 0.09013198654760014\n",
      "epoch: 474\n",
      "EPOCH 474: MSE ON TRAINING and TEST: 0.11354363709688187. 0.11382796987891197\n",
      "epoch: 475\n",
      "EPOCH 475: MSE ON TRAINING and TEST: 0.14798536151647568. 0.14918000400066375\n",
      "epoch: 476\n",
      "EPOCH 476: MSE ON TRAINING and TEST: 0.11875759674744173. 0.11840875446796417\n",
      "epoch: 477\n",
      "EPOCH 477: MSE ON TRAINING and TEST: 0.10730262920260429. 0.10745518878102303\n",
      "epoch: 478\n",
      "EPOCH 478: MSE ON TRAINING and TEST: 0.1257822498679161. 0.12557219788432122\n",
      "epoch: 479\n",
      "EPOCH 479: MSE ON TRAINING and TEST: 0.1510363847017288. 0.15077312886714936\n",
      "epoch: 480\n",
      "EPOCH 480: MSE ON TRAINING and TEST: 0.1345009133219719. 0.1350606165148995\n",
      "epoch: 481\n",
      "EPOCH 481: MSE ON TRAINING and TEST: 0.11368480622768402. 0.11362107172608375\n",
      "epoch: 482\n",
      "EPOCH 482: MSE ON TRAINING and TEST: 0.1262630209326744. 0.12698933482170105\n",
      "epoch: 483\n",
      "EPOCH 483: MSE ON TRAINING and TEST: 0.15937506475231863. 0.1574230894446373\n",
      "epoch: 484\n",
      "EPOCH 484: MSE ON TRAINING and TEST: 0.1005016878247261. 0.10128227919340134\n",
      "epoch: 485\n",
      "EPOCH 485: MSE ON TRAINING and TEST: 0.11372773200273514. 0.1132697343826294\n",
      "epoch: 486\n",
      "EPOCH 486: MSE ON TRAINING and TEST: 0.15164823694662613. 0.15125020932067523\n",
      "epoch: 487\n",
      "EPOCH 487: MSE ON TRAINING and TEST: 0.09353120476007462. 0.09355502352118492\n",
      "epoch: 488\n",
      "EPOCH 488: MSE ON TRAINING and TEST: 0.12672296464443206. 0.1261922113597393\n",
      "epoch: 489\n",
      "EPOCH 489: MSE ON TRAINING and TEST: 0.10982454270124435. 0.1095207765698433\n",
      "epoch: 490\n",
      "EPOCH 490: MSE ON TRAINING and TEST: 0.11658957228064537. 0.11640560179948807\n",
      "epoch: 491\n",
      "EPOCH 491: MSE ON TRAINING and TEST: 0.12155144363641739. 0.12206538692116738\n",
      "epoch: 492\n",
      "EPOCH 492: MSE ON TRAINING and TEST: 0.10482184514403343. 0.10559379309415817\n",
      "epoch: 493\n",
      "EPOCH 493: MSE ON TRAINING and TEST: 0.11529219286008315. 0.11492260545492172\n",
      "epoch: 494\n",
      "EPOCH 494: MSE ON TRAINING and TEST: 0.12571725398302078. 0.12523929104208947\n",
      "epoch: 495\n",
      "EPOCH 495: MSE ON TRAINING and TEST: 0.12136260569095611. 0.12136620432138442\n",
      "epoch: 496\n",
      "EPOCH 496: MSE ON TRAINING and TEST: 0.0887976586818695. 0.0880740724503994\n",
      "epoch: 497\n",
      "EPOCH 497: MSE ON TRAINING and TEST: 0.13749129325151443. 0.13802245259284973\n",
      "epoch: 498\n",
      "EPOCH 498: MSE ON TRAINING and TEST: 0.10858304128050804. 0.10786316767334939\n",
      "epoch: 499\n",
      "EPOCH 499: MSE ON TRAINING and TEST: 0.13855209201574326. 0.13962116837501526\n",
      "end of training\n",
      "CPU times: user 1min 23s, sys: 6.62 s, total: 1min 30s\n",
      "Wall time: 1min 35s\n",
      "time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set optimization parameters\n",
    "epochs = 500\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0.\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B00CGK5GWE    33\n",
       "B004Q7AB4I    26\n",
       "B0015MN91O    17\n",
       "B000PEINWS    16\n",
       "B00LNCPM70    15\n",
       "Name: product_id, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.69 ms\n"
     ]
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_id', 'product_id', 'star_rating', 'product_parent',\n",
       "       'product_category', 'product_title', 'helpful_votes', 'user', 'item'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.35 ms\n"
     ]
    }
   ],
   "source": [
    "reduced_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_title</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52181093</td>\n",
       "      <td>B008709MDG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>694981365</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>.925 Sterling Silver 6mm Round Cubic Zirconia ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2229</td>\n",
       "      <td>5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52181093</td>\n",
       "      <td>B004Z1OSRQ</td>\n",
       "      <td>5</td>\n",
       "      <td>609609182</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Sterling Silve Simulated Birthstone Round Crys...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2229</td>\n",
       "      <td>5531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id star_rating  product_parent product_category  \\\n",
       "0     52181093  B008709MDG         5.0       694981365          Jewelry   \n",
       "1     52181093  B004Z1OSRQ           5       609609182          Jewelry   \n",
       "\n",
       "                                       product_title  helpful_votes  user  \\\n",
       "0  .925 Sterling Silver 6mm Round Cubic Zirconia ...            2.0  2229   \n",
       "1  Sterling Silve Simulated Birthstone Round Crys...           11.0  2229   \n",
       "\n",
       "   item  \n",
       "0  5676  \n",
       "1  5531  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "reduced_df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cutomers:  10534\n",
      "test_customer_index: 0\n",
      "test_customer_index: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:39: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:40: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_customer_index: 2\n",
      "test_customer_index: 3\n",
      "test_customer_index: 4\n",
      "test_customer_index: 5\n",
      "test_customer_index: 6\n",
      "test_customer_index: 7\n",
      "test_customer_index: 8\n",
      "test_customer_index: 9\n",
      "test_customer_index: 10\n",
      "test_customer_index: 11\n",
      "test_customer_index: 12\n",
      "test_customer_index: 13\n",
      "test_customer_index: 14\n",
      "test_customer_index: 15\n",
      "test_customer_index: 16\n",
      "test_customer_index: 17\n",
      "test_customer_index: 18\n",
      "test_customer_index: 19\n",
      "test_customer_index: 20\n",
      "test_customer_index: 21\n",
      "test_customer_index: 22\n",
      "test_customer_index: 23\n",
      "test_customer_index: 24\n",
      "test_customer_index: 25\n",
      "test_customer_index: 26\n",
      "test_customer_index: 27\n",
      "test_customer_index: 28\n",
      "test_customer_index: 29\n",
      "test_customer_index: 30\n",
      "test_customer_index: 31\n",
      "test_customer_index: 32\n",
      "test_customer_index: 33\n",
      "test_customer_index: 34\n",
      "test_customer_index: 35\n",
      "test_customer_index: 36\n",
      "test_customer_index: 37\n",
      "test_customer_index: 38\n",
      "test_customer_index: 39\n",
      "test_customer_index: 40\n",
      "test_customer_index: 41\n",
      "test_customer_index: 42\n",
      "test_customer_index: 43\n",
      "test_customer_index: 44\n",
      "test_customer_index: 45\n",
      "test_customer_index: 46\n",
      "test_customer_index: 47\n",
      "test_customer_index: 48\n",
      "test_customer_index: 49\n",
      "test_customer_index: 50\n",
      "test_customer_index: 51\n",
      "test_customer_index: 52\n",
      "test_customer_index: 53\n",
      "test_customer_index: 54\n",
      "test_customer_index: 55\n",
      "test_customer_index: 56\n",
      "test_customer_index: 57\n",
      "test_customer_index: 58\n",
      "test_customer_index: 59\n",
      "test_customer_index: 60\n",
      "test_customer_index: 61\n",
      "test_customer_index: 62\n",
      "test_customer_index: 63\n",
      "test_customer_index: 64\n",
      "test_customer_index: 65\n",
      "test_customer_index: 66\n",
      "test_customer_index: 67\n",
      "test_customer_index: 68\n",
      "test_customer_index: 69\n",
      "test_customer_index: 70\n",
      "test_customer_index: 71\n",
      "test_customer_index: 72\n",
      "test_customer_index: 73\n",
      "test_customer_index: 74\n",
      "test_customer_index: 75\n",
      "test_customer_index: 76\n",
      "test_customer_index: 77\n",
      "test_customer_index: 78\n",
      "test_customer_index: 79\n",
      "test_customer_index: 80\n",
      "test_customer_index: 81\n",
      "test_customer_index: 82\n",
      "test_customer_index: 83\n",
      "test_customer_index: 84\n",
      "test_customer_index: 85\n",
      "test_customer_index: 86\n",
      "test_customer_index: 87\n",
      "test_customer_index: 88\n",
      "test_customer_index: 89\n",
      "test_customer_index: 90\n",
      "test_customer_index: 91\n",
      "test_customer_index: 92\n",
      "test_customer_index: 93\n",
      "test_customer_index: 94\n",
      "test_customer_index: 95\n",
      "test_customer_index: 96\n",
      "test_customer_index: 97\n",
      "test_customer_index: 98\n",
      "test_customer_index: 99\n",
      "test_customer_index: 100\n",
      "test_customer_index: 101\n",
      "test_customer_index: 102\n",
      "test_customer_index: 103\n",
      "test_customer_index: 104\n",
      "test_customer_index: 105\n",
      "test_customer_index: 106\n",
      "test_customer_index: 107\n"
     ]
    }
   ],
   "source": [
    "# customer_index_list=[1,2,3]\n",
    "customer_index_list = customer_index['user'].tolist()\n",
    "\n",
    "# # 0 to 1000 customer\n",
    "# customer_index_list = [*range(0, 1000, 1)]\n",
    "\n",
    "\n",
    "print(\"Total number of cutomers: \", len(customer_index_list))\n",
    "\n",
    "product_index_local = pd.DataFrame({'product_id': products.index, \n",
    "                                    'product_url': 'https://www.amazon.com/dp/'+products.index, \n",
    "                              'item': np.arange(products.shape[0])})\n",
    "all_predictions_from_user = pd.DataFrame(columns=['customer_id', 'product_id', 'product_url', 'prediction', 'product_title'])\n",
    "\n",
    "for user_index in customer_index_list:\n",
    "    print(\"test_customer_index:\", user_index)\n",
    "    \n",
    "    product_index_local['prediction'] = trained_net(nd.array([7] * product_index_local.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index_local['item'].values).as_in_context(ctx)).asnumpy()\n",
    "\n",
    "    product_index_local['customer_id'] = customer_index[customer_index['user'] == user_index]['customer_id'].values.tolist()[0]\n",
    "    \n",
    "    # titles\n",
    "    titles = reduced_df.groupby('product_id')['product_title'].last().reset_index()\n",
    "    predictions_titles_local = product_index_local.merge(titles)\n",
    "    \n",
    "    # product_category\n",
    "    product_category = reduced_df.groupby('product_id')['product_category'].last().reset_index()\n",
    "    predictions_catalogs_local = predictions_titles_local.merge(product_category)\n",
    "    \n",
    "    # product_parent\n",
    "    product_parent = reduced_df.groupby('product_id')['product_parent'].last().reset_index()\n",
    "    predictions_catalogs_local = predictions_catalogs_local.merge(product_parent)\n",
    "    \n",
    "    predictions_titles_local = predictions_catalogs_local.sort_values(['prediction', 'product_id'], ascending=[False, True])\n",
    "    \n",
    "    \n",
    "    #combine all results\n",
    "    all_predictions_from_user = pd.concat([all_predictions_from_user, predictions_titles_local])\n",
    "    all_predictions_from_user = pd.concat([all_predictions_from_user, predictions_catalogs_local])\n",
    "    \n",
    "    # select top 50 recommeded product\n",
    "    predictions_titles_local = predictions_titles_local.head(n=50)\n",
    "    \n",
    "#reset index\n",
    "all_predictions_from_user = all_predictions_from_user.reset_index(drop=True)\n",
    "all_predictions_from_user = all_predictions_from_user[['customer_id', 'product_id', 'product_url', 'prediction',  'product_title', 'product_category', 'product_parent']]\n",
    "\n",
    "# #generate csv file\n",
    "all_predictions_from_user.to_csv(\"./Apparel_Jewelry_Shoes_predictions_from_user.csv\")\n",
    "# #generate pickle file\n",
    "all_predictions_from_user.to_csv(\"./Apparel_Jewelry_Shoes_predictions_from_user.pickle\")\n",
    "\n",
    "all_predictions_from_user.head(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_id', 'product_url', 'item', 'prediction', 'customer_id',\n",
       "       'product_title', 'product_category', 'product_parent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.24 ms\n"
     ]
    }
   ],
   "source": [
    "predictions_titles_local.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alrt Apparel_Jewelry_Shoes_predictions_from_user*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp Apparel_Jewelry_Shoes_predictions_from_user.csv s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_predictions_from_user.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp Apparel_Jewelry_Shoes_predictions_from_user.pickle s3://dse-cohort5-group1/1_Prediction_results/Apparel_Jewelry_Shoes/data/Apparel_Jewelry_Shoes_predictions_from_user.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train with SageMaker\n",
    "\n",
    "Now that we've trained on this smaller dataset, we can expand training in SageMaker's distributed, managed training environment.\n",
    "\n",
    "### Wrap Code\n",
    "\n",
    "To use SageMaker's pre-built MXNet container, we'll need to wrap our code from above into a Python script.  There's a great deal of flexibility in using SageMaker's pre-built containers, and detailed documentation can be found [here](https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators), but for our example, it consisted of:\n",
    "1. Wrapping all data preparation into a `prepare_train_data` function (we could name this whatever we like)\n",
    "1. Copying and pasting classes and functions from above word-for-word\n",
    "1. Defining a `train` function that:\n",
    "  1. Adds a bit of new code to pick up the input TSV dataset on the SageMaker Training cluster\n",
    "  1. Takes in a dict of hyperparameters (which we specified as globals above)\n",
    "  1. Creates the net and executes training\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! You have to apploy all above code change into recommender.py\n",
    "\n",
    "- check recommender.py code before run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat recommender.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Locally\n",
    "\n",
    "Now we can test our train function locally.  This helps ensure we don't have any bugs before submitting our code to SageMaker's pre-built MXNet container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al /tmp/recsys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import recommender\n",
    "\n",
    "local_test_net, local_customer_index, local_product_index = recommender.train(\n",
    "    {'train': '/tmp/recsys/'}, \n",
    "    {'num_embeddings': 64, \n",
    "     'opt': 'sgd', \n",
    "     'lr': 0.02, \n",
    "     'momentum': 0.9, \n",
    "     'wd': 0.,\n",
    "     'epochs': 2},\n",
    "    ['local'],\n",
    "    1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Data\n",
    "\n",
    "Holding our data in memory works fine when we're interactively exploring a sample of data, but for larger, longer running processes, we'd prefer to run them in the background with SageMaker Training.  To do this, let's move the dataset to S3 so that it can be picked up by SageMaker training.  This is perfect for use cases like periodic re-training, expanding to a larger dataset, or moving production workloads to larger hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change log level\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler()) # Writes to console\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.getLogger('boto3').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('botocore').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('s3transfer').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('urllib3').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_reviews_us_Shoes_v1_00\n",
    "\n",
    "boto3.client('s3').copy({'Bucket': 'amazon-reviews-pds', \n",
    "                         'Key': 'tsv/amazon_reviews_us_Shoes_v1_00.tsv.gz'},\n",
    "                        bucket,\n",
    "                        prefix + '/train/amazon_reviews_us_Shoes_v1_00.tsv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit\n",
    "\n",
    "Now, we can create an MXNet estimator from the SageMaker Python SDK.  To do so, we need to pass in:\n",
    "1. Instance type and count for our SageMaker Training cluster.  SageMaker's MXNet containers support distributed GPU training, so we could easily set this to multiple ml.p2 or ml.p3 instances if we wanted.\n",
    "  - *Note, this would require some changes to our recommender.py script as we would need to setup the context an key value store properly, as well as determining if and how to distribute the training data.*\n",
    "1. An S3 path for out model artifacts and a role with access to S3 input and output paths.\n",
    "1. Hyperparameters for our neural network.  Since with a 64 dimensional embedding, our recommendations reverted too closely to the mean, let's increase this by an order of magnitude when we train outside of our local instance.  We'll also increase the epochs to see how our accuracy evolves over time. We'll leave all other hyperparameters the same.\n",
    "\n",
    "Once we use `.fit()` this creates a SageMaker Training Job that spins up instances, loads the appropriate packages and data, runs our `train` function from `recommender.py`, wraps up and saves model artifacts to S3, and finishes by tearing down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 's3://{}/{}/train/'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://dse-cohort5-group1/sagemaker/amazon_reviews_us_Shoes_v1_00/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ml.p3.8xlarge for training\n",
    "```\n",
    "2020-05-16 06:28:03 Completed - Training job completed\n",
    "Training seconds: 203\n",
    "Billable seconds: 203\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set optimization parameters\n",
    "# opt = 'sgd'\n",
    "# lr = 0.02\n",
    "# momentum = 0.9\n",
    "# wd = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MXNet('recommender.py', \n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.p3.8xlarge\",\n",
    "          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "          hyperparameters={'num_embeddings': 512, \n",
    "                           'opt': opt, \n",
    "                           'lr': lr, \n",
    "                           'momentum': momentum, \n",
    "                           'wd': wd,\n",
    "                           'epochs': 50},\n",
    "         framework_version='1.1')\n",
    "\n",
    "m.fit({'train': 's3://{}/{}/train/'.format(bucket, prefix)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Hyperparameter Tune using Sagemaker Hyperparameter tune\n",
    "\n",
    "### working log - JH\n",
    "### [05/17/2020] need to apply my metric for this model to use Sagemaker hyperparameter. But I don't know how to apply cusome metric to Couldwatch\n",
    " \n",
    "\n",
    "Similar to training a single MXNet job in SageMaker, we define our MXNet estimator passing in the MXNet script, IAM role, (per job) hardware configuration, and any hyperparameters we're not tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = MXNet('recommender.py', \n",
    "#                   py_version='py3',\n",
    "#                   role=role, \n",
    "#                   train_instance_count=1, \n",
    "#                   train_instance_type=\"ml.p3.8xlarge\",\n",
    "#                   output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "#                   base_job_name='Amazon-recomender-hpo-mxnet',\n",
    "#                   hyperparameters={'num_embeddings': 512, \n",
    "#                                    'opt': opt, \n",
    "#                                    'lr': lr, \n",
    "#                                    'momentum': momentum, \n",
    "#                                    'wd': wd,\n",
    "#                                    'epochs': 50},\n",
    "#                  framework_version='1.4.1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined our estimator we can specify the hyperparameters we'd like to tune and their possible values.  We have three different types of hyperparameters.\n",
    "- Categorical parameters need to take one value from a discrete set.  We define this by passing the list of possible values to `CategoricalParameter(list)`\n",
    "- Continuous parameters can take any real number value between the minimum and maximum value, defined by `ContinuousParameter(min, max)`\n",
    "- Integer parameters can take any integer value between the minimum and maximum value, defined by `IntegerParameter(min, max)`\n",
    "\n",
    "*Note, if possible, it's almost always best to specify a value as the least restrictive type.  For example, tuning `thresh` as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with possible values of 0.01, 0.1, 0.15, or 0.2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'Adam']),\n",
    "#                          'learning_rate': ContinuousParameter(0.01, 0.2),\n",
    "#                          'momentum': ContinuousParameter(0., 0.99),\n",
    "#                          'wd': ContinuousParameter(0., 0.001),\n",
    "#                          'num_epoch': IntegerParameter(10, 50)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition.  This includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective_metric_name = 'MSE-ON-TEST'\n",
    "# metric_definitions = [{'Name': 'MSE-ON-TEST',\n",
    "#                        'Regex': 'MSE-ON-TEST=([0-9\\\\.]+)'}]\n",
    "\n",
    "\n",
    "# # # THE SCORING METRIC TO MAXIMIZE\n",
    "# # objective_metric_name = 'Validation-accuracy'\n",
    "# # metric_definitions = [{'Name': 'Validation-accuracy',\n",
    "# #                        'Regex': 'validation: accuracy=([0-9\\\\.]+)'}]\n",
    "\n",
    "\n",
    "# # objective_metric_name = 'loss'\n",
    "# # metric_definitions = [{'Name': 'loss',\n",
    "# #                        'Regex': 'Loss = (.*?);'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a `HyperparameterTuner` object and fit it by pointing to our data in S3.  This kicks our tuning job off in the background.\n",
    "\n",
    "Notice, we specify a much smaller number of total jobs, and a smaller number of parallel jobs.  Since our model uses previous training job runs to predict where to test next, we get better results (although it takes longer) when setting this to a smaller value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = HyperparameterTuner(estimator,\n",
    "#                             objective_metric_name,\n",
    "#                             hyperparameter_ranges,\n",
    "#                             metric_definitions,\n",
    "#                             max_jobs=2,\n",
    "#                             max_parallel_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can start our tuning job by calling `.fit()` and passing in the S3 paths to our train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.fit(train_iter={'train': 's3://{}/{}/train/'.format(bucket, prefix)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just run a quick check of the hyperparameter tuning jobs status to make sure it started successfully and is `InProgress`.\n",
    "\n",
    "_You will be unable to successfully run the following cells until the tuning job completes.  This step may take up to 2 hours._\n",
    "\n",
    "Once the tuning job finishes, we can bring in a table of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuner._current_job_name).dataframe()\n",
    "# bayes_metrics.sort_values(['FinalObjectiveValue'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our results, we can see that, with one fourth the total training jobs, SageMaker's Automatic Model Tuning has produced a model with better accuracy 74% than our random search.  In addition, there's no guarantee that the effectiveness of random search wouldn't change over subsequent runs.\n",
    "\n",
    "Let's compare our hyperparameter's relationship to eachother and the objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(pd.concat([bayes_metrics[['FinalObjectiveValue',\n",
    "#                                                      'learning_rate',\n",
    "#                                                      'momentum',\n",
    "#                                                      'wd']],\n",
    "#                                       bayes_metrics['TrainingStartTime'].rank()],\n",
    "#                            axis=1),\n",
    "#                            figsize=(12, 12))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our accuracy is only about 53% on our validation dataset.  CIFAR-10 can be challenging, but we'd want our accuracy much better than just over half if users are depending on an accurate prediction.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Tune: Random\n",
    "\n",
    "One method of hyperparameter tuning that performs surprisingly well for how simple it is, is randomly trying a variety of hyperparameter values within set ranges.  So, for this example, we've created a helper script `random_tuner.py` to help us do this.\n",
    "\n",
    "We'll need to supply:\n",
    "\n",
    "* A function that trains our MXNet model given a job name and list of hyperparameters.  Note, `wait` is set to false in our `fit()` call so that we can train multiple jobs at once.\n",
    "* A dictionary of hyperparameters where the ones we want to tune are defined as one of three types (`ContinuousParameter`, `IntegerParameter`, or `CategoricalParameter`) and appropriate minimum and maximum ranges or a list of possible values are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = {'train': 's3://{}/{}/train/'.format(bucket, prefix)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_random(job_name, hyperparameters):\n",
    "#     m = MXNet('recommender.py', \n",
    "#               py_version='py3',\n",
    "#               sagemaker_session=sagemaker.Session(),\n",
    "#               role=role, \n",
    "#               train_instance_count=1, \n",
    "#               train_instance_type=\"ml.p2.8xlarge\",\n",
    "#               framework_version='1.4.1',\n",
    "#               base_job_name='Amazon-hpo-mxnet-0516',\n",
    "#               hyperparameters=hyperparameters\n",
    "#               )\n",
    "    \n",
    "#     inputs = {'train': 's3://{}/{}/train/'.format(bucket, prefix)}\n",
    "#     print(\"input for hyperparameter tunning: \", inputs)\n",
    "#     m.fit(inputs, wait=False, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     {'num_embeddings': [64, 128] \n",
    "#      'opt': ['sgd', 'adam'] \n",
    "#      'lr': 0.02, \n",
    "#      'momentum': 0.9, \n",
    "#      'wd': 0.,\n",
    "#      'epochs': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for Test\n",
    "\n",
    "# hyperparameters = {'batch_size': 1024,\n",
    "#                    'epochs': 2,\n",
    "#                    'learning_rate': rt.ContinuousParameter(0.001, 0.5),\n",
    "#                    'momentum': rt.ContinuousParameter(0., 0.99),\n",
    "#                    'wd': rt.ContinuousParameter(0., 0.001)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {'batch_size': rt.CategoricalParameter([1024]),\n",
    "#                    'num_embeddings': rt.CategoricalParameter([64, 128]),\n",
    "#                    'opt': rt.CategoricalParameter(['sgd', 'Adam']),\n",
    "#                    'epochs': 50,\n",
    "#                    'learning_rate': rt.ContinuousParameter(0.001, 0.5),\n",
    "#                    'momentum': rt.ContinuousParameter(0., 0.99),\n",
    "#                    'wd': rt.ContinuousParameter(0., 0.001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can kick off our random search.  We've defined the total number of training jobs to be 120.  This is a large amount and drives most of the cost of this notebook.  Also, we've specified up to 8 jobs to be run in parallel.  This exceeds the default concurrent instance limit for ml.p3.8xlarge instances.  If you're just testing this notebook out, decreasing both values will control costs and allow you to complete successfully without requiring a service limit increase.\n",
    "\n",
    "_Note, this step may take up to 2 hours to complete.  Even if you loose connection with the notebook in the middle, as long as the notebook instance continues to run, `jobs` should still be successfully created for future use._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# '''\n",
    "# Runs random search for hyperparameters.\n",
    "# Takes in:\n",
    "#     train_fn: A function that kicks off a training job based on two positional arguments-\n",
    "#         job name and hyperparameter dictionary.  Note, wait must be set to False if using .fit()\n",
    "#     hyperparameters: A dictonary of hyperparameters defined with hyperparameter classes.\n",
    "#     base_name: Base name for training jobs.  Defaults to 'random-hp-<timestamp>'.\n",
    "#     max_jobs: Total number of training jobs to run.\n",
    "#     max_parallel_jobs: Most training jobs to run concurrently. This does not affect the quality\n",
    "#         of search, just helps stay under account service limits.\n",
    "# Returns a dictionary of max_jobs job names with associated hyperparameter values.\n",
    "# '''\n",
    "# jobs = rt.random_search(fit_random,\n",
    "#                         hyperparameters,\n",
    "#                         max_jobs=120,\n",
    "#                         max_parallel_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our random search completes, we'll want to compare our training jobs (which may take a few extra minutes to finish) in order to understand how our objective metric (% accuracy on our validation dataset) varies by hyperparameter values.  In this case, our helper function includes two functions.\n",
    "\n",
    "* `get_metrics()` scrapes the CloudWatch logs for our training jobs and uses a regex to return any reported values of our objective metric.\n",
    "* `table_metrics()` joins on the hyperparameter values for each job, grabs the ending objective value, and converts the result to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_metrics = rt.table_metrics(jobs, rt.get_metrics(jobs, 'validation: accuracy=([0-9\\\\.]+)'))\n",
    "# random_metrics.sort_values(['objective'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's a huge variation in percent accuracy.  Had we initially (unknowingly) set our learning rate near 0.5, momentum at 0.15, and weight decay to 0.0004, we would have an accuracy just over 20% (this is particularly bad considering random guessing would produce 10% accuracy).\n",
    "\n",
    "But, we also found many successful hyperparameter value combinations, and reached a peak validation accuracy of 73.7%.  Note, this peak job occurs relatively early in our search but, due to randomness, our next best objective value occurred 89 jobs later.  The actual peak could have occurred anywhere within the 120 jobs and will change across multiple runs.  We can see that with hyperparameter tuning our accuracy is well above the default value baseline of 53%.\n",
    "\n",
    "To get a rough understanding of how the hyperparameter values relate to one another and the objective metric, let's quickly plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(random_metrics[['objective',\n",
    "#                                            'learning_rate',\n",
    "#                                            'momentum',\n",
    "#                                            'wd',\n",
    "#                                            'job_number']],\n",
    "#                            figsize=(12, 12))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Deploying to Sagemaker Endpoint - Host\n",
    "\n",
    "### Now that we've trained our model, deploying it to a real-time, production endpoint is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, \n",
    "                     instance_type='ml.m4.xlarge')\n",
    "predictor.serializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an endpoint, let's test it out.  We'll predict user #6's ratings for the top and bottom ASINs from our local model.\n",
    "\n",
    "*This could be done by sending HTTP POST requests from a separate web service, but to keep things easy, we'll just use the `.predict()` method from the SageMaker Python SDK.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist(), \n",
    "                              'product_id': ['B00HSJRT7I', 'B001FA1O1S']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note, some of our predictions are actually greater than 5, which is to be expected as we didn't do anything special to account for ratings being capped at that value.  Since we are only looking to ranking by predicted rating, this won't create problems for our specific use case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist(), 'product_id': ['B00HSJRT7I', 'B001FA1O1S']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.sample(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Checking the Endpoint Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws sagemaker describe-endpoint --endpoint-name  sagemaker-mxnet-2020-05-18-03-55-49-266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Let's start by calculating a naive baseline to approximate how well our model is doing.  The simplest estimate would be to assume every user item rating is just the average rating over all ratings.\n",
    "\n",
    "*Note, we could do better by using each individual product's average, however, in this case it doesn't really matter as the same conclusions would hold.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive MSE:', np.mean((test_df['star_rating'] - np.mean(train_df['star_rating'])) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll calculate predictions for our test dataset.\n",
    "\n",
    "*Note, this will align closely to our CloudWatch output above, but may differ slightly due to skipping partial mini-batches in our eval_net function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for array in np.array_split(test_df[['customer_id', 'product_id']].values, 40):\n",
    "    test_preds += predictor.predict(json.dumps({'customer_id': array[:, 0].tolist(), \n",
    "                                                'product_id': array[:, 1].tolist()}))\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "print('MSE:', np.mean((test_df['star_rating'] - test_preds) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our neural network and embedding model produces substantially better results (~1.27 vs 1.65 on mean square error).\n",
    "\n",
    "For recommender systems, subjective accuracy also matters.  Let's get some recommendations for a random user to see if they make intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df[reduced_df['user'] == 6].sort_values(['star_rating', 'item'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.to_csv(\"./amazon_reviews_us_Shoes_v1_00.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, user #6 seems to like sprawling dramamtic television series and sci-fi, but they dislike silly comedies.\n",
    "\n",
    "Now we'll loop through and predict user #6's ratings for every common product in the catalog, to see which ones we'd recommend and which ones we wouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                 'product_id': array.tolist()}))\n",
    "\n",
    "predictions = pd.DataFrame({'product_id': product_index['product_id'],\n",
    "                            'prediction': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_user6 = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions_user6 += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                       'product_id': array.tolist()}))\n",
    "plt.scatter(predictions['prediction'], np.array(predictions_user6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = reduced_df.groupby('product_id')['product_title'].last().reset_index()\n",
    "predictions_titles = predictions.merge(titles)\n",
    "predictions_titles = predictions_titles.sort_values(['prediction', 'product_id'], ascending=[False, True])\n",
    "# pickup top 100 recommeded products only\n",
    "predictions_titles = predictions_titles.head(n=100)\n",
    "predictions_titles.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our predicted highly rated shows have some well-reviewed TV dramas and some sci-fi.  Meanwhile, our bottom rated shows include goofball comedies.\n",
    "\n",
    "*Note, because of random initialization in the weights, results on subsequent runs may differ slightly.*\n",
    "\n",
    "Let's confirm that we no longer have almost perfect correlation in recommendations with user #7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_titles.to_csv(\"./user_6_amazon_reviews_us_Shoes_v1_00.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find user_id from user index\n",
    "\n",
    "user_index = 3\n",
    "\n",
    "customer_index[customer_index['user'] == user_index]['customer_id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_index_list = customer_index['user'].tolist()\n",
    "\n",
    "print(\"Total number of cutomers: \", len(customer_index_list))\n",
    "\n",
    "# test\n",
    "# customer_index_list=[1,2,3]\n",
    "\n",
    "all_predictions_from_user = pd.DataFrame(columns=['customer_id', 'product_id', 'prediction', 'product_title'])\n",
    "\n",
    "for user_index in customer_index_list:\n",
    "    print(\"test_customer_index:\", user_index)\n",
    "    \n",
    "    predictions = []\n",
    "    for array in np.array_split(product_index['product_id'].values, 40):\n",
    "        predictions += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == user_index]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                     'product_id': array.tolist()}))\n",
    "\n",
    "    \n",
    "    customer_id = customer_index[customer_index['user'] == user_index]['customer_id'].values.tolist()\n",
    "    print(\"customer_id: \",customer_id[0])\n",
    "         \n",
    "    predictions = pd.DataFrame({\n",
    "                                'product_id': product_index['product_id'],\n",
    "                                'prediction': predictions})\n",
    "    predictions['customer_id'] = customer_id[0]\n",
    "    predictions  = predictions[['customer_id', 'product_id', 'prediction']]\n",
    "#     print(predictions.head(n=2))\n",
    "    \n",
    "    titles = reduced_df.groupby('product_id')['product_title'].last().reset_index()\n",
    "    predictions_titles = predictions.merge(titles)\n",
    "    predictions_titles = predictions_titles.sort_values(['prediction', 'product_id'], ascending=[False, True])\n",
    "    # pickup top 100 recommeded products only\n",
    "    predictions_titles = predictions_titles.head(n=100)\n",
    "    \n",
    "    print(predictions_titles.head(n=1))\n",
    "    #combine all results\n",
    "    all_predictions_from_user = pd.concat([all_predictions_from_user, predictions_titles])\n",
    "\n",
    "#reset index\n",
    "all_predictions_from_user = all_predictions_from_user.reset_index(drop=True)\n",
    "\n",
    "#generate csv file\n",
    "all_predictions_from_user.to_csv(\"Shoes_all_predictions_from_user.csv\")\n",
    "\n",
    "#generate pickle file\n",
    "all_predictions_from_user.to_csv(\"Shoes_all_predictions_from_user.pickle\")\n",
    "\n",
    "all_predictions_from_user.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this example, we developed a deep learning model to predict customer ratings.  This could serve as the foundation of a recommender system in a variety of use cases.  However, there are many ways in which it could be improved.  For example we did very little with:\n",
    "- hyperparameter tuning\n",
    "- controlling for overfitting (early stopping, dropout, etc.)\n",
    "- testing whether binarizing our target variable would improve results\n",
    "- including other information sources (historical ratings, time of review)\n",
    "- adjusting our threshold for user and item inclusion \n",
    "\n",
    "In addition to improving the model, we could improve the engineering by:\n",
    "- Setting the context and key value store up for distributed training\n",
    "- Fine tuning our data ingestion (e.g. num_workers on our data iterators) to ensure we're fully utilizing our GPU\n",
    "- Thinking about how pre-processing would need to change as datasets scale beyond a single machine\n",
    "\n",
    "Beyond that, recommenders are a very active area of research and techniques from active learning, reinforcement learning, segmentation, ensembling, and more should be investigated to deliver well-rounded recommendations.\n",
    "\n",
    "### Clean-up (optional)\n",
    "\n",
    "Let's finish by deleting our endpoint to avoid stray hosting charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference APID\n",
    "\n",
    "- MXNet Estimator\n",
    "    - https://sagemaker.readthedocs.io/en/stable/sagemaker.mxnet.html#mxnet-estimator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
