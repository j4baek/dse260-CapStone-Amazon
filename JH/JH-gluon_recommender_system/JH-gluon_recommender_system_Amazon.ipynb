{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Recommender System with SageMaker, MXNet, and Gluon\n",
    "_**Making Video Recommendations Using Neural Networks and Embeddings**_\n",
    "\n",
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "*This work is based on content from the [Cyrus Vahid's 2017 re:Invent Talk](https://github.com/cyrusmvahid/gluontutorials/blob/master/recommendations/MLPMF.ipynb)*\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "  1. [Explore](#Explore)\n",
    "  1. [Clean](#Clean)\n",
    "  1. [Prepare](#Prepare)\n",
    "1. [Train Locally](#Train-Locally)\n",
    "  1. [Define Network](#Define-Network)\n",
    "  1. [Set Parameters](#Set-Parameters)\n",
    "  1. [Execute](#Execute)\n",
    "1. [Train with SageMaker](#Train-with-SageMaker)\n",
    "  1. [Wrap Code](#Wrap-Code)\n",
    "  1. [Move Data](#Move-Data)\n",
    "  1. [Submit](#Submit)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "1. [Wrap-up](#Wrap-up)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.  The minimal required dataset for this is a history of user item ratings.  In our case, we'll use 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "Matrix factorization has been the cornerstone of most user-item prediction models.  This method starts with the large, sparse, user-item ratings in a single matrix, where users index the rows, and items index the columns.  It then seeks to find two lower-dimensional, dense matrices which, when multiplied together, preserve the information and relationships in the larger matrix.\n",
    "\n",
    "![image](https://data-artisans.com/img/blog/factorization.svg)\n",
    "\n",
    "Matrix factorization has been extended and genarlized with deep learning and embeddings.  These techniques allows us to introduce non-linearities for enhanced performance and flexibility.  This notebook will fit a neural network-based model to generate recommendations for the Amazon video dataset.  It will start by exploring our data in the notebook and even training a model on a sample of the data.  Later we'll expand to the full dataset and fit our model using a SageMaker managed training cluster.  We'll then deploy to an endpoint and check our method.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To measure all running time\n",
    "# https://github.com/cpcloud/ipython-autotime\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AWSGlueServiceSageMakerNotebookRole-amazonsagemaker to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 s\n"
     ]
    }
   ],
   "source": [
    "bucket = 'dse-cohort5-group1'\n",
    "prefix = 'JH-sagemaker/DEMO-gluon-recsys'\n",
    "\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Python libraries we'll need for the remainder of this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, ndarray\n",
    "from mxnet.metric import MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "### Explore\n",
    "\n",
    "Let's start by bringing in our dataset from an S3 public bucket.  As mentioned above, this contains 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "_Note, because this dataset is over a half gigabyte, the load from S3 may take ~10 minutes.  Also, since Amazon SageMaker Notebooks start with a 5GB persistent volume by default, and we don't need to keep this data on our instance for long, we'll bring it to the temporary volume (which has up to 20GB of storage)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 814 Âµs\n"
     ]
    }
   ],
   "source": [
    "# !mkdir /tmp/recsys/\n",
    "# !aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) so that we can begin to understand it.\n",
    "\n",
    "*Note, we'll set `error_bad_lines=False` when reading the file in as there appear to be a very small number of records which would create a problem otherwise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A2IC3NZN488KWK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0871167042</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A3OT9BYASFGU2X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A28GK1G2KDXHRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A3NFXFEKW8OK0E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A3I6G5TKBVJEK9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  star_rating     customer_id\n",
       "0  0871167042          5.0  A2IC3NZN488KWK\n",
       "1  0871167042          4.0  A3OT9BYASFGU2X\n",
       "2  0871167042          5.0  A28GK1G2KDXHRP\n",
       "3  0871167042          5.0  A3NFXFEKW8OK0E\n",
       "4  0871167042          5.0  A3I6G5TKBVJEK9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./asin_overall_reviewerID_with_voted_review.csv', names=['product_id', 'star_rating', 'customer_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "- Format is one-review-per-line in json. See examples below for further help reading the data.\n",
    "\n",
    "    - reviewerID ( customer_id ) - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "    - asin ( product_id )- ID of the product, e.g. 0000013714\n",
    "    - reviewerName - name of the reviewer\n",
    "    - vote - helpful votes of the review\n",
    "    - style - a disctionary of the product metadata, e.g., \"Format\" is \"Hardcover\"\n",
    "    - reviewText - text of the review\n",
    "    - overall ( star_rating )- rating of the product\n",
    "    - summary - summary of the review\n",
    "    - unixReviewTime - time of the review (unix time)\n",
    "    - reviewTime - time of the review (raw)\n",
    "    - image - images that users post after they have received the produc\n",
    "    \n",
    "## Drop and Clean data\n",
    "    - Drop null in Vote\n",
    "    - Voted review comment is more reliable.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "df = df[['customer_id', 'product_id', 'star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IC3NZN488KWK</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3OT9BYASFGU2X</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A28GK1G2KDXHRP</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id  product_id  star_rating\n",
       "0  A2IC3NZN488KWK  0871167042          5.0\n",
       "1  A3OT9BYASFGU2X  0871167042          4.0\n",
       "2  A28GK1G2KDXHRP  0871167042          5.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.04 ms\n"
     ]
    }
   ],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32292099, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.73 ms\n"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Because most people haven't review most products, and people rate fewer products than we actually use, we'd expect our data to be sparse.  \n",
    "#### - Our algorithm should work well with this sparse problem in general, but we may still want to clean out some of the long tail.  \n",
    "#### - Let's look at some basic percentiles to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers\n",
      " 0.00      1.0\n",
      "0.01      1.0\n",
      "0.02      1.0\n",
      "0.03      1.0\n",
      "0.04      1.0\n",
      "0.05      1.0\n",
      "0.10      1.0\n",
      "0.25      1.0\n",
      "0.50      1.0\n",
      "0.75      3.0\n",
      "0.90      5.0\n",
      "0.95      8.0\n",
      "0.96      9.0\n",
      "0.97     11.0\n",
      "0.98     13.0\n",
      "0.99     19.0\n",
      "1.00    656.0\n",
      "Name: customer_id, dtype: float64\n",
      "===========================================\n",
      "products\n",
      " 0.00        1.0\n",
      "0.01        1.0\n",
      "0.02        1.0\n",
      "0.03        1.0\n",
      "0.04        1.0\n",
      "0.05        1.0\n",
      "0.10        1.0\n",
      "0.25        1.0\n",
      "0.50        2.0\n",
      "0.75        5.0\n",
      "0.90       16.0\n",
      "0.95       36.0\n",
      "0.96       47.0\n",
      "0.97       63.0\n",
      "0.98       94.0\n",
      "0.99      178.0\n",
      "1.00    19702.0\n",
      "Name: product_id, dtype: float64\n",
      "time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "print('===========================================')\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, only about 10% of customers have rated 5 or more products, and only 10% of products have been rated by 16+ customers.\n",
    "\n",
    "## Clean\n",
    "\n",
    "### - Let's filter out this long tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 16]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IC3NZN488KWK</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A30FG02C424EJ5</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2G9GWQEWWNQUB</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id  product_id  star_rating\n",
       "0  A2IC3NZN488KWK  0871167042          5.0\n",
       "1  A30FG02C424EJ5  0871167042          5.0\n",
       "2  A2G9GWQEWWNQUB  0871167042          5.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.21 ms\n"
     ]
    }
   ],
   "source": [
    "reduced_df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering out long tail:  (11634699, 3)\n",
      "time: 1.02 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"After filtering out long tail: \", reduced_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The removed rows after filtering out long tail:  20657400\n",
      "time: 987 Âµs\n"
     ]
    }
   ],
   "source": [
    "print(\"The removed rows after filtering out long tail: \", df.shape[0]-reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe to csv/pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.to_csv('remove_long_tail_customer_product_start_with_voted_review.csv', index=False)\n",
    "reduced_df.to_pickle('remove_long_tail_customer_product_start_with_voted_review.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframe from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "reduced_df = pd.read_csv('remove_long_tail_customer_product_start_with_voted_review.csv')\n",
    "# temp_df = pd.read_pickle('remove_long_tail_customer_product_start_with_voted_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IC3NZN488KWK</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A30FG02C424EJ5</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id  product_id  star_rating\n",
       "0  A2IC3NZN488KWK  0871167042          5.0\n",
       "1  A30FG02C424EJ5  0871167042          5.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.7 ms\n"
     ]
    }
   ],
   "source": [
    "reduced_df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7G\n",
      "drwxrwxr-x 4 ec2-user ec2-user 4.0K May 14 22:53 .\n",
      "drwxrwxr-x 6 ec2-user ec2-user 4.0K May 14 18:18 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 916M May 14 18:17 asin_overall_reviewerID_with_voted_review.csv\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K May 14 21:33 .ipynb_checkpoints\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  41K May 14 22:53 JH-gluon_recommender_system_Amazon.ipynb\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K May 14 22:48 __pycache__\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 9.4K May 14 22:46 recommender.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 330M May 14 22:43 remove_long_tail_customer_product_start_with_voted_review.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 409M May 14 22:35 remove_long_tail_customer_product_start_with_voted_review.pickle\n",
      "time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll recreate our customer and product lists since there are customers with more than 5 reviews, but all of their reviews are on products with less than 5 reviews (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.71 s\n"
     ]
    }
   ],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll number each user and item, giving them their own sequential index.  This will allow us to hold the information in a sparse format where the sequential indices indicate the row and column in our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IC3NZN488KWK</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>99781</td>\n",
       "      <td>144877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A30FG02C424EJ5</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>867499</td>\n",
       "      <td>144877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2G9GWQEWWNQUB</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>331727</td>\n",
       "      <td>144877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3NI5OGW35SLY2</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>789540</td>\n",
       "      <td>144877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1OPRA4NE56EV6</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>917476</td>\n",
       "      <td>144877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id  product_id  star_rating    user    item\n",
       "0  A2IC3NZN488KWK  0871167042          5.0   99781  144877\n",
       "1  A30FG02C424EJ5  0871167042          5.0  867499  144877\n",
       "2  A2G9GWQEWWNQUB  0871167042          5.0  331727  144877\n",
       "3  A3NI5OGW35SLY2  0871167042          5.0  789540  144877\n",
       "4  A1OPRA4NE56EV6  0871167042          5.0  917476  144877"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0])})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "Let's start by splitting in training and test sets.  This will allow us to estimate the model's accuracy on videos our customers rated, but wasn't included in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert our Pandas DataFrames into MXNet NDArrays, use those to create a member of the SparseMatrixDataset class, and add that to an MXNet Data Iterator.  This process is the same for both test and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 465 ms\n"
     ]
    }
   ],
   "source": [
    "# [JH] need to tune batch_size for hyperparameter \n",
    "batch_size = 1024\n",
    "\n",
    "train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['star_rating'].values, dtype=np.float32))\n",
    "test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['star_rating'].values, dtype=np.float32))\n",
    "\n",
    "train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\n",
    "test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Locally\n",
    "\n",
    "### Define Network\n",
    "\n",
    "### [JH] Need to tune by hyperparameter\n",
    "\n",
    "Let's start by defining the neural network version of our matrix factorization task.  In this case, our network is quite simple.  The main components are:\n",
    "- [Embeddings](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding) which turn our indexes into dense vectors of fixed size.  In this case, 64.\n",
    "- [Dense layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dense) with ReLU activation.  Each dense layer has the same number of units as our number of embeddings.  Our ReLU activation here also adds some non-linearity to our matrix factorization.\n",
    "- [Dropout layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dropout) which can be used to prevent over-fitting.\n",
    "- Matrix multiplication of our user matrix and our item matrix to create an estimate of our rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.45 ms\n"
     ]
    }
   ],
   "source": [
    "class MFBlock(gluon.HybridBlock):\n",
    "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\n",
    "        super(MFBlock, self).__init__()\n",
    "        \n",
    "        self.max_users = max_users\n",
    "        self.max_items = max_items\n",
    "        self.dropout_p = dropout_p\n",
    "        self.num_emb = num_emb\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\n",
    "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\n",
    "            \n",
    "            self.dropout_user = gluon.nn.Dropout(dropout_p)\n",
    "            self.dropout_item = gluon.nn.Dropout(dropout_p)\n",
    "\n",
    "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            \n",
    "    def hybrid_forward(self, F, users, items):\n",
    "        a = self.user_embeddings(users)\n",
    "        a = self.dense_user(a)\n",
    "        \n",
    "        b = self.item_embeddings(items)\n",
    "        b = self.dense_item(b)\n",
    "\n",
    "        predictions = self.dropout_user(a) * self.dropout_item(b)     \n",
    "        predictions = F.sum(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.88 ms\n"
     ]
    }
   ],
   "source": [
    "# [JH] Need to tune by hyperparameter\n",
    "num_embeddings = 64\n",
    "\n",
    "net = MFBlock(max_users=customer_index.shape[0], \n",
    "              max_items=product_index.shape[0],\n",
    "              num_emb=num_embeddings,\n",
    "              dropout_p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters\n",
    "\n",
    "Let's initialize network weights and set our optimization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set GPU:  gpu(0)\n",
      "time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize network parameters\n",
    "ctx = mx.gpu()\n",
    "print(\"Set GPU: \", ctx\n",
    "     )\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=60),\n",
    "                                ctx=ctx,\n",
    "                                force_reinit=True)\n",
    "net.hybridize()\n",
    "\n",
    "# [JH] Need to tune by hpyerparameter\n",
    "# Set optimization parameters\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0.\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n",
    "\n",
    "Let's define a function to carry out the training of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.24 ms\n"
     ]
    }
   ],
   "source": [
    "def execute(train_iter, test_iter, net, epochs, ctx):\n",
    "    \n",
    "    loss_function = gluon.loss.L2Loss()\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        print(\"epoch: {}\".format(e))\n",
    "        \n",
    "        for i, (user, item, label) in enumerate(train_iter):\n",
    "                user = user.as_in_context(ctx)\n",
    "                item = item.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                \n",
    "                with mx.autograd.record():\n",
    "                    output = net(user, item)               \n",
    "                    loss = loss_function(output, label)\n",
    "                    \n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "\n",
    "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\n",
    "                                                                   eval_net(train_iter, net, ctx, loss_function),\n",
    "                                                                   eval_net(test_iter, net, ctx, loss_function)))\n",
    "    print(\"end of training\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Let's also define a function which evaluates our network on a given dataset.  \n",
    "#### - This is called by our `execute` function above to provide `mean squared error` values on our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.11 ms\n"
     ]
    }
   ],
   "source": [
    "def eval_net(data, net, ctx, loss_function):\n",
    "    acc = MSE()\n",
    "    for i, (user, item, label) in enumerate(data):\n",
    "        \n",
    "            user = user.as_in_context(ctx)\n",
    "            item = item.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            predictions = net(user, item).reshape((batch_size, 1))\n",
    "            acc.update(preds=[predictions], labels=[label])\n",
    "   \n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 1.007230795285174. 1.0072267761242601\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 0.8999958781174043. 0.899986450896615\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 0.861638595366296. 0.8616399044480942\n",
      "end of training\n",
      "CPU times: user 27min 13s, sys: 7min 1s, total: 34min 15s\n",
      "Wall time: 24min 29s\n",
      "time: 24min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# [JH] Need to tune by hyperparameter\n",
    "epochs = 3\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Validation\n",
    "\n",
    "We can see our training error going down, but our validation accuracy bounces around a bit.  Let's check how our model is predicting for an individual user.  We could pick randomly, but for this case, let's try user #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>u6_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>B0149X6QKG</td>\n",
       "      <td>8351</td>\n",
       "      <td>4.307781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17238</th>\n",
       "      <td>B004OA7QVS</td>\n",
       "      <td>17238</td>\n",
       "      <td>4.305843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>B00ES8FDZC</td>\n",
       "      <td>946</td>\n",
       "      <td>4.269756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16771</th>\n",
       "      <td>B00ODXK276</td>\n",
       "      <td>16771</td>\n",
       "      <td>4.266338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5587</th>\n",
       "      <td>B01ELCZAX4</td>\n",
       "      <td>5587</td>\n",
       "      <td>4.250441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31685</th>\n",
       "      <td>B01D2UI1Q4</td>\n",
       "      <td>31685</td>\n",
       "      <td>2.130192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19496</th>\n",
       "      <td>B00P3U6SVM</td>\n",
       "      <td>19496</td>\n",
       "      <td>2.097210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42640</th>\n",
       "      <td>B000JL2KJE</td>\n",
       "      <td>42640</td>\n",
       "      <td>2.017553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23739</th>\n",
       "      <td>B01FLKKNOG</td>\n",
       "      <td>23739</td>\n",
       "      <td>1.999463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12934</th>\n",
       "      <td>B01E71G6QE</td>\n",
       "      <td>12934</td>\n",
       "      <td>1.983527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277909 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id   item  u6_predictions\n",
       "8351   B0149X6QKG   8351        4.307781\n",
       "17238  B004OA7QVS  17238        4.305843\n",
       "946    B00ES8FDZC    946        4.269756\n",
       "16771  B00ODXK276  16771        4.266338\n",
       "5587   B01ELCZAX4   5587        4.250441\n",
       "...           ...    ...             ...\n",
       "31685  B01D2UI1Q4  31685        2.130192\n",
       "19496  B00P3U6SVM  19496        2.097210\n",
       "42640  B000JL2KJE  42640        2.017553\n",
       "23739  B01FLKKNOG  23739        1.999463\n",
       "12934  B01E71G6QE  12934        1.983527\n",
       "\n",
       "[277909 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "product_index['u6_predictions'] = trained_net(nd.array([6] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u6_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this to the predictions for another user (we'll try user #7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>u6_predictions</th>\n",
       "      <th>u7_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>B0149X6QKG</td>\n",
       "      <td>8351</td>\n",
       "      <td>4.307781</td>\n",
       "      <td>5.550335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17238</th>\n",
       "      <td>B004OA7QVS</td>\n",
       "      <td>17238</td>\n",
       "      <td>4.305843</td>\n",
       "      <td>5.517408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16771</th>\n",
       "      <td>B00ODXK276</td>\n",
       "      <td>16771</td>\n",
       "      <td>4.266338</td>\n",
       "      <td>5.496499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>B00ES8FDZC</td>\n",
       "      <td>946</td>\n",
       "      <td>4.269756</td>\n",
       "      <td>5.471542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>B00FN3CSH2</td>\n",
       "      <td>6862</td>\n",
       "      <td>4.242658</td>\n",
       "      <td>5.465276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39725</th>\n",
       "      <td>B00IZB0AII</td>\n",
       "      <td>39725</td>\n",
       "      <td>2.155313</td>\n",
       "      <td>2.770017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19496</th>\n",
       "      <td>B00P3U6SVM</td>\n",
       "      <td>19496</td>\n",
       "      <td>2.097210</td>\n",
       "      <td>2.723620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23739</th>\n",
       "      <td>B01FLKKNOG</td>\n",
       "      <td>23739</td>\n",
       "      <td>1.999463</td>\n",
       "      <td>2.598801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42640</th>\n",
       "      <td>B000JL2KJE</td>\n",
       "      <td>42640</td>\n",
       "      <td>2.017553</td>\n",
       "      <td>2.598763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12934</th>\n",
       "      <td>B01E71G6QE</td>\n",
       "      <td>12934</td>\n",
       "      <td>1.983527</td>\n",
       "      <td>2.596066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277909 rows Ã 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id   item  u6_predictions  u7_predictions\n",
       "8351   B0149X6QKG   8351        4.307781        5.550335\n",
       "17238  B004OA7QVS  17238        4.305843        5.517408\n",
       "16771  B00ODXK276  16771        4.266338        5.496499\n",
       "946    B00ES8FDZC    946        4.269756        5.471542\n",
       "6862   B00FN3CSH2   6862        4.242658        5.465276\n",
       "...           ...    ...             ...             ...\n",
       "39725  B00IZB0AII  39725        2.155313        2.770017\n",
       "19496  B00P3U6SVM  19496        2.097210        2.723620\n",
       "23739  B01FLKKNOG  23739        1.999463        2.598801\n",
       "42640  B000JL2KJE  42640        2.017553        2.598763\n",
       "12934  B01E71G6QE  12934        1.983527        2.596066\n",
       "\n",
       "[277909 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 137 ms\n"
     ]
    }
   ],
   "source": [
    "product_index['u7_predictions'] = trained_net(nd.array([7] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u7_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted ratings are different between the two users, but the same top (and bottom) items for user #6 appear for #7 as well.  Let's look at the correlation across the full set of 38K items to see if this relationship holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "product_index[['u6_predictions', 'u7_predictions']].plot.scatter('u6_predictions', 'u7_predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this correlation is nearly perfect.  Essentially the average rating of items dominates across users and we'll recommend the same well-reviewed items to everyone.  As it turns out, we can add more embeddings and this relationship will go away since we're better able to capture differential preferences across users.\n",
    "\n",
    "However, with just a 64 dimensional embedding, it took 7 minutes to run just 3 epochs.  If we ran this outside of our Notebook Instance we could run larger jobs and move on to other work would improve productivity.\n",
    "\n",
    "---\n",
    "\n",
    "## Train with SageMaker\n",
    "\n",
    "Now that we've trained on this smaller dataset, we can expand training in SageMaker's distributed, managed training environment.\n",
    "\n",
    "### Wrap Code\n",
    "\n",
    "To use SageMaker's pre-built MXNet container, we'll need to wrap our code from above into a Python script.  There's a great deal of flexibility in using SageMaker's pre-built containers, and detailed documentation can be found [here](https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators), but for our example, it consisted of:\n",
    "1. Wrapping all data preparation into a `prepare_train_data` function (we could name this whatever we like)\n",
    "1. Copying and pasting classes and functions from above word-for-word\n",
    "1. Defining a `train` function that:\n",
    "  1. Adds a bit of new code to pick up the input TSV dataset on the SageMaker Training cluster\n",
    "  1. Takes in a dict of hyperparameters (which we specified as globals above)\n",
    "  1. Creates the net and executes training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\n",
      "import json\n",
      "import time\n",
      "import os\n",
      "import mxnet as mx\n",
      "from mxnet import gluon, nd, ndarray\n",
      "from mxnet.metric import MSE\n",
      "import numpy as np\n",
      "\n",
      "os.system('pip install pandas')\n",
      "import pandas as pd\n",
      "\n",
      "logging.basicConfig(level=logging.DEBUG)\n",
      "\n",
      "#########\n",
      "# Globals\n",
      "#########\n",
      "\n",
      "batch_size = 1024\n",
      "\n",
      "\n",
      "##########\n",
      "# Training\n",
      "##########\n",
      "\n",
      "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, **kwargs):\n",
      "    \n",
      "    # get data\n",
      "    training_dir = channel_input_dirs['train']\n",
      "    train_iter, test_iter, customer_index, product_index = prepare_train_data(training_dir)\n",
      "    \n",
      "    # get hyperparameters\n",
      "    num_embeddings = hyperparameters.get('num_embeddings', 64)\n",
      "    opt = hyperparameters.get('opt', 'sgd')\n",
      "    lr = hyperparameters.get('lr', 0.02)\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\n",
      "    wd = hyperparameters.get('wd', 0.)\n",
      "    epochs = hyperparameters.get('epochs', 5)\n",
      "\n",
      "    # define net\n",
      "    ctx = mx.gpu()\n",
      "\n",
      "    net = MFBlock(max_users=customer_index.shape[0], \n",
      "                  max_items=product_index.shape[0],\n",
      "                  num_emb=num_embeddings,\n",
      "                  dropout_p=0.5)\n",
      "    \n",
      "    net.collect_params().initialize(mx.init.Xavier(magnitude=60),\n",
      "                                    ctx=ctx,\n",
      "                                    force_reinit=True)\n",
      "    net.hybridize()\n",
      "\n",
      "    trainer = gluon.Trainer(net.collect_params(),\n",
      "                            opt,\n",
      "                            {'learning_rate': lr,\n",
      "                             'wd': wd,\n",
      "                             'momentum': momentum})\n",
      "    \n",
      "    # execute\n",
      "    trained_net = execute(train_iter, test_iter, net, trainer, epochs, ctx)\n",
      "    \n",
      "    return trained_net, customer_index, product_index\n",
      "\n",
      "\n",
      "class MFBlock(gluon.HybridBlock):\n",
      "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\n",
      "        super(MFBlock, self).__init__()\n",
      "        \n",
      "        self.max_users = max_users\n",
      "        self.max_items = max_items\n",
      "        self.dropout_p = dropout_p\n",
      "        self.num_emb = num_emb\n",
      "        \n",
      "        with self.name_scope():\n",
      "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\n",
      "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\n",
      "\n",
      "            self.dropout_user = gluon.nn.Dropout(dropout_p)\n",
      "            self.dropout_item = gluon.nn.Dropout(dropout_p)\n",
      "\n",
      "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\n",
      "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\n",
      "            \n",
      "    def hybrid_forward(self, F, users, items):\n",
      "        a = self.user_embeddings(users)\n",
      "        a = self.dense_user(a)\n",
      "        \n",
      "        b = self.item_embeddings(items)\n",
      "        b = self.dense_item(b)\n",
      "\n",
      "        predictions = self.dropout_user(a) * self.dropout_item(b)      \n",
      "        predictions = F.sum(predictions, axis=1)\n",
      "\n",
      "        return predictions\n",
      "\n",
      "    \n",
      "def execute(train_iter, test_iter, net, trainer, epochs, ctx):\n",
      "    loss_function = gluon.loss.L2Loss()\n",
      "    for e in range(epochs):\n",
      "        print(\"epoch: {}\".format(e))\n",
      "        for i, (user, item, label) in enumerate(train_iter):\n",
      "\n",
      "                user = user.as_in_context(ctx)\n",
      "                item = item.as_in_context(ctx)\n",
      "                label = label.as_in_context(ctx)\n",
      "\n",
      "                with mx.autograd.record():\n",
      "                    output = net(user, item)               \n",
      "                    loss = loss_function(output, label)\n",
      "                loss.backward()\n",
      "                trainer.step(batch_size)\n",
      "\n",
      "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\n",
      "                                                                   eval_net(train_iter, net, ctx, loss_function),\n",
      "                                                                   eval_net(test_iter, net, ctx, loss_function)))\n",
      "    print(\"end of training\")\n",
      "    return net\n",
      "\n",
      "\n",
      "def eval_net(data, net, ctx, loss_function):\n",
      "    acc = MSE()\n",
      "    for i, (user, item, label) in enumerate(data):\n",
      "\n",
      "            user = user.as_in_context(ctx)\n",
      "            item = item.as_in_context(ctx)\n",
      "            label = label.as_in_context(ctx)\n",
      "\n",
      "            predictions = net(user, item).reshape((batch_size, 1))\n",
      "            acc.update(preds=[predictions], labels=[label])\n",
      "\n",
      "    return acc.get()[1]\n",
      "\n",
      "\n",
      "def save(model, model_dir):\n",
      "    net, customer_index, product_index = model\n",
      "    net.save_params('{}/model.params'.format(model_dir))\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'w')\n",
      "    json.dump({'max_users': net.max_users,\n",
      "               'max_items': net.max_items,\n",
      "               'num_emb': net.num_emb,\n",
      "               'dropout_p': net.dropout_p},\n",
      "              f)\n",
      "    f.close()\n",
      "    customer_index.to_csv('{}/customer_index.csv'.format(model_dir), index=False)\n",
      "    product_index.to_csv('{}/product_index.csv'.format(model_dir), index=False)\n",
      "\n",
      "    \n",
      "######\n",
      "# Data\n",
      "######\n",
      "\n",
      "def prepare_train_data(training_dir):\n",
      "#     f = os.listdir(training_dir)\n",
      "#     df = pd.read_csv(os.path.join(training_dir, f[0]), delimiter='\\t', error_bad_lines=False)\n",
      "    print(training_dir)\n",
      "    df = pd.read_csv(training_dir)\n",
      "#     df = pd.read_pickle(training_dir)\n",
      "    df = df[['customer_id', 'product_id', 'star_rating']]\n",
      "    customers = df['customer_id'].value_counts()\n",
      "    products = df['product_id'].value_counts()\n",
      "    \n",
      "    # Filter long-tail\n",
      "    customers = customers[customers >= 5]\n",
      "    products = products[products >= 10]\n",
      "\n",
      "    reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))\n",
      "    customers = reduced_df['customer_id'].value_counts()\n",
      "    products = reduced_df['product_id'].value_counts()\n",
      "\n",
      "    # Number users and items\n",
      "    customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
      "    product_index = pd.DataFrame({'product_id': products.index, 'item': np.arange(products.shape[0])})\n",
      "\n",
      "    reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
      "\n",
      "    # Split train and test\n",
      "    test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
      "\n",
      "    train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
      "                                on=['customer_id', 'product_id'], \n",
      "                                how='outer', \n",
      "                                indicator=True)\n",
      "    train_df = train_df[(train_df['_merge'] == 'left_only')]\n",
      "\n",
      "    # MXNet data iterators\n",
      "#     train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32), \n",
      "#                                     nd.array(train_df['item'].values, dtype=np.float32),\n",
      "#                                     nd.array(train_df['star_rating'].values, dtype=np.float32))\n",
      "#     test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32), \n",
      "#                                     nd.array(test_df['item'].values, dtype=np.float32),\n",
      "#                                     nd.array(test_df['star_rating'].values, dtype=np.float32))\n",
      "    \n",
      "    # [JH] need to tune batch_size for hyperparameter \n",
      "#     batch_size = 1024\n",
      "\n",
      "    train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32),\n",
      "                                    nd.array(train_df['item'].values, dtype=np.float32),\n",
      "                                    nd.array(train_df['star_rating'].values, dtype=np.float32))\n",
      "    test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32),\n",
      "                                    nd.array(test_df['item'].values, dtype=np.float32),\n",
      "                                    nd.array(test_df['star_rating'].values, dtype=np.float32))\n",
      "\n",
      "    train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\n",
      "    test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\n",
      "\n",
      "    return train_iter, test_iter, customer_index, product_index \n",
      "\n",
      "#########\n",
      "# Hosting\n",
      "#########\n",
      "\n",
      "def model_fn(model_dir):\n",
      "    \"\"\"\n",
      "    Load the gluon model. Called once when hosting service starts.\n",
      "\n",
      "    :param: model_dir The directory where model files are stored.\n",
      "    :return: a model (in this case a Gluon network)\n",
      "    \"\"\"\n",
      "    ctx = mx.cpu()\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'r')\n",
      "    block_params = json.load(f)\n",
      "    f.close()\n",
      "    net = MFBlock(max_users=block_params['max_users'], \n",
      "                  max_items=block_params['max_items'],\n",
      "                  num_emb=block_params['num_emb'],\n",
      "                  dropout_p=block_params['dropout_p'])\n",
      "    net.load_params('{}/model.params'.format(model_dir), ctx)\n",
      "    customer_index = pd.read_csv('{}/customer_index.csv'.format(model_dir))\n",
      "    product_index = pd.read_csv('{}/product_index.csv'.format(model_dir))\n",
      "    return net, customer_index, product_index\n",
      "\n",
      "\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\n",
      "    \"\"\"\n",
      "    Transform a request using the Gluon model. Called once per request.\n",
      "\n",
      "    :param net: The Gluon model.\n",
      "    :param data: The request payload.\n",
      "    :param input_content_type: The request content type.\n",
      "    :param output_content_type: The (desired) response content type.\n",
      "    :return: response payload and content type.\n",
      "    \"\"\"\n",
      "    ctx = mx.cpu()\n",
      "    parsed = json.loads(data)\n",
      "\n",
      "    trained_net, customer_index, product_index = net\n",
      "    users = pd.DataFrame({'customer_id': parsed['customer_id']}).merge(customer_index, how='left')['user'].values\n",
      "    items = pd.DataFrame({'product_id': parsed['product_id']}).merge(product_index, how='left')['item'].values\n",
      "    \n",
      "    predictions = trained_net(nd.array(users).as_in_context(ctx), nd.array(items).as_in_context(ctx))\n",
      "    response_body = json.dumps(predictions.asnumpy().tolist())\n",
      "\n",
      "    return response_body, output_content_type\n",
      "time: 207 ms\n"
     ]
    }
   ],
   "source": [
    "!cat recommender.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Locally\n",
    "\n",
    "Now we can test our train function locally.  This helps ensure we don't have any bugs before submitting our code to SageMaker's pre-built MXNet container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/dse260-CapStone-Amazon/JH/JH-gluon_recommender_system\n",
      "time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./remove_long_tail_customer_product_start_with_voted_review.csv\n",
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 0.9664453546489093. 0.9664322767348427\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 0.8999113434460612. 0.8999067421581192\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 0.8642080704547411. 0.8642087107196188\n",
      "end of training\n",
      "CPU times: user 21min 48s, sys: 4min 47s, total: 26min 36s\n",
      "Wall time: 20min 2s\n",
      "time: 20min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# need to update recommender.py code for my case\n",
    "# del recommender\n",
    "import recommender\n",
    "\n",
    "local_test_net, local_customer_index, local_product_index = recommender.train(\n",
    "    {'train': './remove_long_tail_customer_product_start_with_voted_review.csv'}, \n",
    "    {'num_embeddings': 64, \n",
    "     'opt': 'sgd', \n",
    "     'lr': 0.02, \n",
    "     'momentum': 0.9, \n",
    "     'wd': 0.,\n",
    "     'epochs': 3},\n",
    "    ['local'],\n",
    "    1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Data\n",
    "\n",
    "Holding our data in memory works fine when we're interactively exploring a sample of data, but for larger, longer running processes, we'd prefer to run them in the background with SageMaker Training.  To do this, let's move the dataset to S3 so that it can be picked up by SageMaker training.  This is perfect for use cases like periodic re-training, expanding to a larger dataset, or moving production workloads to larger hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.18 ms\n"
     ]
    }
   ],
   "source": [
    "# change log level\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler()) # Writes to console\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.getLogger('boto3').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('botocore').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('s3transfer').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('urllib3').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "import boto3 \n",
    "import os \n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv() # this loads the .env file with our credentials\n",
    "\n",
    "file_name = 'remove_long_tail_customer_product_start_with_voted_review.csv' # name of the file to upload\n",
    "file_name_1 = 'remove_long_tail_customer_product_start_with_voted_review.pickle' # name of the file to upload\n",
    "bucket_name = 'dse-cohort5-group1' # name of the bucket\n",
    "\n",
    "AWS_ACCESS_KEY_ID='AKIAZAERIKDLAVRFZK35'\n",
    "AWS_SECRET_ACCESS_KEY='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'\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv(AWS_ACCESS_KEY_ID),\n",
    "    aws_secret_access_key=os.getenv(AWS_SECRET_ACCESS_KEY)\n",
    ")\n",
    "\n",
    "# s3://dse-cohort5-group1/data-lake-landing-zone/reviews/Clothing_Shoes_and_Jewelry.json.gz\n",
    "key_file='data-lake-landing-zone/reviews/'+file_name\n",
    "response = s3_client.upload_file(file_name, bucket_name, key_file)\n",
    "print(response)\n",
    "\n",
    "key_file='data-lake-landing-zone/reviews/'+file_name_1\n",
    "response = s3_client.upload_file(file_name_1, bucket_name, key_file)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 862 Âµs\n"
     ]
    }
   ],
   "source": [
    "# boto3.client('s3').copy({'Bucket': 'amazon-reviews-pds', \n",
    "#                          'Key': 'tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz'},\n",
    "#                         bucket,\n",
    "#                         prefix + '/train/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit\n",
    "\n",
    "Now, we can create an MXNet estimator from the SageMaker Python SDK.  To do so, we need to pass in:\n",
    "1. Instance type and count for our SageMaker Training cluster.  SageMaker's MXNet containers support distributed GPU training, so we could easily set this to multiple ml.p2 or ml.p3 instances if we wanted.\n",
    "  - *Note, this would require some changes to our recommender.py script as we would need to setup the context an key value store properly, as well as determining if and how to distribute the training data.*\n",
    "1. An S3 path for out model artifacts and a role with access to S3 input and output paths.\n",
    "1. Hyperparameters for our neural network.  Since with a 64 dimensional embedding, our recommendations reverted too closely to the mean, let's increase this by an order of magnitude when we train outside of our local instance.  We'll also increase the epochs to see how our accuracy evolves over time. We'll leave all other hyperparameters the same.\n",
    "\n",
    "Once we use `.fit()` this creates a SageMaker Training Job that spins up instances, loads the appropriate packages and data, runs our `train` function from `recommender.py`, wraps up and saves model artifacts to S3, and finishes by tearing down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 851 Âµs\n"
     ]
    }
   ],
   "source": [
    "bucket = 'dse-cohort5-group1'\n",
    "prefix = 'sagemaker/DEMO-gluon-recsys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2020-05-15-00-06-33-417\n",
      "Creating training-job with name: sagemaker-mxnet-2020-05-15-00-06-33-417\n",
      "DEBUG:sagemaker:train request: {\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"TrainingInputMode\": \"File\",\n",
      "        \"TrainingImage\": \"520713654638.dkr.ecr.us-east-1.amazonaws.com/sagemaker-mxnet:1.1-gpu-py3\"\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "        \"S3OutputPath\": \"s3://dse-cohort5-group1/sagemaker/DEMO-gluon-recsys/output\"\n",
      "    },\n",
      "    \"TrainingJobName\": \"sagemaker-mxnet-2020-05-15-00-06-33-417\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    },\n",
      "    \"ResourceConfig\": {\n",
      "        \"InstanceCount\": 1,\n",
      "        \"InstanceType\": \"ml.p2.xlarge\",\n",
      "        \"VolumeSizeInGB\": 30\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::618779922646:role/AWSGlueServiceSageMakerNotebookRole-amazonsagemaker\",\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://dse-cohort5-group1/sagemaker/DEMO-gluon-recsys/train/\",\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                }\n",
      "            },\n",
      "            \"ChannelName\": \"train\"\n",
      "        }\n",
      "    ],\n",
      "    \"HyperParameters\": {\n",
      "        \"num_embeddings\": \"512\",\n",
      "        \"opt\": \"\\\"sgd\\\"\",\n",
      "        \"lr\": \"0.02\",\n",
      "        \"momentum\": \"0.9\",\n",
      "        \"wd\": \"0.0\",\n",
      "        \"epochs\": \"10\",\n",
      "        \"sagemaker_submit_directory\": \"\\\"s3://dse-cohort5-group1/sagemaker-mxnet-2020-05-15-00-06-33-417/source/sourcedir.tar.gz\\\"\",\n",
      "        \"sagemaker_program\": \"\\\"recommender.py\\\"\",\n",
      "        \"sagemaker_enable_cloudwatch_metrics\": \"false\",\n",
      "        \"sagemaker_container_log_level\": \"20\",\n",
      "        \"sagemaker_job_name\": \"\\\"sagemaker-mxnet-2020-05-15-00-06-33-417\\\"\",\n",
      "        \"sagemaker_region\": \"\\\"us-east-1\\\"\"\n",
      "    },\n",
      "    \"DebugHookConfig\": {\n",
      "        \"S3OutputPath\": \"s3://dse-cohort5-group1/sagemaker/DEMO-gluon-recsys/output\",\n",
      "        \"CollectionConfigurations\": []\n",
      "    }\n",
      "}\n",
      "train request: {\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"TrainingInputMode\": \"File\",\n",
      "        \"TrainingImage\": \"520713654638.dkr.ecr.us-east-1.amazonaws.com/sagemaker-mxnet:1.1-gpu-py3\"\n",
      "    },\n",
      "    \"OutputDataConfig\": {\n",
      "        \"S3OutputPath\": \"s3://dse-cohort5-group1/sagemaker/DEMO-gluon-recsys/output\"\n",
      "    },\n",
      "    \"TrainingJobName\": \"sagemaker-mxnet-2020-05-15-00-06-33-417\",\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400\n",
      "    },\n",
      "    \"ResourceConfig\": {\n",
      "        \"InstanceCount\": 1,\n",
      "        \"InstanceType\": \"ml.p2.xlarge\",\n",
      "        \"VolumeSizeInGB\": 30\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::618779922646:role/AWSGlueServiceSageMakerNotebookRole-amazonsagemaker\",\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://dse-cohort5-group1/sagemaker/DEMO-gluon-recsys/train/\",\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                }\n",
      "            },\n",
      "            \"ChannelName\": \"train\"\n",
      "        }\n",
      "    ],\n",
      "    \"HyperParameters\": {\n",
      "        \"num_embeddings\": \"512\",\n",
      "        \"opt\": \"\\\"sgd\\\"\",\n",
      "        \"lr\": \"0.02\",\n",
      "        \"momentum\": \"0.9\",\n",
      "        \"wd\": \"0.0\",\n",
      "        \"epochs\": \"10\",\n",
      "        \"sagemaker_submit_directory\": \"\\\"s3://dse-cohort5-group1/sagemaker-mxnet-2020-05-15-00-06-33-417/source/sourcedir.tar.gz\\\"\",\n",
      "        \"sagemaker_program\": \"\\\"recommender.py\\\"\",\n",
      "        \"sagemaker_enable_cloudwatch_metrics\": \"false\",\n",
      "        \"sagemaker_container_log_level\": \"20\",\n",
      "        \"sagemaker_job_name\": \"\\\"sagemaker-mxnet-2020-05-15-00-06-33-417\\\"\",\n",
      "        \"sagemaker_region\": \"\\\"us-east-1\\\"\"\n",
      "    },\n",
      "    \"DebugHookConfig\": {\n",
      "        \"S3OutputPath\": \"s3://dse-cohort5-group1/sagemaker/DEMO-gluon-recsys/output\",\n",
      "        \"CollectionConfigurations\": []\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::618779922646:assumed-role/AWSGlueServiceSageMakerNotebookRole-amazonsagemaker/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:618779922646:training-job/sagemaker-mxnet-2020-05-15-00-06-33-417",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f15cd225dcf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m          framework_version='1.1')\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m's3://{}/{}/train/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     def process(\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::618779922646:assumed-role/AWSGlueServiceSageMakerNotebookRole-amazonsagemaker/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:618779922646:training-job/sagemaker-mxnet-2020-05-15-00-06-33-417"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 337 ms\n"
     ]
    }
   ],
   "source": [
    "m = MXNet('recommender.py', \n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.p2.xlarge\",\n",
    "          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "          hyperparameters={'num_embeddings': 512, \n",
    "                           'opt': opt, \n",
    "                           'lr': lr, \n",
    "                           'momentum': momentum, \n",
    "                           'wd': wd,\n",
    "                           'epochs': 10},\n",
    "         framework_version='1.1')\n",
    "\n",
    "m.fit({'train': 's3://{}/{}/train/'.format(bucket, prefix)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Now that we've trained our model, deploying it to a real-time, production endpoint is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, \n",
    "                     instance_type='ml.m4.xlarge')\n",
    "predictor.serializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an endpoint, let's test it out.  We'll predict user #6's ratings for the top and bottom ASINs from our local model.\n",
    "\n",
    "*This could be done by sending HTTP POST requests from a separate web service, but to keep things easy, we'll just use the `.predict()` method from the SageMaker Python SDK.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist(), \n",
    "                              'product_id': ['B00KH1O9HW', 'B00M5KODWO']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note, some of our predictions are actually greater than 5, which is to be expected as we didn't do anything special to account for ratings being capped at that value.  Since we are only looking to ranking by predicted rating, this won't create problems for our specific use case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Let's start by calculating a naive baseline to approximate how well our model is doing.  The simplest estimate would be to assume every user item rating is just the average rating over all ratings.\n",
    "\n",
    "*Note, we could do better by using each individual video's average, however, in this case it doesn't really matter as the same conclusions would hold.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive MSE:', np.mean((test_df['star_rating'] - np.mean(train_df['star_rating'])) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll calculate predictions for our test dataset.\n",
    "\n",
    "*Note, this will align closely to our CloudWatch output above, but may differ slightly due to skipping partial mini-batches in our eval_net function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for array in np.array_split(test_df[['customer_id', 'product_id']].values, 40):\n",
    "    test_preds += predictor.predict(json.dumps({'customer_id': array[:, 0].tolist(), \n",
    "                                                'product_id': array[:, 1].tolist()}))\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "print('MSE:', np.mean((test_df['star_rating'] - test_preds) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our neural network and embedding model produces substantially better results (~1.27 vs 1.65 on mean square error).\n",
    "\n",
    "For recommender systems, subjective accuracy also matters.  Let's get some recommendations for a random user to see if they make intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df[reduced_df['user'] == 6].sort_values(['star_rating', 'item'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, user #6 seems to like sprawling dramamtic television series and sci-fi, but they dislike silly comedies.\n",
    "\n",
    "Now we'll loop through and predict user #6's ratings for every common video in the catalog, to see which ones we'd recommend and which ones we wouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                 'product_id': array.tolist()}))\n",
    "\n",
    "predictions = pd.DataFrame({'product_id': product_index['product_id'],\n",
    "                            'prediction': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = reduced_df.groupby('product_id')['product_title'].last().reset_index()\n",
    "predictions_titles = predictions.merge(titles)\n",
    "predictions_titles.sort_values(['prediction', 'product_id'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our predicted highly rated shows have some well-reviewed TV dramas and some sci-fi.  Meanwhile, our bottom rated shows include goofball comedies.\n",
    "\n",
    "*Note, because of random initialization in the weights, results on subsequent runs may differ slightly.*\n",
    "\n",
    "Let's confirm that we no longer have almost perfect correlation in recommendations with user #7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_user7 = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions_user7 += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 7]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                       'product_id': array.tolist()}))\n",
    "plt.scatter(predictions['prediction'], np.array(predictions_user7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this example, we developed a deep learning model to predict customer ratings.  This could serve as the foundation of a recommender system in a variety of use cases.  However, there are many ways in which it could be improved.  For example we did very little with:\n",
    "- hyperparameter tuning\n",
    "- controlling for overfitting (early stopping, dropout, etc.)\n",
    "- testing whether binarizing our target variable would improve results\n",
    "- including other information sources (video genres, historical ratings, time of review)\n",
    "- adjusting our threshold for user and item inclusion \n",
    "\n",
    "In addition to improving the model, we could improve the engineering by:\n",
    "- Setting the context and key value store up for distributed training\n",
    "- Fine tuning our data ingestion (e.g. num_workers on our data iterators) to ensure we're fully utilizing our GPU\n",
    "- Thinking about how pre-processing would need to change as datasets scale beyond a single machine\n",
    "\n",
    "Beyond that, recommenders are a very active area of research and techniques from active learning, reinforcement learning, segmentation, ensembling, and more should be investigated to deliver well-rounded recommendations.\n",
    "\n",
    "### Clean-up (optional)\n",
    "\n",
    "Let's finish by deleting our endpoint to avoid stray hosting charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
