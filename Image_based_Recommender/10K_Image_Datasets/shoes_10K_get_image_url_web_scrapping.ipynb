{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.parse\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image as PilImage\n",
    "from io import BytesIO\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "# Get productIds\n",
    "shoes_df = pd.read_csv('./amazon_reviews_us_Shoes_v1_00_help_voted_And_cut_lognTail.csv')\n",
    "raw_productIds = shoes_df['product_id'].to_list()\n",
    "unique_products = list(np.unique(raw_productIds))\n",
    "products_10k = random.sample(unique_products,10000)\n",
    "products_20k = random.sample(unique_products,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shoes dataset: 155509\n",
      "Unique shoes products: 97758\n",
      "10K samples: 10000\n",
      "20K samples: 20000\n",
      "time: 1.86 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw shoes dataset:\", len(raw_productIds))\n",
    "print(\"Unique shoes products:\", len(unique_products))\n",
    "print(\"10K samples:\", len(products_10k))\n",
    "print(\"20K samples:\", len(products_20k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42.1 ms\n"
     ]
    }
   ],
   "source": [
    "# top 1 recommended products for each user.\n",
    "# Get productIds\n",
    "shoes_df = pd.read_csv('../prediction_JH/Shoes_for_100_users_per_100_products_prediction_Ver4.csv')\n",
    "productIds_for_100_users = shoes_df['asin'].to_list()\n",
    "top100recommended = [productIds_for_100_users[i] for i in range(0,1000,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10K sample: 10088\n",
      "20K_sample: 20079\n",
      "time: 36.3 ms\n"
     ]
    }
   ],
   "source": [
    "for item in top100recommended:\n",
    "    products_10k.append(item)\n",
    "for item in top100recommended:\n",
    "    products_20k.append(item)\n",
    "\n",
    "products_10k = list(np.unique(products_10k))\n",
    "products_20k = list(np.unique(products_20k))\n",
    "\n",
    "print(\"10K sample:\", len(products_10k))\n",
    "print(\"20K_sample:\", len(products_20k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50K_sample: 50050\n",
      "time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "products_50k = random.sample(unique_products,50000)\n",
    "for item in top100recommended:\n",
    "    products_50k.append(item)\n",
    "products_50k = list(np.unique(products_50k))\n",
    "\n",
    "print(\"50K_sample:\", len(products_50k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image urls ( 10K dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "# Get website url\n",
    "product_urls_10k={}\n",
    "for i in products_10k:\n",
    "    url = urllib.parse.urljoin('https://www.amazon.com/dp/', i)\n",
    "    product_urls_10k[i]=url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 550 Âµs\n"
     ]
    }
   ],
   "source": [
    "# product_urls_10k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1h 5min 50s\n"
     ]
    }
   ],
   "source": [
    "# Execute Chromedriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"user-agent=whatever you want\")\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_urls_10k_list = list(product_urls_10k.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract img urls \n",
    "image_urls_10k={}\n",
    "for product in list(product_urls_10k.items()):\n",
    "    asin, url = product[0], product[1]\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content,\"lxml\")\n",
    "    for div in soup.find_all('div', id=\"imgTagWrapperId\"):\n",
    "        image=div.find('img', alt=True)\n",
    "        image_urls_10k[asin]=(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.49 ms\n"
     ]
    }
   ],
   "source": [
    "len(list(image_urls_10k.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 52 ms\n"
     ]
    }
   ],
   "source": [
    "asin_url_10k_df = pd.DataFrame.from_dict(image_urls_10k, orient ='index',columns=['url']).reset_index().rename(columns={'index': 'asin'})\n",
    "asin_url_10k_df.to_csv (r'./asin_url_for_10k.csv',index = False, header=True)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "# Download images\n",
    "for asin,url in image_urls_10k.items():\n",
    "    # Open the url image, set stream to True, this will return the stream content.\n",
    "    resp = requests.get(url, stream=True)\n",
    "    # Open a local file with wb ( write binary ) permission. (created increment filename)\n",
    "    path = os.path.join(\"./10K_Shoes_images/\", asin + \".\" + \"jpg\")\n",
    "#     print(path)\n",
    "    local_file = open(path,'wb')\n",
    "    # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "    resp.raw.decode_content = True\n",
    "    # Copy the response stream raw data to local image file.\n",
    "    shutil.copyfileobj(resp.raw, local_file)\n",
    "    # Remove the image url response object.\n",
    "    del resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image urls ( 10K dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get website url\n",
    "product_urls_20k={}\n",
    "for i in products_20k:\n",
    "    url = urllib.parse.urljoin('https://www.amazon.com/dp/', i)\n",
    "    product_urls_20k[i]=url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Chromedriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"user-agent=whatever you want\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Extract img urls \n",
    "image_urls_20k={}\n",
    "for product in list(product_urls_20k.items()):\n",
    "    asin, url = product[0], product[1]\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content,\"lxml\")\n",
    "    for div in soup.find_all('div', id=\"imgTagWrapperId\"):\n",
    "        image=div.find('img', alt=True)\n",
    "        image_urls_20k[asin]=(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
