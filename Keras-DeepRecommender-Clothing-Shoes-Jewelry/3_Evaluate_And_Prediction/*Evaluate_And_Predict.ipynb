{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and compare the different models\n",
    "Using the 10% of cross validated training set records  and the history I saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "#Keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and Check GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_check_gpu():\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.per_process_gpu_memory_fraction =1 # allow all of the GPU memory to be allocated\n",
    "    # for 8 GPUs\n",
    "    # cfg.gpu_options.visible_device_list = \"0,1,2,3,4,5,6,7\" # \"0,1\"\n",
    "    # for 1 GPU\n",
    "    cfg.gpu_options.visible_device_list = \"0\"\n",
    "    #cfg.gpu_options.allow_growth = True  # # Don't pre-allocate memory; dynamically allocate the memory used on the GPU as-needed\n",
    "    #cfg.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    sess = K.tf.Session(config=cfg)\n",
    "    K.set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    print(\"* TF version: \", [tf.__version__, tf.test.is_gpu_available()])\n",
    "    print(\"* List of GPU(s): \", tf.config.experimental.list_physical_devices() )\n",
    "    print(\"* Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU'))) \n",
    "  \n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "    # set for 8 GPUs\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\";\n",
    "    # set for 1 GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\";\n",
    "\n",
    "    # Tf debugging option\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "#     print(tf.config.list_logical_devices('GPU'))\n",
    "    print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* TF version:  ['1.15.2', True]\n",
      "* List of GPU(s):  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "* Num GPUs Available:  1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "set_check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_2_Concatenate_10_embeddings_10_epochs',\n",
       " 'dense_1_Multiply_50_embeddings_7_epochs',\n",
       " 'dense_2_Concatenate_20_embeddings_25_epochs',\n",
       " 'dense_1_Multiply_50_embeddings_4_epochs_dropout',\n",
       " 'dense_2_Meta_Concatenate_15_embeddings_50_epochs-DropOut0.6',\n",
       " 'matrix_facto_10_embeddings_3_epochs',\n",
       " 'dense_1_Multiply_50_embeddings_20_epochs_dropout',\n",
       " 'dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.6-dropout-input',\n",
       " 'dense_1_Multiply_50_embeddings_7_epochs_dropout',\n",
       " 'dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.8',\n",
       " 'matrix_facto_10_embeddings_100_epochs',\n",
       " 'dense_2_Meta_Concatenate_15_embeddings_30_epochs-DropOut0.6',\n",
       " 'matrix_facto_10_embeddings_20_epochs',\n",
       " 'dense_4_Multiply_5_embeddings_7_epochs',\n",
       " 'dense_1_Multiply_50_embeddings_2_epochs_dropout']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = './models'\n",
    "\n",
    "onlyfiles = [f.replace('.h5', '') for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_history =['dense_2_Concatenate_10_embeddings_10_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_7_epochs',\n",
    " 'dense_2_Concatenate_20_embeddings_25_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_4_epochs_dropout',\n",
    " 'matrix_facto_10_embeddings_3_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_20_epochs_dropout',\n",
    " 'dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.6-dropout-input',\n",
    " 'dense_1_Multiply_50_embeddings_7_epochs_dropout',\n",
    " 'dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.8',\n",
    " 'matrix_facto_10_embeddings_100_epochs',\n",
    " 'dense_2_Meta_Concatenate_15_embeddings_30_epochs-DropOut0.6',\n",
    " 'matrix_facto_10_embeddings_20_epochs',\n",
    " 'dense_4_Multiply_5_embeddings_7_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_2_epochs_dropout']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare MSE validation error / Train error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE validation error \n",
      " dense_4_Multiply_5_embeddings_7_epochs                               1.542893\n",
      "dense_1_Multiply_50_embeddings_7_epochs                              1.552457\n",
      "dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.8                   1.555063\n",
      "dense_1_Multiply_50_embeddings_7_epochs_dropout                      1.574078\n",
      "dense_1_Multiply_50_embeddings_20_epochs_dropout                     1.575834\n",
      "dense_1_Multiply_50_embeddings_4_epochs_dropout                      1.576017\n",
      "dense_2_Concatenate_20_embeddings_25_epochs                          1.580353\n",
      "dense_2_Concatenate_10_embeddings_10_epochs                          1.598811\n",
      "dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.6-dropout-input     1.663208\n",
      "dense_2_Meta_Concatenate_15_embeddings_30_epochs-DropOut0.6          1.672508\n",
      "dense_1_Multiply_50_embeddings_2_epochs_dropout                      1.707937\n",
      "matrix_facto_10_embeddings_20_epochs                                17.537834\n",
      "matrix_facto_10_embeddings_100_epochs                               17.537837\n",
      "matrix_facto_10_embeddings_3_epochs                                 17.814081\n",
      "dtype: float64\n",
      "\n",
      "Train error \n",
      " dense_2_Concatenate_20_embeddings_25_epochs                          0.485030\n",
      "dense_2_Concatenate_10_embeddings_10_epochs                          0.532478\n",
      "dense_4_Multiply_5_embeddings_7_epochs                               0.607777\n",
      "dense_1_Multiply_50_embeddings_7_epochs                              0.676085\n",
      "dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.8                   0.712627\n",
      "dense_1_Multiply_50_embeddings_20_epochs_dropout                     0.847064\n",
      "dense_2_Meta_Concatenate_15_embeddings_30_epochs-DropOut0.6          1.074772\n",
      "dense_1_Multiply_50_embeddings_7_epochs_dropout                      1.137871\n",
      "dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.6-dropout-input     1.545202\n",
      "dense_1_Multiply_50_embeddings_4_epochs_dropout                      1.718759\n",
      "dense_1_Multiply_50_embeddings_2_epochs_dropout                      2.102343\n",
      "matrix_facto_10_embeddings_100_epochs                               15.232453\n",
      "matrix_facto_10_embeddings_20_epochs                                15.269036\n",
      "matrix_facto_10_embeddings_3_epochs                                 17.620597\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "hist_path = \"./histories/\"\n",
    "\n",
    "validation_error = {}\n",
    "train_error = {}\n",
    "\n",
    "for val in models_history:\n",
    "    with open(hist_path +  val +'.pkl', 'rb') as file_pi:\n",
    "        thepickle = pickle.load(file_pi)\n",
    "        \n",
    "        validation_error[val]=np.min(thepickle[\"val_loss\"])\n",
    "        train_error[val]=np.min(thepickle[\"loss\"])\n",
    "        \n",
    "validation_error = pd.Series(validation_error)\n",
    "train_error = pd.Series(train_error)\n",
    "print (\"MSE validation error \\n\",validation_error.sort_values(ascending=True).head(20))\n",
    "print (\"\\nTrain error \\n\",train_error.sort_values(ascending=True).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can notice the following points from the above:\n",
    "\n",
    "- Performance got way better when using neural network comparing to using matrix factorization.\n",
    "\n",
    "- When using neural network, I converge to the best model very quickly, sometimes after 2 epochs and after that the model starts overfitting or at least the validation error does not seem to go down anymore. Matrix factorization does not converge at all.\n",
    "\n",
    "- Adding epochs lead to overfitting\n",
    "\n",
    "- Adding layers (over 3) does not help much and actually leads to overfitting\n",
    "\n",
    "- Changing the number of hidden units does not help.\n",
    "\n",
    "- Simplifying the model by reducing embedding size does not help either.\n",
    "\n",
    "- Choosing large values of embedding has made a small improvement in the results.\n",
    "\n",
    "- Multiply or concatenate user and item embeddings does not seem to matter, but concatenate seems to give little better results\n",
    "\n",
    "- Training with Dropout seem to prevent some overfitting\n",
    "\n",
    "- Adding dense layers on top of the embeddings before the merge helps a bit.\n",
    "\n",
    "- Adding some metadata lead to some improvement in the results.\n",
    "\n",
    "- Running on a larger dataset does not help either, because the data in both datasets is very skewed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/dse260-CapStone-Amazon/2-**Final-Keras-DeepRecommender\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3704\n",
      "drwxrwxr-x  8 ec2-user ec2-user   4096 May 27 04:20 .\n",
      "drwxrwxr-x 14 ec2-user ec2-user   4096 May 26 15:19 ..\n",
      "-rw-rw-r--  1 ec2-user ec2-user  11399 May 27 04:06 Amazon_Recommender_System.ipynb\n",
      "drwxrwxr-x  5 ec2-user ec2-user   4096 May 26 16:08 data\n",
      "-rw-rw-r--  1 ec2-user ec2-user 468696 May 27 01:29 DeepRecommendation_Keras.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 278681 May 27 04:19 *dense_1_Multiply_50_embeddings_4_epochs_dropout.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 285939 May 27 04:11 dense_1_Multiply_50_embeddings_7_epochs_dropout.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 191476 May 27 03:45 *dense_1_Multiply_50_embeddings_7_epochs.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 273534 May 26 21:26 *dense_2_Concatenate_10_embeddings_10_epochs.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 278732 May 26 22:02 *dense_2_Concatenate_20_embeddings_25_epochs.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 302811 May 27 03:36 *dense_2_Meta_Concatenate_15_embeddings_30_epochs-DropOut0.6.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 275764 May 26 21:10 *dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.5.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 280507 May 26 22:25 *dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.6.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 272693 May 26 20:50 *dense_4_Multiply_5_embeddings_7_epochs.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user  12804 May 27 04:20 Evaluate_And_Predict.ipynb\n",
      "drwxrwxr-x  3 ec2-user ec2-user   4096 May 27 04:19 histories\n",
      "drwxrwxr-x  2 ec2-user ec2-user   4096 May 27 04:19 .ipynb_checkpoints\n",
      "-rw-rw-r--  1 ec2-user ec2-user 211046 May 26 20:15 matrix_facto_10_embeddings_100_epochs.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 279161 May 26 17:56 matrix_facto_10_embeddings_20_epochs.ipynb\n",
      "-rw-rw-r--  1 ec2-user ec2-user 303973 May 26 20:14 matrix_facto_10_embeddings_3_epochs.ipynb\n",
      "drwxrwxr-x  3 ec2-user ec2-user   4096 May 27 04:18 models\n",
      "drwxrwxr-x  2 ec2-user ec2-user   4096 May 26 15:30 spark-warehouse\n",
      "drwxrwxr-x  5 ec2-user ec2-user   4096 May 26 20:15 temp\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict - Verifying the performance on the test set.\n",
    "- Check whether our results are reproducible on unseen data.\n",
    "- Test on new data using previously saved models.\n",
    "- I got the following results on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test = pd.read_parquet('./data/ratings_test.parquet')\n",
    "ratings_train = pd.read_parquet('./data/ratings_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models =['dense_2_Concatenate_10_embeddings_10_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_7_epochs',\n",
    " 'dense_2_Concatenate_20_embeddings_25_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_4_epochs_dropout',\n",
    " 'matrix_facto_10_embeddings_3_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_20_epochs_dropout',\n",
    " 'dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.6-dropout-input',\n",
    " 'dense_1_Multiply_50_embeddings_7_epochs_dropout',\n",
    " 'dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.8',\n",
    " 'matrix_facto_10_embeddings_100_epochs',\n",
    " 'matrix_facto_10_embeddings_20_epochs',\n",
    " 'dense_4_Multiply_5_embeddings_7_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_2_epochs_dropout']\n",
    "\n",
    "models_with_Meta =[\n",
    "'dense_2_Meta_Concatenate_15_embeddings_50_epochs-DropOut0.6',\n",
    "'dense_2_Meta_Concatenate_15_embeddings_30_epochs-DropOut0.6'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dense_1_Multiply_50_embeddings_7_epochs                              1.565703\n",
       "dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.8                   1.567906\n",
       "dense_4_Multiply_5_embeddings_7_epochs                               1.572504\n",
       "dense_2_Concatenate_20_embeddings_25_epochs                          1.575907\n",
       "dense_1_Multiply_50_embeddings_7_epochs_dropout                      1.587513\n",
       "dense_1_Multiply_50_embeddings_20_epochs_dropout                     1.587922\n",
       "dense_1_Multiply_50_embeddings_4_epochs_dropout                      1.588667\n",
       "dense_2_Concatenate_10_embeddings_10_epochs                          1.595932\n",
       "dense_4_Multiply_5_embeddings_7_epochs-DropOut-0.6-dropout-input     1.677053\n",
       "dense_1_Multiply_50_embeddings_2_epochs_dropout                      1.722337\n",
       "matrix_facto_10_embeddings_100_epochs                               17.545488\n",
       "matrix_facto_10_embeddings_20_epochs                                17.552762\n",
       "matrix_facto_10_embeddings_3_epochs                                 17.600340\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"./models/\"\n",
    "perfs = {}\n",
    "\n",
    "for mod in models:\n",
    "    model = load_model(load_path+mod+'.h5')\n",
    "    ratings_test['preds_' + mod] = model.predict([ratings_test['user_id'],\n",
    "                                                  ratings_test['item_id']])\n",
    "    perfs[mod] = mean_squared_error(ratings_test['score'], ratings_test['preds_'+mod])\n",
    "\n",
    "perfs= pd.Series(perfs)\n",
    "perfs.sort_values(ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dense_2_Meta_Concatenate_15_embeddings_50_epochs-DropOut0.6    1.649172\n",
       "dense_2_Meta_Concatenate_15_embeddings_30_epochs-DropOut0.6    1.686280\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfs = {}\n",
    "\n",
    "for mod in models_with_Meta:\n",
    "    model = load_model(load_path+mod+'.h5')\n",
    "    ratings_test['preds_' + mod] = model.predict([ratings_test[\"user_id\"]\n",
    "                                                , ratings_test[\"item_id\"]\n",
    "                                                , ratings_test[\"price_id\"]\n",
    "                                                , ratings_test[\"title_id\"]\n",
    "                                                ])\n",
    "    \n",
    "    perfs[mod] = mean_squared_error(ratings_test['score'], ratings_test['preds_'+mod]) ## MSE between real score and prdicted score\n",
    "\n",
    "perfs= pd.Series(perfs)\n",
    "perfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE on test data is very similar to what I got on the evaluation data\n",
    "### The best result  on both the internal keras random cross validation scheme and test-set acheived when using 1 layers, 15 layered concatenated embeddings, Dropout and 7 epochs\n",
    "### I will use this model further for executing recommendations (dense_1_Multiply_50_embeddings_7_epochs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- In this work I created and compared 2 models for predicting user's ratings on top of Amazon's review data: a matrix factorization model and deep network model, and used the models for recommending items to users.\n",
    "\n",
    "- I showed that using deep neural networks can achieve better performance than using matrix factorization. \n",
    "\n",
    "- Going deeper (more than 3 layers) seems to lead to overfitting and not to further improvement.\n",
    "\n",
    "- Adding epochs, reducing embedding size or change hidden units numbers does not help either.\n",
    "\n",
    "- Running on a larger dataset does not help either, because the data in both datasets is very skewed.\n",
    "\n",
    "- Choosing large values of embedding (50) and adding dense layers on top of the embeddings before concatenating helps a bit.\n",
    "\n",
    "- Adding metadata and training with Dropout lead to some improvement in the results.\n",
    "\n",
    "- The fact that the data is so sparsed and skewed has a huge impact on the ability to model the recommendation problem and to achieve smaller test MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
