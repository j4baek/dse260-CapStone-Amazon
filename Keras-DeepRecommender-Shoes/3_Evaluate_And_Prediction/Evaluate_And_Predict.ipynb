{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and compare the different models\n",
    "Using the 10% of cross validated training set records  and the history I saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "#Keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and Check GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_check_gpu():\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.per_process_gpu_memory_fraction =1 # allow all of the GPU memory to be allocated\n",
    "    # for 8 GPUs\n",
    "    # cfg.gpu_options.visible_device_list = \"0,1,2,3,4,5,6,7\" # \"0,1\"\n",
    "    # for 1 GPU\n",
    "    cfg.gpu_options.visible_device_list = \"0\"\n",
    "    #cfg.gpu_options.allow_growth = True  # # Don't pre-allocate memory; dynamically allocate the memory used on the GPU as-needed\n",
    "    #cfg.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    sess = K.tf.Session(config=cfg)\n",
    "    K.set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    print(\"* TF version: \", [tf.__version__, tf.test.is_gpu_available()])\n",
    "    print(\"* List of GPU(s): \", tf.config.experimental.list_physical_devices() )\n",
    "    print(\"* Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU'))) \n",
    "  \n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "    # set for 8 GPUs\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\";\n",
    "    # set for 1 GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\";\n",
    "\n",
    "    # Tf debugging option\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "#     print(tf.config.list_logical_devices('GPU'))\n",
    "    print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* TF version:  ['1.15.2', True]\n",
      "* List of GPU(s):  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "* Num GPUs Available:  1\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "set_check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_1_Multiply_50_embeddings_4_epochs_dropout',\n",
       " 'dense_5_Multiply_50_embeddings_10_epochs_dropout',\n",
       " 'matrix_facto_10_embeddings_100_epochs',\n",
       " 'dense_5_Meta_Multiply_50_embeddings_10_epochs_dropout',\n",
       " 'dense_1_Multiply_50_embeddings_100_epochs_dropout']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = '../models'\n",
    "\n",
    "onlyfiles = [f.replace('.h5', '') for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_history =['dense_1_Multiply_50_embeddings_4_epochs_dropout',\n",
    " 'dense_5_Multiply_50_embeddings_10_epochs_dropout',\n",
    " 'matrix_facto_10_embeddings_100_epochs',\n",
    " 'dense_5_Meta_Multiply_50_embeddings_10_epochs_dropout',\n",
    " 'dense_1_Multiply_50_embeddings_100_epochs_dropout']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare MSE validation error / Train error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE validation error \n",
      " dense_1_Multiply_50_embeddings_4_epochs_dropout           1.573231\n",
      "dense_1_Multiply_50_embeddings_100_epochs_dropout         1.577702\n",
      "dense_5_Multiply_50_embeddings_10_epochs_dropout          1.599752\n",
      "dense_5_Meta_Multiply_50_embeddings_10_epochs_dropout     1.608152\n",
      "matrix_facto_10_embeddings_100_epochs                    18.228967\n",
      "dtype: float64\n",
      "\n",
      "Train error \n",
      " matrix_facto_10_embeddings_100_epochs                    0.026353\n",
      "dense_1_Multiply_50_embeddings_100_epochs_dropout        0.360574\n",
      "dense_1_Multiply_50_embeddings_4_epochs_dropout          0.889537\n",
      "dense_5_Meta_Multiply_50_embeddings_10_epochs_dropout    0.913438\n",
      "dense_5_Multiply_50_embeddings_10_epochs_dropout         1.125387\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "hist_path = \"../histories/\"\n",
    "\n",
    "validation_error = {}\n",
    "train_error = {}\n",
    "\n",
    "for val in models_history:\n",
    "    with open(hist_path +  val +'.pkl', 'rb') as file_pi:\n",
    "        thepickle = pickle.load(file_pi)\n",
    "        \n",
    "        validation_error[val]=np.min(thepickle[\"val_loss\"])\n",
    "        train_error[val]=np.min(thepickle[\"loss\"])\n",
    "        \n",
    "validation_error = pd.Series(validation_error)\n",
    "train_error = pd.Series(train_error)\n",
    "print (\"MSE validation error \\n\",validation_error.sort_values(ascending=True).head(20))\n",
    "print (\"\\nTrain error \\n\",train_error.sort_values(ascending=True).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can notice the following points from the above:\n",
    "\n",
    "- Performance got way better when using neural network comparing to using matrix factorization.\n",
    "\n",
    "- When using neural network, I converge to the best model very quickly, sometimes after 2 epochs and after that the model starts overfitting or at least the validation error does not seem to go down anymore. Matrix factorization does not converge at all.\n",
    "\n",
    "- Adding epochs lead to overfitting\n",
    "\n",
    "- Adding layers (over 3) does not help much and actually leads to overfitting\n",
    "\n",
    "- Changing the number of hidden units does not help.\n",
    "\n",
    "- Simplifying the model by reducing embedding size does not help either.\n",
    "\n",
    "- Choosing large values of embedding has made a small improvement in the results.\n",
    "\n",
    "- Multiply or concatenate user and item embeddings does not seem to matter, but concatenate seems to give little better results\n",
    "\n",
    "- Training with Dropout seem to prevent some overfitting\n",
    "\n",
    "- Adding dense layers on top of the embeddings before the merge helps a bit.\n",
    "\n",
    "- Adding some metadata lead to some improvement in the results.\n",
    "\n",
    "- Running on a larger dataset does not help either, because the data in both datasets is very skewed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/dse260-CapStone-Amazon/3-**Final-Keras-DeepRecommender-Shoes/3_Evaluate_And_Prediction\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\n",
      "drwxrwxr-x  3 ec2-user ec2-user  4096 May 27 20:25 .\n",
      "drwxrwxr-x 11 ec2-user ec2-user  4096 May 27 18:24 ..\n",
      "-rw-rw-r--  1 ec2-user ec2-user 16856 May 27 20:25 *Evaluate_And_Predict.ipynb\n",
      "drwxrwxr-x  2 ec2-user ec2-user  4096 May 27 17:25 .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict - Verifying the performance on the test set.\n",
    "- Check whether our results are reproducible on unseen data.\n",
    "- Test on new data using previously saved models.\n",
    "- I got the following results on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 97758 unique items in metadata \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "review_data = pd.read_csv('../data/amazon_reviews_us_Shoes_v1_00_help_voted_And_cut_lognTail.csv')\n",
    "review_data.rename(columns={ 'star_rating': 'score','customer_id': 'user_id', 'user': 'user_name'}, inplace=True)\n",
    "\n",
    "items = review_data.product_id.unique()\n",
    "item_map = {i:val for i,val in enumerate(items)}\n",
    "inverse_item_map = {val:i for i,val in enumerate(items)}\n",
    "review_data[\"old_item_id\"] = review_data[\"product_id\"] # copying for join with metadata\n",
    "review_data[\"item_id\"] = review_data[\"product_id\"].map(inverse_item_map)\n",
    "items = review_data.item_id.unique()\n",
    "print (\"We have %d unique items in metadata \"%items.shape[0])\n",
    "\n",
    "users = review_data.user_id.unique()\n",
    "user_map = {i:val for i,val in enumerate(users)}\n",
    "inverse_user_map = {val:i for i,val in enumerate(users)}\n",
    "review_data[\"old_user_id\"] = review_data[\"user_id\"] \n",
    "review_data[\"user_id\"] = review_data[\"user_id\"].map(inverse_user_map)\n",
    "\n",
    "items_reviewed = review_data.product_id.unique()\n",
    "review_data[\"old_item_id\"] = review_data[\"product_id\"] # copying for join with metadata\n",
    "review_data[\"item_id\"] = review_data[\"product_id\"].map(inverse_item_map)\n",
    "\n",
    "items_reviewed = review_data.item_id.unique()\n",
    "users = review_data.user_id.unique()\n",
    "helpful_votes = review_data.helpful_votes.unique()\n",
    "\n",
    "\n",
    "\n",
    "ratings_train, ratings_test = train_test_split( review_data, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models =['dense_1_Multiply_50_embeddings_4_epochs_dropout',\n",
    " 'dense_5_Multiply_50_embeddings_10_epochs_dropout',\n",
    " 'matrix_facto_10_embeddings_100_epochs',\n",
    " 'dense_1_Multiply_50_embeddings_100_epochs_dropout']\n",
    "\n",
    "models_with_Meta =[\n",
    " 'dense_5_Meta_Multiply_50_embeddings_10_epochs_dropout'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dense_5_Multiply_50_embeddings_10_epochs_dropout      1.596477\n",
       "dense_1_Multiply_50_embeddings_4_epochs_dropout       1.615289\n",
       "dense_1_Multiply_50_embeddings_100_epochs_dropout     1.616922\n",
       "matrix_facto_10_embeddings_100_epochs                18.230796\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"../models/\"\n",
    "\n",
    "perfs = {}\n",
    "\n",
    "for mod in models:\n",
    "    model = load_model(load_path+mod+'.h5')\n",
    "    ratings_test['preds_' + mod] = model.predict([ratings_test['user_id'],\n",
    "                                                  ratings_test['item_id']])\n",
    "    perfs[mod] = mean_squared_error(ratings_test['score'], ratings_test['preds_'+mod])\n",
    "\n",
    "perfs= pd.Series(perfs)\n",
    "perfs.sort_values(ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dense_5_Meta_Multiply_50_embeddings_10_epochs_dropout    1.605814\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfs = {}\n",
    "\n",
    "for mod in models_with_Meta:\n",
    "    model = load_model(load_path+mod+'.h5')\n",
    "    ratings_test['preds_' + mod] = model.predict([ratings_test[\"user_id\"]\n",
    "                                                , ratings_test[\"item_id\"]\n",
    "                                                , ratings_test[\"helpful_votes\"]\n",
    "                                                ])\n",
    "    \n",
    "    perfs[mod] = mean_squared_error(ratings_test['score'], ratings_test['preds_'+mod]) ## MSE between real score and prdicted score\n",
    "\n",
    "perfs= pd.Series(perfs)\n",
    "perfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE on test data is very similar to what I got on the evaluation data\n",
    "### The best result  on both the internal keras random cross validation scheme and test-set acheived when using 5 layers, 5 layered concatenated embeddings, Dropout and 10 epochs\n",
    "### I will use this model further for executing recommendations (dense_5_Multiply_50_embeddings_10_epochs_dropout )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- In this work I created and compared 2 models for predicting user's ratings on top of Amazon's review data: a matrix factorization model and deep network model, and used the models for recommending items to users.\n",
    "\n",
    "- I showed that using deep neural networks can achieve better performance than using matrix factorization. \n",
    "\n",
    "- Going deeper (more than 3 layers) seems to lead to overfitting and not to further improvement.\n",
    "\n",
    "- Adding epochs, reducing embedding size or change hidden units numbers does not help either.\n",
    "\n",
    "- Running on a larger dataset does not help either, because the data in both datasets is very skewed.\n",
    "\n",
    "- Choosing large values of embedding (50) and adding dense layers on top of the embeddings before concatenating helps a bit.\n",
    "\n",
    "- Adding metadata and training with Dropout lead to some improvement in the results.\n",
    "\n",
    "- The fact that the data is so sparsed and skewed has a huge impact on the ability to model the recommendation problem and to achieve smaller test MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
